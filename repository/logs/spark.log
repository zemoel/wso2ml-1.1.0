[2016-06-20 13:52:12,238]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-20 13:52:12,362]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-20 13:52:12,429]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-20 13:52:12,429]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-20 13:52:12,449]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 13:52:12,449]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 13:52:12,450]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-20 13:52:12,877]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-20 13:52:12,915]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-20 13:52:13,056]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:36705] {Remoting}
[2016-06-20 13:52:13,064]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 36705. {org.apache.spark.util.Utils}
[2016-06-20 13:52:13,098]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-20 13:52:13,116]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-20 13:52:13,147]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-ba3a4eb6-816d-46fa-ac8d-4d3385aff9f1/blockmgr-e2dea92a-ca95-4987-8c7f-204d1cc4fe80 {org.apache.spark.storage.DiskBlockManager}
[2016-06-20 13:52:13,155]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:13,189]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-ba3a4eb6-816d-46fa-ac8d-4d3385aff9f1/httpd-0f758093-0195-484a-8141-e976528a4ef9 {org.apache.spark.HttpFileServer}
[2016-06-20 13:52:13,192]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-20 13:52:13,236]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 13:52:13,247]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:44219 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 13:52:13,247]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 44219. {org.apache.spark.util.Utils}
[2016-06-20 13:52:13,256]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-20 13:52:13,344]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 13:52:13,354]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 13:52:13,354]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-20 13:52:13,356]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 13:52:13,415]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-20 13:52:13,726]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47355. {org.apache.spark.util.Utils}
[2016-06-20 13:52:13,726]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 47355 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-20 13:52:13,727]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 13:52:13,731]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:47355 with 983.1 MB RAM, BlockManagerId(driver, localhost, 47355) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 13:52:13,733]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 13:52:51,199]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,202]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,364]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,364]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,366]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:51,374]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:52:51,497]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:52:51,590]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:52:51,622]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,628]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,637]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,644]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,650]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,682]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1886) called with curMem=45737, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,685]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1886.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:51,687]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:47355 (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:51,688]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:51,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,696]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:51,755]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:51,772]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:51,808]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:51,821]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 13:52:51,821]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 13:52:51,821]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 13:52:51,821]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 13:52:51,821]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 13:52:51,909]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:51,940]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 203 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:51,944]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:51,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.238 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:51,971]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.380817 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,079]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47623, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,081]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,109]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86111, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,110]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,111]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,112]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLUtils.java:73 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,125]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:52:52,155]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:75 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:75) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:75) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,164]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,164]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,167]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=90200, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,167]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,173]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1897) called with curMem=93432, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,179]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 1897.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,180]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:47355 (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,180]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,181]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,181]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,182]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,183]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,186]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:52,193]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,204]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 22 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,204]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:75) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,205]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:75, took 0.049953 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,205]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,213]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:153 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,214]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:153) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,214]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:153) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,214]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,215]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,216]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[3] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,217]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=95329, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,217]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,222]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1897) called with curMem=98561, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,222]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1897.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,223]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:47355 (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,225]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,227]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,227]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,228]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,229]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,233]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:52,239]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,253]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 25 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,253]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:153) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,259]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:153, took 0.045938 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,302]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:163 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,304]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (first at MLUtils.java:163) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,304]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(first at MLUtils.java:163) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,304]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,305]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,306]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[3] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,307]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=100458, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,307]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,312]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1897) called with curMem=103690, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,313]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 1897.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,314]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:47355 (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,318]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,319]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[3] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,319]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,321]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,321]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,324]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:52,331]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,343]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,343]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (first at MLUtils.java:163) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,344]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: first at MLUtils.java:163, took 0.041325 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,344]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,379]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 4 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 13:52:52,388]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 4 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:52:52,416]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:187 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,419]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (first at MLUtils.java:187) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,419]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 4(first at MLUtils.java:187) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,419]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,421]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,421]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 4 (MapPartitionsRDD[3] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,423]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=105587, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,423]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,428]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1897) called with curMem=108819, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,429]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 1897.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,430]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:47355 (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,435]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[3] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,435]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,437]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,437]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,440]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:52,448]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,455]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 18 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,455]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 4 (first at MLUtils.java:187) finished in 0.019 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,455]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,456]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: first at MLUtils.java:187, took 0.039614 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,460]  INFO {org.apache.spark.SparkContext} -  Starting job: takeSample at MLUtils.java:194 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (takeSample at MLUtils.java:194) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(takeSample at MLUtils.java:194) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,464]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,465]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[5] at map at MLUtils.java:167), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,472]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4288) called with curMem=110716, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,473]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,478]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2540) called with curMem=115004, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,478]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,479]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:47355 (size: 2.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,479]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,480]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[5] at map at MLUtils.java:167) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,480]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,481]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,481]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,494]  INFO {org.apache.spark.CacheManager} -  Partition rdd_5_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 13:52:52,494]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:52:52,742]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(362816) called with curMem=117544, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,743]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_5_0 stored as values in memory (estimated size 354.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,743]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_5_0 in memory on localhost:47355 (size: 354.3 KB, free: 982.7 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,753]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 2331 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,758]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 278 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,758]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (takeSample at MLUtils.java:194) finished in 0.276 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,758]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,759]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: takeSample at MLUtils.java:194, took 0.298198 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,786]  INFO {org.apache.spark.SparkContext} -  Starting job: takeSample at MLUtils.java:194 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (takeSample at MLUtils.java:194) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 6(takeSample at MLUtils.java:194) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 6 (MapPartitionsRDD[5] at map at MLUtils.java:167), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,791]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4432) called with curMem=480360, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,792]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 4.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,796]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2591) called with curMem=484792, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,797]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:52:52,798]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:47355 (size: 2.5 KB, free: 982.7 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:52:52,798]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:52:52,798]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[5] at map at MLUtils.java:167) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,798]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:52:52,803]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,803]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,808]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_5_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 13:52:52,845]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 31102 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:52:52,862]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 6 (takeSample at MLUtils.java:194) finished in 0.055 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: takeSample at MLUtils.java:194, took 0.076673 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:52:52,866]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 5 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 13:52:52,868]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 5 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:52:52,875]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 60 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:52:52,875]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:54:27,989]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=124567, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:27,990]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:27,999]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=163055, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:27,999]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:28,000]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:28,001]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:54:28,008]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:54:28,013]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:54:28,014]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,015]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 7(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,015]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,015]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 7 (MapPartitionsRDD[7] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,016]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=167144, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:28,017]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 3.2 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:28,021]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1900) called with curMem=170376, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:28,021]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 1900.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:28,022]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:47355 (size: 1900.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:28,022]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:54:28,022]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[7] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,022]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:54:28,023]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:54:28,023]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-20 13:54:28,026]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423667988:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:54:28,031]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:54:28,036]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:54:28,036]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:54:28,037]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 7 (first at MLUtils.java:91) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:28,037]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: first at MLUtils.java:91, took 0.023687 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:44,390]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:47355 in memory (size: 1900.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,400]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:47355 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,409]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:47355 in memory (size: 2.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,411]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:47355 in memory (size: 2.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,414]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:47355 in memory (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,415]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 5 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:54:44,417]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 5 {org.apache.spark.ContextCleaner}
[2016-06-20 13:54:44,418]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 4 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:54:44,418]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 4 {org.apache.spark.ContextCleaner}
[2016-06-20 13:54:44,420]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:47355 in memory (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,421]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:47355 in memory (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,422]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:47355 in memory (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,424]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:47355 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,425]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:47355 in memory (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:44,426]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:47355 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:48,736]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,736]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,746]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,747]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,748]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:48,748]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:54:48,756]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:54:48,761]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:54:48,762]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,763]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 8(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,763]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,764]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,764]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 8 (MapPartitionsRDD[9] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,765]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,766]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,770]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1901) called with curMem=45809, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,770]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 1901.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:54:48,771]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:47355 (size: 1901.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:54:48,772]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:54:48,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,772]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:54:48,773]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:54:48,773]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-20 13:54:48,775]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423688734:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:54:48,780]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:54:48,786]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:54:48,786]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:54:48,786]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 8 (first at MLUtils.java:91) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:54:48,787]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: first at MLUtils.java:91, took 0.025490 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,551]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47710, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,551]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,559]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86198, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,560]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,561]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:56:37,561]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:56:37,568]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:56:37,573]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 13:56:37,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 9(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,576]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,576]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 9 (MapPartitionsRDD[11] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,577]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=90287, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,577]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,580]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1900) called with curMem=93527, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,580]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 1900.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:56:37,581]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:47355 (size: 1900.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:56:37,581]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 13:56:37,581]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[11] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,581]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:56:37,582]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:56:37,582]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-20 13:56:37,584]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423797549:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 13:56:37,587]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 13:56:37,590]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 13:56:37,590]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 9 (first at MLUtils.java:91) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:56:37,590]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 13:56:37,590]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: first at MLUtils.java:91, took 0.016525 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:57:09,099]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=95427, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:57:09,099]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:57:09,106]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=133915, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:57:09,107]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:57:09,107]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:47355 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 13:57:09,108]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 13:57:09,171]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 13:57:09,175]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:57:09,192]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 21 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 13:57:09,192]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 21 {org.apache.spark.storage.BlockManager}
[2016-06-20 13:57:46,898]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-20 13:57:46,917]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 13:57:46,921]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,922]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,926]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,927]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,928]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,928]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,928]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,929]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,929]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,930]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,930]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,930]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,930]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,932]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,934]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,935]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,935]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,939]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,940]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,941]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 13:57:46,994]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 13:57:47,011]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 13:57:47,116]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-20 13:57:47,127]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-ba3a4eb6-816d-46fa-ac8d-4d3385aff9f1/blockmgr-e2dea92a-ca95-4987-8c7f-204d1cc4fe80, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-20 13:57:47,130]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-20 13:57:47,130]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-20 13:57:47,133]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 13:57:47,140]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-20 13:57:47,141]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-20 13:57:47,141]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-ba3a4eb6-816d-46fa-ac8d-4d3385aff9f1 {org.apache.spark.util.Utils}
[2016-06-20 13:57:47,143]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-20 13:57:47,162]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 13:57:47,163]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 13:57:47,200]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 13:57:48,073]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 13:57:52,306]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 14:02:43,458]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-20 14:02:43,570]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-20 14:02:43,625]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-20 14:02:43,625]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-20 14:02:43,639]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 14:02:43,640]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 14:02:43,641]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-20 14:02:44,104]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-20 14:02:44,179]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-20 14:02:44,352]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:33774] {Remoting}
[2016-06-20 14:02:44,363]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 33774. {org.apache.spark.util.Utils}
[2016-06-20 14:02:44,389]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-20 14:02:44,403]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-20 14:02:44,427]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-90679c12-bfa0-4bec-ae48-ebe65f8fb91b/blockmgr-b07195dc-35db-45b1-b99b-1eb00bf7a7b8 {org.apache.spark.storage.DiskBlockManager}
[2016-06-20 14:02:44,434]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:02:44,463]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-90679c12-bfa0-4bec-ae48-ebe65f8fb91b/httpd-e0e5ffc3-2b23-46d6-ad76-c054e32a2b9e {org.apache.spark.HttpFileServer}
[2016-06-20 14:02:44,466]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-20 14:02:44,514]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 14:02:44,528]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:41084 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 14:02:44,529]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 41084. {org.apache.spark.util.Utils}
[2016-06-20 14:02:44,537]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-20 14:02:44,642]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 14:02:44,659]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 14:02:44,660]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-20 14:02:44,661]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 14:02:44,731]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-20 14:02:44,884]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48669. {org.apache.spark.util.Utils}
[2016-06-20 14:02:44,884]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 48669 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-20 14:02:44,885]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:02:44,888]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:48669 with 983.1 MB RAM, BlockManagerId(driver, localhost, 48669) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:02:44,890]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:03:16,471]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,474]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,636]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,636]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,639]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:03:16,643]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:03:16,766]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:03:16,831]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:03:16,853]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,855]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,861]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,866]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,877]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,878]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,892]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1886) called with curMem=45737, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,893]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1886.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:16,894]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:48669 (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:03:16,897]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:03:16,906]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:16,910]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:03:16,953]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:03:16,961]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-20 14:03:16,989]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466424196242:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:03:16,999]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 14:03:16,999]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 14:03:16,999]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 14:03:16,999]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 14:03:16,999]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 14:03:17,057]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:03:17,072]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.142 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:17,071]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 129 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:03:17,076]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:03:17,088]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.256394 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:03:48,939]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47623, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:48,939]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:48,952]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86111, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:48,953]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:03:48,954]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:03:48,955]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 14:03:49,043]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:03:49,051]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:03:49,068]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:03:49,069]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:17,241]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 2603313 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-20 14:47:17,257] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 2603313 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:17,260]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,261]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:47:17,262]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 48669) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:47:17,262]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:47:17,269]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-20 14:47:17,269]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:17,269]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:47:17,270]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,275]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:48669 with 983.1 MB RAM, BlockManagerId(driver, localhost, 48669) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:47:17,276]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:47:17,276]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 6 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:17,280]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,281]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,283]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:48669 (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,317]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:47:17,438]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-20 14:47:17,440]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,440]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,440]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,467]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,468]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,478]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=90200, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,479]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 7.4 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,504]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3673) called with curMem=97808, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,504]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,513]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:48669 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,536]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:17,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,536]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:17,545]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:17,546]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:17,620]  INFO {org.apache.spark.CacheManager} -  Partition rdd_12_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:47:17,620]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:47:17,914]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=101481, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,915]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_12_0 stored as values in memory (estimated size 55.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,916]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_12_0 in memory on localhost:48669 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,926] ERROR {org.apache.spark.util.Utils} -  Uncaught exception in thread driver-heartbeater {org.apache.spark.util.Utils}
java.lang.ClassNotFoundException: org.apache.spark.storage.RDDBlockId cannot be found by org.scala-lang.scala-library_2.10.4.v20140209-180020-VFINAL-b66a39653b
	at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:501)
	at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)
	at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)
	at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:626)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:103)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:432)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:423)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:423)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:421)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:421)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-20 14:47:17,937]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:17,959]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 413 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:17,959]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (take at DecisionTreeMetadata.scala:110) finished in 0.422 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,959]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:17,960]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: take at DecisionTreeMetadata.scala:110, took 0.521112 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,963]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-20 14:47:17,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,966]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,967]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,969]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=157849, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,970]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 7.3 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,976]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3583) called with curMem=165289, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,977]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:17,978]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:17,981]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:17,982]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:17,982]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:17,986]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:17,987]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:17,991]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:17,997]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,008]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (count at DecisionTreeMetadata.scala:111) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,009]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: count at DecisionTreeMetadata.scala:111, took 0.045534 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,009]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,010]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,043]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,044]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,044]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,044]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,046]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,049]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8168) called with curMem=168872, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,049]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 8.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,057]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3909) called with curMem=177040, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,058]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,059]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:48669 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,060]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,060]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,060]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,063]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,063]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,067]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:18,086]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,110]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 49 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,110]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (collect at DecisionTree.scala:977) finished in 0.047 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,111]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,111]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: collect at DecisionTree.scala:977, took 0.067763 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,156]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=180949, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,157]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 624.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,164]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=181573, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,168]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,170]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:48669 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,171]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,232]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 18 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,243]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,257]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=181644, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,257]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 18.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,264]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8079) called with curMem=200492, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,264]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,265]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:48669 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,266]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,268]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,268]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,270]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,271]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,280]  INFO {org.apache.spark.CacheManager} -  Partition rdd_17_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:47:18,280]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:18,352]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=208571, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,352]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_17_0 stored as values in memory (estimated size 64.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,353]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_17_0 in memory on localhost:48669 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,442]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,455]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 186 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,455]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,456]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 4 (mapPartitions at DecisionTree.scala:613) finished in 0.181 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,456]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,457]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,457]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 5) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,457]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,460]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 5: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,463]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,464]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=274539, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,464]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,471]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=282587, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,472]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,473]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,473]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,473]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,473]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,476]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,476]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,495]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,496]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 7 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,569]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,578]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 103 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,578]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (collectAsMap at DecisionTree.scala:642) finished in 0.096 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,578]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,578]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: collectAsMap at DecisionTree.scala:642, took 0.345842 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,583]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=286156, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,583]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 1160.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,588]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=287316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,588]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,589]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:48669 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,590]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,606]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 21 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 7(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,608]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,609]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,611]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=287429, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,611]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 20.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,615]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8912) called with curMem=307981, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,616]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,616]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:48669 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,617]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,618]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,619]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,619]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,624]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:18,647]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 32 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 6 (mapPartitions at DecisionTree.scala:613) finished in 0.027 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,652]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 7: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,652]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,653]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=316893, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,653]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 8.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,658]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3826) called with curMem=325245, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,658]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,659]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:48669 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,660]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,660]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,660]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,661]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,661]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,666]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,666]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,705]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,715]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 7 (collectAsMap at DecisionTree.scala:642) finished in 0.043 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,715]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: collectAsMap at DecisionTree.scala:642, took 0.109316 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,715]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 53 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,715]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,718]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=329071, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,718]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,723]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=331311, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,723]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,724]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:48669 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,725]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,736]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 24 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 9(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,740]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,741]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,744]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=331520, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,744]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,750]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10106) called with curMem=355104, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,750]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,751]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:48669 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,752]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,752]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,752]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,753]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,754]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,758]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:18,782]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 8 (mapPartitions at DecisionTree.scala:613) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,788]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,789]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 35 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,789]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 9: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,791]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=365210, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,792]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,797]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4054) called with curMem=374170, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,797]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,798]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,798]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,799]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,800]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,800]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,805]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,805]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,838]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,843]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 43 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 9 (collectAsMap at DecisionTree.scala:642) finished in 0.039 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,843]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: collectAsMap at DecisionTree.scala:642, took 0.106234 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,846]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=378224, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,846]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,850]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=380464, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,851]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,851]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:48669 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,852]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,868]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,869]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 27 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 11(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,871]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,873]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,875]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28952) called with curMem=380681, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,876]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 28.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,881]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11800) called with curMem=409633, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,881]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,882]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:48669 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,882]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,883]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,883]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 10.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,884]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 10.0 (TID 10, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,884]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 10.0 (TID 10) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,888]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:18,905]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 10.0 (TID 10). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 10.0 (TID 10) in 26 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 10.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 10 (mapPartitions at DecisionTree.scala:613) finished in 0.025 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,909]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,910]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 11: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,910]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,912]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=421433, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,912]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4004) called with curMem=430393, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,916]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,916]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,916]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,917]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,917]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 11.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,917]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 11.0 (TID 11, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,918]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 11.0 (TID 11) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,922]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,922]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:18,954]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 11.0 (TID 11). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:18,961]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 11 (collectAsMap at DecisionTree.scala:642) finished in 0.043 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,961]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: collectAsMap at DecisionTree.scala:642, took 0.093242 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,964]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=434397, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,964]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,966]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 11.0 (TID 11) in 43 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:18,966]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 11.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:18,969]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=436205, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,969]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,969]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:48669 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:18,970]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,987]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:47:18,988]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 30 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,989]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,989]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 13(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,989]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,990]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,991]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:18,994]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=436378, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:18,994]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 33.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,000]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13190) called with curMem=470282, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,000]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,001]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:48669 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,001]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,002]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 12.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,003]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 12.0 (TID 12, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,003]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 12.0 (TID 12) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,008]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,028]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 12.0 (TID 12). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 12.0 (TID 12) in 29 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 12.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 12 (mapPartitions at DecisionTree.scala:613) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,033]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 13: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,033]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,035]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=483472, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,035]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 8.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,039]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4009) called with curMem=492192, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,039]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,040]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,040]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,041]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,041]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 13.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,041]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 13.0 (TID 13, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,042]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 13.0 (TID 13) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,046]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,046]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,072]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 13.0 (TID 13). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,077]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 13.0 (TID 13) in 36 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,077]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 13 (collectAsMap at DecisionTree.scala:642) finished in 0.036 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,077]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 13.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,078]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: collectAsMap at DecisionTree.scala:642, took 0.090650 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,079]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:47:19,081]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,084]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:47:19,085]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.840952051
  total: 1.7837997
  findSplitsBins: 0.109287403
  findBestSplits: 0.926274054
  chooseSplits: 0.919135282 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:47:19,089]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 12 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:47:19,090]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,117]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,118]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,118]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 14(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,118]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,121]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,122]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,124]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28656) called with curMem=373865, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,124]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,128]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10839) called with curMem=402521, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,128]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,129]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_21_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,130]  INFO {org.apache.spark.SparkContext} -  Created broadcast 21 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,130]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,130]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 14.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,131]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 14.0 (TID 14, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,132]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 14.0 (TID 14) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,138]  INFO {org.apache.spark.CacheManager} -  Partition rdd_33_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:47:19,139]  INFO {org.apache.spark.CacheManager} -  Partition rdd_13_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:47:19,139]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:47:19,172]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=413360, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,173]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_13_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,173]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_13_0 in memory on localhost:48669 (size: 29.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,190]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=443056, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,190]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_33_0 stored as values in memory (estimated size 15.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,191]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_33_0 in memory on localhost:48669 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,195]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 14.0 (TID 14). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,198]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 14.0 (TID 14) in 67 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 14 (take at SparkModelUtils.java:215) finished in 0.068 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,198]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 14.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: take at SparkModelUtils.java:215, took 0.081121 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,205]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 10 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 15(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,208]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=459184, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,209]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22 stored as values in memory (estimated size 7.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,213]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3551) called with curMem=466456, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,213]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,214]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_22_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,214]  INFO {org.apache.spark.SparkContext} -  Created broadcast 22 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,215]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,215]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 15.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,216]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 15.0 (TID 15, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,216]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 15.0 (TID 15) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,218]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_13_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,223]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 15.0 (TID 15). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,230]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 15.0 (TID 15) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,230]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 15.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,230]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 15 (take at SparkModelUtils.java:223) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,231]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 10 finished: take at SparkModelUtils.java:223, took 0.025655 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,238]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 11 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 16(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,242]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=470007, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,242]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23 stored as values in memory (estimated size 1320.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,245]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=471327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,245]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,246]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_23_piece0 in memory on localhost:48669 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,246]  INFO {org.apache.spark.SparkContext} -  Created broadcast 23 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,247]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,247]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 16.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,258]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 16.0 (TID 16, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,259]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 16.0 (TID 16) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,268]  INFO {org.apache.spark.CacheManager} -  Partition rdd_34_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:47:19,272]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=472204, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,273]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_34_0 stored as values in memory (estimated size 32.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,273]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_34_0 in memory on localhost:48669 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,276]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 16.0 (TID 16). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,279]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 16.0 (TID 16) in 32 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,279]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 16.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,280]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 16 (count at SparkModelUtils.java:241) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,280]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 11 finished: count at SparkModelUtils.java:241, took 0.041614 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,283]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,284]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 12 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,284]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 17(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,284]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,284]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,285]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,285]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=505292, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,285]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24 stored as values in memory (estimated size 1496.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,289]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=506788, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,289]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,290]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_24_piece0 in memory on localhost:48669 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,290]  INFO {org.apache.spark.SparkContext} -  Created broadcast 24 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,290]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 17.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,296]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 17.0 (TID 17, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,296]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 17.0 (TID 17) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,301]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_34_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,305]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 17.0 (TID 17). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,310]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 17.0 (TID 17) in 19 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,311]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 17 (collect at SparkModelUtils.java:245) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,311]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 12 finished: collect at SparkModelUtils.java:245, took 0.027579 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,311]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 17.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,311]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 34 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:47:19,312]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,318]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 13 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 18(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,320]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,320]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,323]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=474630, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,323]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,327]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10911) called with curMem=503342, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,328]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,328]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_25_piece0 in memory on localhost:48669 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,329]  INFO {org.apache.spark.SparkContext} -  Created broadcast 25 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,329]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,329]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 18.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,330]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 18.0 (TID 18, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,330]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 18.0 (TID 18) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,334]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,337]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 18.0 (TID 18). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,340]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 18.0 (TID 18) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,340]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 18.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 18 (count at SparkModelUtils.java:252) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,341]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 13 finished: count at SparkModelUtils.java:252, took 0.022916 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,342]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,343]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 14 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,343]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 19(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,343]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,344]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,344]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,346]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=514253, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,347]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26 stored as values in memory (estimated size 27.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,349]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10748) called with curMem=542749, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,350]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,350]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_26_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,351]  INFO {org.apache.spark.SparkContext} -  Created broadcast 26 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,351]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,351]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 19.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,352]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 19.0 (TID 19, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,352]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 19.0 (TID 19) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,356]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,359]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 19.0 (TID 19). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,365]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 19 (count at SparkModelUtils.java:252) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,366]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 14 finished: count at SparkModelUtils.java:252, took 0.023548 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,366]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:47:19,367]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,367]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 19.0 (TID 19) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,367]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 19.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,374]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,375]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 15 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,375]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 20(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,375]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,376]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,377]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,379]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28640) called with curMem=523801, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,379]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,383]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10804) called with curMem=552441, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,384]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,384]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_27_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,385]  INFO {org.apache.spark.SparkContext} -  Created broadcast 27 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,385]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,385]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 20.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,386]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 20.0 (TID 20, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,386]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 20.0 (TID 20) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,389]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,394]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 20.0 (TID 20). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,398]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 20.0 (TID 20) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,399]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 20.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,399]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 20 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 15 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.025022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,401]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 36 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:47:19,401]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,403]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 33 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:47:19,404]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 33 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,415]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 37 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 16 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 22(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,416]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,418]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=547117, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,418]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,422]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1708) called with curMem=549981, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,422]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28_piece0 stored as bytes in memory (estimated size 1708.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,422]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:48669 (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,423]  INFO {org.apache.spark.SparkContext} -  Created broadcast 28 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,423]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,423]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 21.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,428]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 21.0 (TID 21, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,428]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 21.0 (TID 21) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,495]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 21.0 (TID 21). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 21 (map at MulticlassMetrics.scala:47) finished in 0.074 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 22) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,503]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 22: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,503]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,504]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=551689, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,506]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,508]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 21.0 (TID 21) in 78 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,508]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 21.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,512]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1392) called with curMem=553985, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,512]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29_piece0 stored as bytes in memory (estimated size 1392.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,514]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:48669 (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,515]  INFO {org.apache.spark.SparkContext} -  Created broadcast 29 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,515]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,515]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 22.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,516]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 22.0 (TID 22, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,516]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 22.0 (TID 22) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,520]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,520]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,526]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 22.0 (TID 22). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,529]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 22.0 (TID 22) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,529]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 22 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,529]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 22.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,529]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 16 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.114714 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,572]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,573]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 39 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 17 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 24(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,576]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=542648, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,576]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,588]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=534716, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,588]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_27_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,589]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,589]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:48669 (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,590]  INFO {org.apache.spark.SparkContext} -  Created broadcast 30 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,590]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,590]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 23.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,591]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_26_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,594]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_25_piece0 on localhost:48669 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,599]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 23.0 (TID 23, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,600]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 23.0 (TID 23) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,602]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_24_piece0 on localhost:48669 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,604]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_23_piece0 on localhost:48669 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,605]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,606]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 34 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,620]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_22_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,622]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_21_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,624]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:48669 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,626]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 23.0 (TID 23). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 23.0 (TID 23) in 40 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 23.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 23 (map at MulticlassMetrics.scala:57) finished in 0.031 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,630]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,632]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 24: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,632]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,634]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=326886, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,635]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31 stored as values in memory (estimated size 2.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,637]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 4 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,639]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=329190, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,639]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:19,640]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:48669 (size: 1390.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,641]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:48669 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,642]  INFO {org.apache.spark.SparkContext} -  Created broadcast 31 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:47:19,642]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,642]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 24.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,643]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,643]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 24.0 (TID 24, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,644]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 24.0 (TID 24) {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,645]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:48669 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,647]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,647]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:47:19,652]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 3 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,653]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_15_piece0 on localhost:48669 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,654]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_14_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,656]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:48669 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,656]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 24.0 (TID 24). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:47:19,658]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 2 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,659]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:48669 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,659]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 24.0 (TID 24) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:47:19,659]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 24.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:47:19,659]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 24 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,660]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 17 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.086992 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:19,660]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:48669 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,662]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:48669 in memory (size: 8.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,668]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 1 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,669]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:48669 in memory (size: 113.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,671]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,673]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:48669 in memory (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,674]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 0 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,675]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:48669 in memory (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,677]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:47:19,678]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-20 14:47:19,679]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:48669 in memory (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,685]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,686]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:48669 in memory (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,687]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:48669 in memory (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:19,688]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:57,143]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=59115, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:57,159]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:57,775]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=97603, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:57,790]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:57,828]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:47:58,033]  INFO {org.apache.spark.SparkContext} -  Created broadcast 32 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:47:58,939]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:47:59,321]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:47:59,357]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 18 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:59,359]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 25(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:59,362]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:59,440]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:59,491]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 25 (MapPartitionsRDD[42] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:47:59,719]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=101692, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:47:59,769]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:48:00,184]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1900) called with curMem=104932, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:48:00,201]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33_piece0 stored as bytes in memory (estimated size 1900.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:48:00,237]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:48669 (size: 1900.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:48:00,296]  INFO {org.apache.spark.SparkContext} -  Created broadcast 33 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:48:00,308]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[42] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:48:00,312]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 25.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:48:00,367]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 25.0 (TID 25, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:48:00,388]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 25.0 (TID 25) {org.apache.spark.executor.Executor}
[2016-06-20 14:48:00,527]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466426875873:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:48:01,065]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 25.0 (TID 25). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:48:01,678]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 25 (first at MLUtils.java:91) finished in 1.325 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:48:01,693]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 25.0 (TID 25) in 1304 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:48:01,717]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 25.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:48:01,722]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 18 finished: first at MLUtils.java:91, took 2.372527 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:49:21,620]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=106832, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:49:21,636]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:49:22,323]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=145320, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:49:22,377]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:49:22,471]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_34_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:49:22,732]  INFO {org.apache.spark.SparkContext} -  Created broadcast 34 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 14:49:30,131]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 44 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:49:30,244]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 44 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:49:31,670]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 52 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:49:31,719]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 52 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:17,956]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:50:17,963]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 180045 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-20 14:50:17,976] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 180045 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:17,976]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:17,976]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:50:17,989]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 48669) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:50:17,989]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:50:17,989]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:17,990]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-20 14:50:17,991]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 19 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:17,991]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 26(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:17,991]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,011]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,011]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 26 (MapPartitionsRDD[55] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,014]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=149409, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,014]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_35 stored as values in memory (estimated size 7.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,018]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3667) called with curMem=157017, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,018]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,023]  INFO {org.apache.spark.storage.BlockManager} -  Got told to re-register updating block broadcast_35_piece0 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,027]  INFO {org.apache.spark.SparkContext} -  Created broadcast 35 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,027]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[55] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,027]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 26.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,028]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,028]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:50:18,029]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 26.0 (TID 26, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,029]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:48669 with 983.1 MB RAM, BlockManagerId(driver, localhost, 48669) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 14:50:18,035]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 14:50:18,035]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 18 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,035]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,036]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:48669 (size: 1708.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,036]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:48669 (size: 1392.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,037]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:48669 (size: 1390.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,037]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 26.0 (TID 26) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,039]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:48669 (size: 1900.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,040]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_35_piece0 in memory on localhost:48669 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,042]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,042]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:48669 (size: 1712.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,043]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_34_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,044]  INFO {org.apache.spark.CacheManager} -  Partition rdd_53_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:50:18,045]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:50:18,083]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=160684, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,083]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_53_0 stored as values in memory (estimated size 55.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,084]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_53_0 in memory on localhost:48669 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,088]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 26.0 (TID 26). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,100]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 26 (take at DecisionTreeMetadata.scala:110) finished in 0.072 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,100]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 19 finished: take at DecisionTreeMetadata.scala:110, took 0.109709 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,101]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 26.0 (TID 26) in 71 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,101]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 26.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,103]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,103]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 20 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,103]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 27(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,103]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,104]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,105]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 27 (MapPartitionsRDD[55] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,105]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=217052, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,106]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_36 stored as values in memory (estimated size 7.3 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,110]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3577) called with curMem=224492, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,110]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,110]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_36_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,111]  INFO {org.apache.spark.SparkContext} -  Created broadcast 36 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,111]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,111]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 27.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,112]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 27.0 (TID 27, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,113]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 27.0 (TID 27) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,117]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_53_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,120]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 27.0 (TID 27). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,122]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 27.0 (TID 27) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,122]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 27.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,123]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 27 (count at DecisionTreeMetadata.scala:111) finished in 0.005 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,123]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 20 finished: count at DecisionTreeMetadata.scala:111, took 0.020802 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,128]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 21 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 28(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,132]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,133]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 28 (PartitionwiseSampledRDD[56] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,134]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8176) called with curMem=228069, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,135]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_37 stored as values in memory (estimated size 8.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,138]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3907) called with curMem=236245, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,138]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,138]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_37_piece0 in memory on localhost:48669 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,139]  INFO {org.apache.spark.SparkContext} -  Created broadcast 37 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,139]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 28 (PartitionwiseSampledRDD[56] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,139]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 28.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,140]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 28.0 (TID 28, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,140]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 28.0 (TID 28) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,142]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_53_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,147]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 28.0 (TID 28). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,153]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 28.0 (TID 28) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,153]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 28.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,153]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 28 (collect at DecisionTree.scala:977) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,153]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 21 finished: collect at DecisionTree.scala:977, took 0.024908 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,162]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=240152, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,163]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_38 stored as values in memory (estimated size 624.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,166]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=240776, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,167]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_38_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,167]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_38_piece0 in memory on localhost:48669 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,168]  INFO {org.apache.spark.SparkContext} -  Created broadcast 38 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,178]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 59 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 22 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 30(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 29) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,180]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 29) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,184]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 29 (MapPartitionsRDD[59] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,185]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=240847, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,185]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_39 stored as values in memory (estimated size 18.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,192]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8072) called with curMem=259695, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,192]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_39_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,193]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_39_piece0 in memory on localhost:48669 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,193]  INFO {org.apache.spark.SparkContext} -  Created broadcast 39 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,194]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[59] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,194]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 29.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,194]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 29.0 (TID 29, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,195]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 29.0 (TID 29) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,199]  INFO {org.apache.spark.CacheManager} -  Partition rdd_58_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:50:18,199]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_53_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,211]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=267767, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,211]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_58_0 stored as values in memory (estimated size 64.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,212]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_58_0 in memory on localhost:48669 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,222]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 29.0 (TID 29). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 29.0 (TID 29) in 30 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 29.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 29 (mapPartitions at DecisionTree.scala:613) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 30) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,225]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 30: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 30 (MapPartitionsRDD[61] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,227]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=333735, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,227]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_40 stored as values in memory (estimated size 7.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,230]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3572) called with curMem=341783, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,231]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,231]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_40_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,232]  INFO {org.apache.spark.SparkContext} -  Created broadcast 40 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,232]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,232]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 30.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,233]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 30.0 (TID 30, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,233]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 30.0 (TID 30) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,237]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,237]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,251]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 30.0 (TID 30). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,257]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 30.0 (TID 30) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,258]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 30 (collectAsMap at DecisionTree.scala:642) finished in 0.024 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,258]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 22 finished: collectAsMap at DecisionTree.scala:642, took 0.079993 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,259]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 30.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,260]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=345355, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,260]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_41 stored as values in memory (estimated size 1160.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,263]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=346515, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,264]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_41_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,267]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_41_piece0 in memory on localhost:48669 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,267]  INFO {org.apache.spark.SparkContext} -  Created broadcast 41 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,277]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 62 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 23 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 32(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 31) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,279]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 31) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,281]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 31 (MapPartitionsRDD[62] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,282]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=346628, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,282]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_42 stored as values in memory (estimated size 20.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,286]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8904) called with curMem=367180, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,286]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_42_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,287]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_42_piece0 in memory on localhost:48669 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,287]  INFO {org.apache.spark.SparkContext} -  Created broadcast 42 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[62] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,288]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 31.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,288]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 31.0 (TID 31, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,289]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 31.0 (TID 31) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,292]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_58_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,306]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 31.0 (TID 31). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,308]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 31.0 (TID 31) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 31.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 31 (mapPartitions at DecisionTree.scala:613) finished in 0.018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 32) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,309]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,310]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 32: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,311]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 32 (MapPartitionsRDD[64] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,311]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=376084, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,312]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_43 stored as values in memory (estimated size 8.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,315]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3832) called with curMem=384436, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,316]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,316]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_43_piece0 in memory on localhost:48669 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,316]  INFO {org.apache.spark.SparkContext} -  Created broadcast 43 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,317]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[64] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,317]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 32.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,317]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 32.0 (TID 32, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,318]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 32.0 (TID 32) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,321]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,322]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,342]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 32.0 (TID 32). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,346]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 32.0 (TID 32) in 29 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,346]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 32 (collectAsMap at DecisionTree.scala:642) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,346]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 32.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,346]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 23 finished: collectAsMap at DecisionTree.scala:642, took 0.069003 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,348]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=388268, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,348]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_44 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,352]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=390508, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,352]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_44_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,353]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_44_piece0 in memory on localhost:48669 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,353]  INFO {org.apache.spark.SparkContext} -  Created broadcast 44 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,366]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,369]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 65 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,370]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 24 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,370]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 34(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,370]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 33) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,370]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 33) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 33 (MapPartitionsRDD[65] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,373]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=390717, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,373]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_45 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,377]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10098) called with curMem=414301, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,377]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_45_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,378]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_45_piece0 in memory on localhost:48669 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,378]  INFO {org.apache.spark.SparkContext} -  Created broadcast 45 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[65] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,378]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 33.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,379]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 33.0 (TID 33, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,379]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 33.0 (TID 33) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,383]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_58_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,409]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 33.0 (TID 33). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 33.0 (TID 33) in 32 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 33.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 33 (mapPartitions at DecisionTree.scala:613) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 34) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,413]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 34: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,413]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 34 (MapPartitionsRDD[67] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,414]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=424399, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,414]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_46 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,418]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4052) called with curMem=433359, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,418]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_46_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,419]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_46_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,421]  INFO {org.apache.spark.SparkContext} -  Created broadcast 46 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,422]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[67] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,422]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 34.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,422]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 34.0 (TID 34, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,423]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 34.0 (TID 34) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,426]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,427]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,447]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 34.0 (TID 34). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,453]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 34.0 (TID 34) in 31 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,453]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 34 (collectAsMap at DecisionTree.scala:642) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,453]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 34.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,453]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 24 finished: collectAsMap at DecisionTree.scala:642, took 0.087647 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,455]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=437411, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,456]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_47 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,459]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=439651, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,459]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_47_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,460]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_47_piece0 in memory on localhost:48669 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,460]  INFO {org.apache.spark.SparkContext} -  Created broadcast 47 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,471]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,472]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 68 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,472]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 25 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,472]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 36(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,472]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 35) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,473]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 35) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,476]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 35 (MapPartitionsRDD[68] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,477]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28960) called with curMem=439868, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,478]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_48 stored as values in memory (estimated size 28.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,482]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11788) called with curMem=468828, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,482]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_48_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,482]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_48_piece0 in memory on localhost:48669 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,483]  INFO {org.apache.spark.SparkContext} -  Created broadcast 48 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[68] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,483]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 35.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,484]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 35.0 (TID 35, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,484]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 35.0 (TID 35) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,500]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_58_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,506]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_46_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,511]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_45_piece0 on localhost:48669 in memory (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,513]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 9 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,514]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_44_piece0 on localhost:48669 in memory (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,517]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_43_piece0 on localhost:48669 in memory (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,518]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_42_piece0 on localhost:48669 in memory (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,519]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 8 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,521]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_41_piece0 on localhost:48669 in memory (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,522]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_40_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,524]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_39_piece0 on localhost:48669 in memory (size: 7.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,525]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 7 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,525]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_38_piece0 on localhost:48669 in memory (size: 71.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,527]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_37_piece0 on localhost:48669 in memory (size: 3.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,528]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_36_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,529]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_35_piece0 on localhost:48669 in memory (size: 3.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,531]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_33_piece0 on localhost:48669 in memory (size: 1900.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,532]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_32_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,534]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_31_piece0 on localhost:48669 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,535]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_30_piece0 on localhost:48669 in memory (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,536]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 35.0 (TID 35). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,537]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 6 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,538]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_29_piece0 on localhost:48669 in memory (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,538]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 35.0 (TID 35) in 54 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 35 (mapPartitions at DecisionTree.scala:613) finished in 0.055 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 35.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 36) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,540]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_28_piece0 on localhost:48669 in memory (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,541]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 5 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,541]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 36: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,542]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 36 (MapPartitionsRDD[70] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,542]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,542]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 36 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,543]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 33 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,543]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=250695, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,543]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_49 stored as values in memory (estimated size 8.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,543]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 33 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,543]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,544]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 13 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:18,546]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4006) called with curMem=259655, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,547]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_49_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,547]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_49_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,548]  INFO {org.apache.spark.SparkContext} -  Created broadcast 49 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[70] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,548]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 36.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,549]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 36.0 (TID 36, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,549]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 36.0 (TID 36) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,553]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,553]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,610]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 36.0 (TID 36). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,617]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 36.0 (TID 36) in 69 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 36 (collectAsMap at DecisionTree.scala:642) finished in 0.069 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,617]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 36.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 25 finished: collectAsMap at DecisionTree.scala:642, took 0.145742 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,619]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=263661, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,619]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_50 stored as values in memory (estimated size 1808.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,622]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=265469, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,622]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_50_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,623]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_50_piece0 in memory on localhost:48669 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,623]  INFO {org.apache.spark.SparkContext} -  Created broadcast 50 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,634]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 71 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 26 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 38(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 37) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,636]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 37) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,638]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 37 (MapPartitionsRDD[71] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,640]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=265642, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,640]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_51 stored as values in memory (estimated size 33.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,644]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13180) called with curMem=299546, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,644]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_51_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,645]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_51_piece0 in memory on localhost:48669 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,645]  INFO {org.apache.spark.SparkContext} -  Created broadcast 51 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,645]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[71] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,645]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 37.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,646]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 37.0 (TID 37, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,647]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 37.0 (TID 37) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,651]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_58_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,663]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 37.0 (TID 37). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,666]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 37.0 (TID 37) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,666]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 37.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 37 (mapPartitions at DecisionTree.scala:613) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 38) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,668]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 38: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,668]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 38 (MapPartitionsRDD[73] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,669]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=312726, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,669]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_52 stored as values in memory (estimated size 8.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,673]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4008) called with curMem=321446, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,673]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,674]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_52_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,674]  INFO {org.apache.spark.SparkContext} -  Created broadcast 52 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[73] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,675]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 38.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,675]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 38.0 (TID 38, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,675]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 38.0 (TID 38) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,678]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,679]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:18,697]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 38.0 (TID 38). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,702]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 38.0 (TID 38) in 27 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,702]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 38 (collectAsMap at DecisionTree.scala:642) finished in 0.027 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,702]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 38.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,703]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 26 finished: collectAsMap at DecisionTree.scala:642, took 0.068230 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,703]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 58 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:50:18,704]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 58 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,704]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:50:18,704]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.210595609
  total: 0.753385884
  findSplitsBins: 0.03435884
  findBestSplits: 0.538779836
  chooseSplits: 0.537501983 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:50:18,705]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 53 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:50:18,705]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 53 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,717]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,718]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 27 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,718]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 39(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,718]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 39 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,723]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28664) called with curMem=203118, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,723]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_53 stored as values in memory (estimated size 28.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,726]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10834) called with curMem=231782, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,727]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_53_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,727]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_53_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,727]  INFO {org.apache.spark.SparkContext} -  Created broadcast 53 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,728]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,728]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 39.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,728]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 39.0 (TID 39, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,729]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 39.0 (TID 39) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,732]  INFO {org.apache.spark.CacheManager} -  Partition rdd_74_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:50:18,732]  INFO {org.apache.spark.CacheManager} -  Partition rdd_54_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:50:18,732]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:50:18,755]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=242616, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,755]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_54_0 stored as values in memory (estimated size 29.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,755]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_54_0 in memory on localhost:48669 (size: 29.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,764]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=272312, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,765]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_74_0 stored as values in memory (estimated size 15.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,765]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_74_0 in memory on localhost:48669 (size: 15.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,768]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 39.0 (TID 39). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,771]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 39.0 (TID 39) in 43 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,771]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 39 (take at SparkModelUtils.java:215) finished in 0.043 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,772]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 39.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 27 finished: take at SparkModelUtils.java:215, took 0.054472 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,777]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 28 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 40(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 40 (MapPartitionsRDD[54] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,779]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=288440, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,779]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_54 stored as values in memory (estimated size 7.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,783]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3548) called with curMem=295712, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,783]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_54_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,784]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_54_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,784]  INFO {org.apache.spark.SparkContext} -  Created broadcast 54 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,784]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[54] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,784]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 40.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,785]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 40.0 (TID 40, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,785]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 40.0 (TID 40) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,787]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_54_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,790]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 40.0 (TID 40). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,795]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 40.0 (TID 40) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 40 (take at SparkModelUtils.java:223) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 28 finished: take at SparkModelUtils.java:223, took 0.018487 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,795]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 40.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,798]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,798]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 29 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,798]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 41(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 41 (ParallelCollectionRDD[75] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,800]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=299260, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,800]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_55 stored as values in memory (estimated size 1320.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,803]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=300580, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,803]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_55_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,804]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_55_piece0 in memory on localhost:48669 (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,804]  INFO {org.apache.spark.SparkContext} -  Created broadcast 55 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,804]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 41 (ParallelCollectionRDD[75] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,804]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 41.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,810]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 41.0 (TID 41, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,810]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 41.0 (TID 41) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,815]  INFO {org.apache.spark.CacheManager} -  Partition rdd_75_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:50:18,818]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=301457, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,819]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_75_0 stored as values in memory (estimated size 32.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,819]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_75_0 in memory on localhost:48669 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,822]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 41.0 (TID 41). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,824]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 41.0 (TID 41) in 19 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,824]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 41.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 41 (count at SparkModelUtils.java:241) finished in 0.017 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 29 finished: count at SparkModelUtils.java:241, took 0.026775 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,828]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 30 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 42(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 42 (ParallelCollectionRDD[75] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,830]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=334545, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,831]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_56 stored as values in memory (estimated size 1496.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,834]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=336041, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,834]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_56_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,834]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_56_piece0 in memory on localhost:48669 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,835]  INFO {org.apache.spark.SparkContext} -  Created broadcast 56 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 42 (ParallelCollectionRDD[75] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,835]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 42.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,839]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 42.0 (TID 42, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,839]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 42.0 (TID 42) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,843]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_75_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,846]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 42.0 (TID 42). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,849]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 42.0 (TID 42) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,849]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 42.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,849]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 42 (collect at SparkModelUtils.java:245) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,850]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 30 finished: collect at SparkModelUtils.java:245, took 0.022576 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,851]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 75 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:50:18,851]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 75 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,855]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,856]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 31 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,856]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 43(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,856]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,856]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,857]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 43 (MapPartitionsRDD[76] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,858]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=303883, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,858]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_57 stored as values in memory (estimated size 28.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,863]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10906) called with curMem=332595, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,863]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_57_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,863]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_57_piece0 in memory on localhost:48669 (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,864]  INFO {org.apache.spark.SparkContext} -  Created broadcast 57 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[76] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,864]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 43.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,865]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 43.0 (TID 43, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,865]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 43.0 (TID 43) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,868]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_74_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,871]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 43.0 (TID 43). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,874]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 43.0 (TID 43) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,874]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 43.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 43 (count at SparkModelUtils.java:252) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 31 finished: count at SparkModelUtils.java:252, took 0.018902 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,875]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,876]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 32 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,876]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 44(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,876]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,877]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,877]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 44 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,882]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=343501, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,883]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_58 stored as values in memory (estimated size 27.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,886]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10743) called with curMem=371997, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,886]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_58_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,887]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_58_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,887]  INFO {org.apache.spark.SparkContext} -  Created broadcast 58 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,887]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 44.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,888]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 44.0 (TID 44, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,890]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 44.0 (TID 44) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,895]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_74_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,897]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 44.0 (TID 44). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,900]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 44.0 (TID 44) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,900]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 44.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,902]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 44 (count at SparkModelUtils.java:252) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,903]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 32 finished: count at SparkModelUtils.java:252, took 0.027763 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,903]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 54 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:50:18,904]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 54 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,911]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,911]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 33 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,911]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 45(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,911]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,913]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 45 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28648) called with curMem=353044, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,915]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_59 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,919]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10799) called with curMem=381692, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,919]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_59_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:18,920]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_59_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:18,920]  INFO {org.apache.spark.SparkContext} -  Created broadcast 59 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,920]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[74] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,920]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 45.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,921]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 45.0 (TID 45, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,921]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 45.0 (TID 45) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,924]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_74_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,930]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 45.0 (TID 45). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:18,933]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 45.0 (TID 45) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:18,933]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 45.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:18,934]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 45 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,934]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 33 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.023505 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,935]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 77 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:50:18,936]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 77 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,936]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 74 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:50:18,938]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 74 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:18,946]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-20 14:50:18,946]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 78 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 34 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 47(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 46) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:18,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 46) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,245]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 46 (MapPartitionsRDD[78] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,246]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=376363, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,247]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_60 stored as values in memory (estimated size 2.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,251]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1708) called with curMem=379227, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,251]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_60_piece0 stored as bytes in memory (estimated size 1708.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,252]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_60_piece0 in memory on localhost:48669 (size: 1708.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,252]  INFO {org.apache.spark.SparkContext} -  Created broadcast 60 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:19,252]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[78] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,253]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 46.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,257]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 46.0 (TID 46, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,257]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 46.0 (TID 46) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,259]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_59_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,264]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_58_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,265]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_57_piece0 on localhost:48669 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,266]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_56_piece0 on localhost:48669 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,267]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_55_piece0 on localhost:48669 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,267]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 75 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:19,268]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 75 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:19,268]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_54_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,269]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_53_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,270]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_52_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,271]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_51_piece0 on localhost:48669 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,271]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 11 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:19,272]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_50_piece0 on localhost:48669 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,273]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 46.0 (TID 46). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,273]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_49_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 46.0 (TID 46) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 46.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 46 (map at MulticlassMetrics.scala:47) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 47: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 47 (ShuffledRDD[79] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,279]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=121143, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,279]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_61 stored as values in memory (estimated size 2.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,282]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1394) called with curMem=123439, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,282]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_61_piece0 stored as bytes in memory (estimated size 1394.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,283]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_48_piece0 on localhost:48669 in memory (size: 11.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,283]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_61_piece0 in memory on localhost:48669 (size: 1394.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,283]  INFO {org.apache.spark.SparkContext} -  Created broadcast 61 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:19,283]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 47 (ShuffledRDD[79] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,283]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 47.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,284]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 10 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:19,284]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_47_piece0 on localhost:48669 in memory (size: 217.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,285]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 47.0 (TID 47, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,285]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 47.0 (TID 47) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,288]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:19,288]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:19,289]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 58 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:50:19,290]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 58 {org.apache.spark.ContextCleaner}
[2016-06-20 14:50:19,293]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 47.0 (TID 47). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,295]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 47.0 (TID 47) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,295]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 47.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,296]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 47 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,296]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 34 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.350402 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,327]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-20 14:50:19,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 80 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 35 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 49(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 48) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 48) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,330]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 48 (MapPartitionsRDD[80] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,330]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=93416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,331]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_62 stored as values in memory (estimated size 2.8 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,334]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1710) called with curMem=96288, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,334]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_62_piece0 stored as bytes in memory (estimated size 1710.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,334]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_62_piece0 in memory on localhost:48669 (size: 1710.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,335]  INFO {org.apache.spark.SparkContext} -  Created broadcast 62 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:19,336]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[80] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,336]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 48.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,339]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 48.0 (TID 48, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,339]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 48.0 (TID 48) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,350]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 48.0 (TID 48). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 48.0 (TID 48) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 48.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 48 (map at MulticlassMetrics.scala:57) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,352]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,353]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 49: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,353]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 49 (ShuffledRDD[81] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,353]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=97998, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,354]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_63 stored as values in memory (estimated size 2.3 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,356]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1388) called with curMem=100302, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,357]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_63_piece0 stored as bytes in memory (estimated size 1388.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:19,357]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_63_piece0 in memory on localhost:48669 (size: 1388.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:50:19,357]  INFO {org.apache.spark.SparkContext} -  Created broadcast 63 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:50:19,358]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 49 (ShuffledRDD[81] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,358]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 49.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,358]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 49.0 (TID 49, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,358]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 49.0 (TID 49) {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,361]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:19,361]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:50:19,364]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 49.0 (TID 49). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:50:19,366]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 49.0 (TID 49) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:50:19,366]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 49 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:19,366]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 49.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:50:19,366]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 35 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.039241 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:50:59,128]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=101690, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:50:59,144]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_64 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:00,026]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=140178, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:00,042]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_64_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:00,074]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_64_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:51:00,236]  INFO {org.apache.spark.SparkContext} -  Created broadcast 64 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:51:01,238]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:51:01,632]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:51:01,664]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 36 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:01,666]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 50(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:01,668]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:01,722]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:01,759]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 50 (MapPartitionsRDD[83] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:01,868]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=144267, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:01,882]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_65 stored as values in memory (estimated size 3.2 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:02,370]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1900) called with curMem=147507, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:02,427]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_65_piece0 stored as bytes in memory (estimated size 1900.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:51:02,527]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_65_piece0 in memory on localhost:48669 (size: 1900.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:51:02,619]  INFO {org.apache.spark.SparkContext} -  Created broadcast 65 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:51:02,631]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[83] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:02,634]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 50.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:51:02,680]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 50.0 (TID 50, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:51:02,695]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 50.0 (TID 50) {org.apache.spark.executor.Executor}
[2016-06-20 14:51:02,838]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466427057872:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:51:03,869]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 50.0 (TID 50). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:51:04,895]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 50.0 (TID 50) in 2198 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:51:04,898]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 50 (first at MLUtils.java:91) finished in 2.219 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:51:04,916]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 50.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:51:04,937]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 36 finished: first at MLUtils.java:91, took 3.284831 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:52:22,509]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=149407, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:52:22,526]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_66 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:52:23,411]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=187895, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:52:23,427]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_66_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:52:23,461]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_66_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:52:23,627]  INFO {org.apache.spark.SparkContext} -  Created broadcast 66 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 14:52:31,856]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 85 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:52:31,970]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 85 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:52:33,526]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 93 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:52:33,576]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 93 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,030]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:55:52,045]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 37 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 51(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,051]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,052]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 51 (MapPartitionsRDD[96] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,052]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=191984, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,052]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_67 stored as values in memory (estimated size 7.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,054]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3672) called with curMem=199592, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,054]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_67_piece0 stored as bytes in memory (estimated size 3.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,058]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_67_piece0 in memory on localhost:48669 (size: 3.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,058]  INFO {org.apache.spark.SparkContext} -  Created broadcast 67 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,059]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[96] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,059]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 51.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,060]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 51.0 (TID 51, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,090]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 51.0 (TID 51) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,093]  INFO {org.apache.spark.CacheManager} -  Partition rdd_94_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:55:52,094]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:55:52,134]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=203264, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,134]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_94_0 stored as values in memory (estimated size 55.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,135]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_94_0 in memory on localhost:48669 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,151]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 51.0 (TID 51). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,156]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 51.0 (TID 51) in 97 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,156]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 51 (take at DecisionTreeMetadata.scala:110) finished in 0.097 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,156]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 51.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,157]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 37 finished: take at DecisionTreeMetadata.scala:110, took 0.110017 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,159]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,159]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 38 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,159]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 52(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,159]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 52 (MapPartitionsRDD[96] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,162]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=259632, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,162]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_68 stored as values in memory (estimated size 7.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,170]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3582) called with curMem=267072, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,171]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_68_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,171]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_68_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,171]  INFO {org.apache.spark.SparkContext} -  Created broadcast 68 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,172]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[96] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,172]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 52.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,173]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 52.0 (TID 52, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,173]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 52.0 (TID 52) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,175]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_94_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,177]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 52.0 (TID 52). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,181]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 52.0 (TID 52) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,181]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 52 (count at DecisionTreeMetadata.scala:111) finished in 0.003 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,181]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 52.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,181]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 38 finished: count at DecisionTreeMetadata.scala:111, took 0.022644 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,185]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,186]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 39 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,186]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 53(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,186]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,186]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,187]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 53 (PartitionwiseSampledRDD[97] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,188]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8176) called with curMem=270654, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,189]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_69 stored as values in memory (estimated size 8.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,191]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3915) called with curMem=278830, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,192]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_69_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,192]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_69_piece0 in memory on localhost:48669 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,192]  INFO {org.apache.spark.SparkContext} -  Created broadcast 69 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,193]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 53 (PartitionwiseSampledRDD[97] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,193]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 53.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,194]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 53.0 (TID 53, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,194]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 53.0 (TID 53) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,195]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_94_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,199]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 53.0 (TID 53). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 53 (collect at DecisionTree.scala:977) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 39 finished: collect at DecisionTree.scala:977, took 0.020887 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,214]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 53.0 (TID 53) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,214]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 53.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,217]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=282745, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,218]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_70 stored as values in memory (estimated size 624.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,230]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=283369, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,231]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_70_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,231]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_70_piece0 in memory on localhost:48669 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,231]  INFO {org.apache.spark.SparkContext} -  Created broadcast 70 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,247]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,247]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 100 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 40 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 55(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 54) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 54) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 54 (MapPartitionsRDD[100] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,252]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=283440, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,252]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_71 stored as values in memory (estimated size 18.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,256]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8084) called with curMem=302288, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,256]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_71_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,256]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_71_piece0 in memory on localhost:48669 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,257]  INFO {org.apache.spark.SparkContext} -  Created broadcast 71 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[100] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,260]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 54.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,261]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 54.0 (TID 54, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,261]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 54.0 (TID 54) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,264]  INFO {org.apache.spark.CacheManager} -  Partition rdd_99_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:55:52,264]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_94_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,277]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=310372, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,277]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_99_0 stored as values in memory (estimated size 64.4 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,277]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_99_0 in memory on localhost:48669 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,286]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 54.0 (TID 54). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 54.0 (TID 54) in 29 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 54 (mapPartitions at DecisionTree.scala:613) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 54.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 55) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 55: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 55 (MapPartitionsRDD[102] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,291]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=376340, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,291]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_72 stored as values in memory (estimated size 7.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,294]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=384388, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,294]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_72_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,294]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_72_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,295]  INFO {org.apache.spark.SparkContext} -  Created broadcast 72 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[102] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,295]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 55.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,296]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 55.0 (TID 55, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,296]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 55.0 (TID 55) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,298]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,299]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,312]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 55.0 (TID 55). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,318]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 55.0 (TID 55) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,318]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 55.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 55 (collectAsMap at DecisionTree.scala:642) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 40 finished: collectAsMap at DecisionTree.scala:642, took 0.071325 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,320]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=387957, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,320]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_73 stored as values in memory (estimated size 1160.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,330]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=389117, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,330]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_73_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,330]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_73_piece0 in memory on localhost:48669 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,331]  INFO {org.apache.spark.SparkContext} -  Created broadcast 73 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,339]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 103 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 41 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 57(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 56) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 56) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,341]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 56 (MapPartitionsRDD[103] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,342]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=389230, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,342]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_74 stored as values in memory (estimated size 20.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,345]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8920) called with curMem=409782, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,346]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_74_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,346]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_74_piece0 in memory on localhost:48669 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,346]  INFO {org.apache.spark.SparkContext} -  Created broadcast 74 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,347]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[103] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,347]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 56.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,347]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 56.0 (TID 56, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,348]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 56.0 (TID 56) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,350]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_99_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,360]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 56.0 (TID 56). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,363]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 56.0 (TID 56) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,363]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 56.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 56 (mapPartitions at DecisionTree.scala:613) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 57: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 57 (MapPartitionsRDD[105] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,365]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=418702, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,365]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_75 stored as values in memory (estimated size 8.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,368]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3831) called with curMem=427054, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,368]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_75_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,369]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_75_piece0 in memory on localhost:48669 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,369]  INFO {org.apache.spark.SparkContext} -  Created broadcast 75 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,369]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[105] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,369]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 57.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,370]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 57.0 (TID 57, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,370]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 57.0 (TID 57) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,373]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,373]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,388]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 57.0 (TID 57). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,393]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 57.0 (TID 57) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,393]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 57.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,394]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 57 (collectAsMap at DecisionTree.scala:642) finished in 0.024 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,394]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 41 finished: collectAsMap at DecisionTree.scala:642, took 0.054703 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,396]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=430885, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,396]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_76 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,399]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=433125, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,399]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_76_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,399]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_76_piece0 in memory on localhost:48669 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,400]  INFO {org.apache.spark.SparkContext} -  Created broadcast 76 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,409]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,410]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 106 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,410]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 42 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,410]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 59(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,410]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 58) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 58) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 58 (MapPartitionsRDD[106] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,413]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=433334, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,413]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_77 stored as values in memory (estimated size 23.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,416]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10109) called with curMem=456918, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,416]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_77_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,416]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_77_piece0 in memory on localhost:48669 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,417]  INFO {org.apache.spark.SparkContext} -  Created broadcast 77 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[106] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,417]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 58.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,418]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 58.0 (TID 58, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,418]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 58.0 (TID 58) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,420]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_99_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,429]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 58.0 (TID 58). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 58.0 (TID 58) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 58.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 58 (mapPartitions at DecisionTree.scala:613) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 59: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 59 (MapPartitionsRDD[108] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,434]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=467027, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,434]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_78 stored as values in memory (estimated size 8.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,437]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4052) called with curMem=475987, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,437]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_78_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,437]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_78_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,438]  INFO {org.apache.spark.SparkContext} -  Created broadcast 78 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,438]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[108] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,438]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 59.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,438]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 59.0 (TID 59, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,439]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 59.0 (TID 59) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,442]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,442]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,462]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 59.0 (TID 59). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,467]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 59.0 (TID 59) in 28 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,467]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 59 (collectAsMap at DecisionTree.scala:642) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,467]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 42 finished: collectAsMap at DecisionTree.scala:642, took 0.057926 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,467]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 59.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,469]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=480039, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,469]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_79 stored as values in memory (estimated size 2.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,472]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=482279, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,472]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_79_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,472]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_79_piece0 in memory on localhost:48669 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,473]  INFO {org.apache.spark.SparkContext} -  Created broadcast 79 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,482]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 109 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 43 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 61(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,484]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,484]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 60 (MapPartitionsRDD[109] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,486]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28960) called with curMem=482496, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,486]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_80 stored as values in memory (estimated size 28.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,489]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11800) called with curMem=511456, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,489]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_80_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,489]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_80_piece0 in memory on localhost:48669 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,490]  INFO {org.apache.spark.SparkContext} -  Created broadcast 80 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,490]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[109] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,490]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 60.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,491]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 60.0 (TID 60, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,491]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 60.0 (TID 60) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,494]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_99_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,504]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 60.0 (TID 60). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,506]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 60.0 (TID 60) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,506]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 60.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 60 (mapPartitions at DecisionTree.scala:613) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 61) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 61: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,508]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 61 (MapPartitionsRDD[111] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,508]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=523256, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,509]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_81 stored as values in memory (estimated size 8.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,511]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4006) called with curMem=532216, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,511]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_81_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,512]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_81_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,512]  INFO {org.apache.spark.SparkContext} -  Created broadcast 81 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,512]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[111] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,512]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 61.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,513]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 61.0 (TID 61, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,513]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 61.0 (TID 61) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,516]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,516]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,534]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 61.0 (TID 61). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,539]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 61.0 (TID 61) in 26 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,539]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 61.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 61 (collectAsMap at DecisionTree.scala:642) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,540]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 43 finished: collectAsMap at DecisionTree.scala:642, took 0.057331 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,541]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=536222, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,541]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_82 stored as values in memory (estimated size 1808.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,544]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=538030, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,544]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_82_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,544]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_82_piece0 in memory on localhost:48669 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,545]  INFO {org.apache.spark.SparkContext} -  Created broadcast 82 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,554]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,555]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 112 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,555]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 44 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,555]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 63(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,555]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 62) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,556]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 62) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 62 (MapPartitionsRDD[112] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,559]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=538203, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,559]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_83 stored as values in memory (estimated size 33.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,562]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13194) called with curMem=572107, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,562]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_83_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,562]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_83_piece0 in memory on localhost:48669 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,563]  INFO {org.apache.spark.SparkContext} -  Created broadcast 83 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[112] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,563]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 62.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,564]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 62.0 (TID 62, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,564]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 62.0 (TID 62) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,567]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_99_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,577]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 62.0 (TID 62). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,579]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 62.0 (TID 62) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,579]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 62.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,579]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 62 (mapPartitions at DecisionTree.scala:613) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 63: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,580]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 63 (MapPartitionsRDD[114] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,581]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=585301, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,581]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_84 stored as values in memory (estimated size 8.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,584]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4013) called with curMem=594021, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,584]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_84_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,584]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_84_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,585]  INFO {org.apache.spark.SparkContext} -  Created broadcast 84 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,585]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[114] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,585]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 63.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,585]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 63.0 (TID 63, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,586]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 63.0 (TID 63) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,588]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,588]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,603]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 63.0 (TID 63). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,608]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 63 (collectAsMap at DecisionTree.scala:642) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,609]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 44 finished: collectAsMap at DecisionTree.scala:642, took 0.054133 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,609]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 99 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:55:52,609]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 63.0 (TID 63) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,610]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 63.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,611]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 99 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,613]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:55:52,614]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.189073471
  total: 0.58614552
  findSplitsBins: 0.027746311
  findBestSplits: 0.390372269
  chooseSplits: 0.389321669 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:55:52,614]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 94 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:55:52,614]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 94 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,622]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 45 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 64(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,625]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,626]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 64 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,627]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28664) called with curMem=475698, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,627]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_85 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,631]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10838) called with curMem=504362, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,631]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_85_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,631]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_85_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,632]  INFO {org.apache.spark.SparkContext} -  Created broadcast 85 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,632]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,632]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 64.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,633]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 64.0 (TID 64, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,634]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 64.0 (TID 64) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,637]  INFO {org.apache.spark.CacheManager} -  Partition rdd_115_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:55:52,637]  INFO {org.apache.spark.CacheManager} -  Partition rdd_95_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:55:52,637]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:55:52,660]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=515200, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,660]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_95_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,660]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_95_0 in memory on localhost:48669 (size: 29.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,668]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=544896, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,668]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_115_0 stored as values in memory (estimated size 15.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,668]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_115_0 in memory on localhost:48669 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,670]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 64.0 (TID 64). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,672]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 64.0 (TID 64) in 40 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,672]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 64.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,672]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 64 (take at SparkModelUtils.java:215) finished in 0.040 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,673]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 45 finished: take at SparkModelUtils.java:215, took 0.050564 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,676]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 46 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 65(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,678]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 65 (MapPartitionsRDD[95] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,678]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=561024, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,678]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_86 stored as values in memory (estimated size 7.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,680]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3551) called with curMem=568296, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,680]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_86_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,681]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_86_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,681]  INFO {org.apache.spark.SparkContext} -  Created broadcast 86 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,681]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[95] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,681]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 65.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,682]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 65.0 (TID 65, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,682]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 65.0 (TID 65) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,684]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_95_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,686]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 65.0 (TID 65). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,691]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 65.0 (TID 65) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,691]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 65.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,691]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 65 (take at SparkModelUtils.java:223) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,691]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 46 finished: take at SparkModelUtils.java:223, took 0.014557 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,694]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 47 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 66(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,696]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 66 (ParallelCollectionRDD[116] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,696]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=571847, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,697]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_87 stored as values in memory (estimated size 1320.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,699]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=573167, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,699]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_87_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,700]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_87_piece0 in memory on localhost:48669 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,700]  INFO {org.apache.spark.SparkContext} -  Created broadcast 87 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,700]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 66 (ParallelCollectionRDD[116] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,700]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 66.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,704]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 66.0 (TID 66, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,704]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 66.0 (TID 66) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,708]  INFO {org.apache.spark.CacheManager} -  Partition rdd_116_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:55:52,712]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=574044, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,712]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_116_0 stored as values in memory (estimated size 32.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,712]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_116_0 in memory on localhost:48669 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,714]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 66.0 (TID 66). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,716]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 66.0 (TID 66) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,717]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 66.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 66 (count at SparkModelUtils.java:241) finished in 0.017 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 47 finished: count at SparkModelUtils.java:241, took 0.022688 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,720]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 48 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 67(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,722]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 67 (ParallelCollectionRDD[116] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,723]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=607132, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,723]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_88 stored as values in memory (estimated size 1496.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,726]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=608628, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,726]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_88_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,726]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_88_piece0 in memory on localhost:48669 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,727]  INFO {org.apache.spark.SparkContext} -  Created broadcast 88 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,727]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 67 (ParallelCollectionRDD[116] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,727]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 67.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,730]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 67.0 (TID 67, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,730]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 67.0 (TID 67) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,733]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_116_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,737]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 67.0 (TID 67). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,740]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 67.0 (TID 67) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,740]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 67.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,740]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 67 (collect at SparkModelUtils.java:245) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,740]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 48 finished: collect at SparkModelUtils.java:245, took 0.019913 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,741]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 116 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:55:52,742]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 116 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,744]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,744]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 49 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,744]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 68(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,745]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,745]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,745]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 68 (MapPartitionsRDD[117] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,747]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=576470, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,747]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_89 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,749]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10910) called with curMem=605182, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,750]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_89_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,750]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_89_piece0 in memory on localhost:48669 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,752]  INFO {org.apache.spark.SparkContext} -  Created broadcast 89 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,752]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[117] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,752]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 68.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,753]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 68.0 (TID 68, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,753]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 68.0 (TID 68) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,755]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_115_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,757]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 68.0 (TID 68). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,759]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 68.0 (TID 68) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,759]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 68.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,759]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 68 (count at SparkModelUtils.java:252) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,759]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 49 finished: count at SparkModelUtils.java:252, took 0.015378 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,761]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 50 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 69(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,763]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,763]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 69 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,765]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=616092, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,765]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_90 stored as values in memory (estimated size 27.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,767]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10747) called with curMem=644588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,768]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_90_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,768]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_90_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,768]  INFO {org.apache.spark.SparkContext} -  Created broadcast 90 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,768]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,768]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 69.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,769]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 69.0 (TID 69, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,769]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 69.0 (TID 69) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,771]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_115_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,774]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 69.0 (TID 69). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,776]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 69.0 (TID 69) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,776]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 69.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,776]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 69 (count at SparkModelUtils.java:252) finished in 0.004 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,776]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 50 finished: count at SparkModelUtils.java:252, took 0.015372 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,776]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 95 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:55:52,777]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 95 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,783]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,784]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 51 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,784]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 70(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,784]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,785]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,785]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 70 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,786]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28648) called with curMem=625639, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,786]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_91 stored as values in memory (estimated size 28.0 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,789]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10803) called with curMem=654287, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,789]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_91_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,790]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_91_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,790]  INFO {org.apache.spark.SparkContext} -  Created broadcast 91 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[115] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,790]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 70.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,791]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 70.0 (TID 70, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,791]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 70.0 (TID 70) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,793]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_115_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,797]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 70.0 (TID 70). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,799]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 70.0 (TID 70) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,799]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 70.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 70 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 51 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.017083 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,801]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 118 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:55:52,801]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 118 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,802]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 115 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:55:52,802]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 115 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:55:52,808]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 119 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 52 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 72(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 71) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 71) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,811]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 71 (MapPartitionsRDD[119] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,811]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=648962, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,811]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_92 stored as values in memory (estimated size 2.8 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,814]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1709) called with curMem=651826, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,814]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_92_piece0 stored as bytes in memory (estimated size 1709.0 B, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,815]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_92_piece0 in memory on localhost:48669 (size: 1709.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,817]  INFO {org.apache.spark.SparkContext} -  Created broadcast 92 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,817]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 71 (MapPartitionsRDD[119] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,817]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 71.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,819]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 71.0 (TID 71, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,820]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 71.0 (TID 71) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,830]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 71.0 (TID 71). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,831]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 71.0 (TID 71) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 71.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 71 (map at MulticlassMetrics.scala:47) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 72) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 72: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 72 (ShuffledRDD[120] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,833]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=653535, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,833]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_93 stored as values in memory (estimated size 2.2 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,835]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1391) called with curMem=655831, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,836]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_93_piece0 stored as bytes in memory (estimated size 1391.0 B, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,836]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_93_piece0 in memory on localhost:48669 (size: 1391.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,836]  INFO {org.apache.spark.SparkContext} -  Created broadcast 93 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,837]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 72 (ShuffledRDD[120] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,837]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 72.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,837]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 72.0 (TID 72, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,838]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 72.0 (TID 72) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,841]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,841]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,844]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 72.0 (TID 72). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,846]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 72.0 (TID 72) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,846]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 72 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,846]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 72.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,846]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 52 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.037712 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,853]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 121 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 53 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 74(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,854]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,855]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 73 (MapPartitionsRDD[121] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,855]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=657222, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,855]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_94 stored as values in memory (estimated size 2.8 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,858]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1711) called with curMem=660094, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,858]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_94_piece0 stored as bytes in memory (estimated size 1711.0 B, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,859]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_94_piece0 in memory on localhost:48669 (size: 1711.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,859]  INFO {org.apache.spark.SparkContext} -  Created broadcast 94 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,859]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 73 (MapPartitionsRDD[121] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,859]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 73.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,862]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 73.0 (TID 73, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,862]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 73.0 (TID 73) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,871]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 73.0 (TID 73). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,873]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 73.0 (TID 73) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,873]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 73.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,873]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 73 (map at MulticlassMetrics.scala:57) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 74) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 74: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 74 (ShuffledRDD[122] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,875]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=661805, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,875]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_95 stored as values in memory (estimated size 2.3 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,878]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=664109, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,878]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_95_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:55:52,879]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_95_piece0 in memory on localhost:48669 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:55:52,879]  INFO {org.apache.spark.SparkContext} -  Created broadcast 95 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:55:52,879]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 74 (ShuffledRDD[122] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,879]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 74.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,880]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 74.0 (TID 74, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,880]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 74.0 (TID 74) {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,881]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,881]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:55:52,885]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 74.0 (TID 74). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:55:52,887]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 74.0 (TID 74) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:55:52,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 74 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:55:52,887]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 74.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:55:52,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 53 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.033924 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:17,755]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=665499, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:17,770]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_96 stored as values in memory (estimated size 37.6 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:18,674]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=703987, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:18,689]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_96_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:18,722]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_96_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:56:18,883]  INFO {org.apache.spark.SparkContext} -  Created broadcast 96 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:56:19,860]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:56:20,244]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 14:56:20,276]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 54 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:20,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 75(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:20,280]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:20,338]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:20,385]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 75 (MapPartitionsRDD[124] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:20,483]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=708076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:20,497]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_97 stored as values in memory (estimated size 3.2 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:20,832]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1902) called with curMem=711316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:20,849]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_97_piece0 stored as bytes in memory (estimated size 1902.0 B, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:56:20,937]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_97_piece0 in memory on localhost:48669 (size: 1902.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:56:21,132]  INFO {org.apache.spark.SparkContext} -  Created broadcast 97 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:56:21,171]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[124] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:21,180]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 75.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:56:21,298]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 75.0 (TID 75, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:56:21,317]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 75.0 (TID 75) {org.apache.spark.executor.Executor}
[2016-06-20 14:56:21,464]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466427376461:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:56:21,892]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 75.0 (TID 75). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:56:22,425]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 75.0 (TID 75) in 1115 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:56:22,426]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 75 (first at MLUtils.java:91) finished in 1.152 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:56:22,452]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 75.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:56:22,484]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 54 finished: first at MLUtils.java:91, took 2.210484 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:57:37,562]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_72_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:37,913]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_97_piece0 on localhost:48669 in memory (size: 1902.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:38,244]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_96_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:38,643]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_95_piece0 on localhost:48669 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:39,112]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_94_piece0 on localhost:48669 in memory (size: 1711.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:39,261]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 20 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:39,559]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_93_piece0 on localhost:48669 in memory (size: 1391.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:39,882]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_92_piece0 on localhost:48669 in memory (size: 1709.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:40,026]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 19 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:40,214]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 118 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:57:40,321]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 118 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:40,696]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_91_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:41,055]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_90_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:41,381]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_89_piece0 on localhost:48669 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:41,699]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_88_piece0 on localhost:48669 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:42,041]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_87_piece0 on localhost:48669 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:42,319]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 116 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:57:42,420]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 116 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:42,720]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_86_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:43,030]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_85_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:43,206]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 115 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:57:43,276]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 115 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:43,535]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_84_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:44,004]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_83_piece0 on localhost:48669 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:44,257]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 18 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:44,648]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_82_piece0 on localhost:48669 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:45,066]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_81_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:45,476]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_80_piece0 on localhost:48669 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:45,711]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 17 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:46,237]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_79_piece0 on localhost:48669 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:46,712]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_78_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:47,016]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_77_piece0 on localhost:48669 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:47,153]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 16 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:47,457]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_71_piece0 on localhost:48669 in memory (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:47,650]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 14 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:48,006]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_70_piece0 on localhost:48669 in memory (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:48,200]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 99 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:57:48,256]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 99 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:48,528]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_69_piece0 on localhost:48669 in memory (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:48,830]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_68_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:49,122]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_67_piece0 on localhost:48669 in memory (size: 3.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:49,527]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_65_piece0 on localhost:48669 in memory (size: 1900.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:49,943]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_64_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:50,238]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_63_piece0 on localhost:48669 in memory (size: 1388.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:50,523]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_62_piece0 on localhost:48669 in memory (size: 1710.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:50,653]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 13 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:50,933]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_61_piece0 on localhost:48669 in memory (size: 1394.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:51,308]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_60_piece0 on localhost:48669 in memory (size: 1708.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:51,741]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_76_piece0 on localhost:48669 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:52,046]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_75_piece0 on localhost:48669 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:52,354]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_74_piece0 on localhost:48669 in memory (size: 8.7 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:52,491]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 15 {org.apache.spark.ContextCleaner}
[2016-06-20 14:57:52,778]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_73_piece0 on localhost:48669 in memory (size: 113.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:53,931]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=127731, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:57:53,946]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_98 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:57:54,824]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=166219, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:57:54,839]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_98_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:57:54,873]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_98_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:57:55,033]  INFO {org.apache.spark.SparkContext} -  Created broadcast 98 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 14:58:01,344]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 126 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:01,454]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 126 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:03,699]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 134 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:03,746]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 134 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:20,789]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 14:58:20,795]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 55 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 76(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,798]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,798]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 76 (MapPartitionsRDD[137] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,799]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=170308, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,799]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_99 stored as values in memory (estimated size 7.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,801]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3673) called with curMem=177916, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,801]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_99_piece0 stored as bytes in memory (estimated size 3.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,802]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_99_piece0 in memory on localhost:48669 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:20,802]  INFO {org.apache.spark.SparkContext} -  Created broadcast 99 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,802]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[137] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,802]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 76.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:20,803]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 76.0 (TID 76, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:20,865]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 76.0 (TID 76) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:20,866]  INFO {org.apache.spark.CacheManager} -  Partition rdd_135_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:58:20,866]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:58:20,945]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=181589, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,945]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_135_0 stored as values in memory (estimated size 55.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,945]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_135_0 in memory on localhost:48669 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:20,959]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 76.0 (TID 76). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:20,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 76 (take at DecisionTreeMetadata.scala:110) finished in 0.161 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 55 finished: take at DecisionTreeMetadata.scala:110, took 0.168870 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,965]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 76.0 (TID 76) in 161 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:20,965]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 76.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:20,966]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,966]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 56 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,966]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 77(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,966]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,967]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,967]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 77 (MapPartitionsRDD[137] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,968]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7448) called with curMem=237957, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,968]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_100 stored as values in memory (estimated size 7.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,971]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3583) called with curMem=245405, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,971]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_100_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,972]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_100_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:20,972]  INFO {org.apache.spark.SparkContext} -  Created broadcast 100 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,972]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[137] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,972]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 77.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:20,973]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 77.0 (TID 77, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:20,973]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 77.0 (TID 77) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:20,975]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_135_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:20,976]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 77.0 (TID 77). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:20,978]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 77.0 (TID 77) in 5 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:20,978]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 77.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:20,979]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 77 (count at DecisionTreeMetadata.scala:111) finished in 0.003 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,979]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 56 finished: count at DecisionTreeMetadata.scala:111, took 0.013187 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,983]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,984]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 57 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,984]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 78(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,984]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,985]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,985]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 78 (PartitionwiseSampledRDD[138] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,986]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8176) called with curMem=248988, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,986]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_101 stored as values in memory (estimated size 8.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,989]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3915) called with curMem=257164, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,989]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_101_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:20,990]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_101_piece0 in memory on localhost:48669 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:20,990]  INFO {org.apache.spark.SparkContext} -  Created broadcast 101 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:20,990]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 78 (PartitionwiseSampledRDD[138] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:20,991]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 78.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:20,991]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 78.0 (TID 78, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:20,991]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 78.0 (TID 78) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:20,993]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_135_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:20,996]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 78.0 (TID 78). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,001]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 78.0 (TID 78) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,001]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 78.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 78 (collect at DecisionTree.scala:977) finished in 0.001 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 57 finished: collect at DecisionTree.scala:977, took 0.018601 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,007]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=261079, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,007]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_102 stored as values in memory (estimated size 624.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,009]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=261703, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,009]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_102_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,010]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_102_piece0 in memory on localhost:48669 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,015]  INFO {org.apache.spark.SparkContext} -  Created broadcast 102 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,033]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 141 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 58 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 80(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 79) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 79) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,036]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 79 (MapPartitionsRDD[141] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,037]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=261774, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,037]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_103 stored as values in memory (estimated size 18.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,039]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8085) called with curMem=280622, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,039]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_103_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,040]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_103_piece0 in memory on localhost:48669 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,040]  INFO {org.apache.spark.SparkContext} -  Created broadcast 103 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,040]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[141] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,040]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 79.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,041]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 79.0 (TID 79, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,041]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 79.0 (TID 79) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,045]  INFO {org.apache.spark.CacheManager} -  Partition rdd_140_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:58:21,045]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_135_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,073]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=288707, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,073]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_140_0 stored as values in memory (estimated size 64.4 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,077]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_140_0 in memory on localhost:48669 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,098]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 79.0 (TID 79). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 79.0 (TID 79) in 60 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 79.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 79 (mapPartitions at DecisionTree.scala:613) finished in 0.052 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 80) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,101]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,102]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 80: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,102]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 80 (MapPartitionsRDD[143] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,103]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=354675, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,103]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_104 stored as values in memory (estimated size 7.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,105]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=362723, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,105]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_104_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,105]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_104_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,106]  INFO {org.apache.spark.SparkContext} -  Created broadcast 104 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,106]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[143] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,106]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 80.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,106]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 80.0 (TID 80, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,107]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 80.0 (TID 80) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,109]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,109]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,134]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 80.0 (TID 80). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,139]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 80 (collectAsMap at DecisionTree.scala:642) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,139]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 58 finished: collectAsMap at DecisionTree.scala:642, took 0.106200 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,141]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=366292, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,141]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_105 stored as values in memory (estimated size 1160.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,143]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=367452, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,144]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_105_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,144]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 80.0 (TID 80) in 33 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,144]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 80.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,144]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_105_piece0 in memory on localhost:48669 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,144]  INFO {org.apache.spark.SparkContext} -  Created broadcast 105 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,160]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 144 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 59 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 82(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 81) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,161]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 81) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 81 (MapPartitionsRDD[144] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,163]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20560) called with curMem=367565, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,163]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_106 stored as values in memory (estimated size 20.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,165]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8921) called with curMem=388125, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,166]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_106_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,166]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_106_piece0 in memory on localhost:48669 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,166]  INFO {org.apache.spark.SparkContext} -  Created broadcast 106 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,167]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[144] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,167]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 81.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,167]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 81.0 (TID 81, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,168]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 81.0 (TID 81) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,171]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_140_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,178]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 81.0 (TID 81). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 81.0 (TID 81) in 22 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 81.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 81 (mapPartitions at DecisionTree.scala:613) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 82) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 82: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,190]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 82 (MapPartitionsRDD[146] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,190]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=397046, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,191]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_107 stored as values in memory (estimated size 8.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,193]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3831) called with curMem=405398, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,193]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_107_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,193]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_107_piece0 in memory on localhost:48669 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,193]  INFO {org.apache.spark.SparkContext} -  Created broadcast 107 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,194]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[146] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,194]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 82.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,194]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 82.0 (TID 82, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,194]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 82.0 (TID 82) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,197]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,197]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,236]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 82.0 (TID 82). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,245]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 82 (collectAsMap at DecisionTree.scala:642) finished in 0.047 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,245]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 59 finished: collectAsMap at DecisionTree.scala:642, took 0.085002 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,245]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 82.0 (TID 82) in 51 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,245]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 82.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,247]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=409229, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,247]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_108 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,249]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=411469, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,249]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_108_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,250]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_108_piece0 in memory on localhost:48669 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,250]  INFO {org.apache.spark.SparkContext} -  Created broadcast 108 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,258]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,258]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 147 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 60 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 84(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 83) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 83) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 83 (MapPartitionsRDD[147] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,261]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23592) called with curMem=411678, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,261]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_109 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,264]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10111) called with curMem=435270, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,264]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_109_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,264]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_109_piece0 in memory on localhost:48669 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,264]  INFO {org.apache.spark.SparkContext} -  Created broadcast 109 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[147] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,265]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 83.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,265]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 83.0 (TID 83, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,265]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 83.0 (TID 83) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,268]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_140_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,282]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 83.0 (TID 83). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 83.0 (TID 83) in 22 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 83 (mapPartitions at DecisionTree.scala:613) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 83.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 84) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 84: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 84 (MapPartitionsRDD[149] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,289]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=445381, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,290]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_110 stored as values in memory (estimated size 8.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,292]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4048) called with curMem=454341, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,292]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_110_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,293]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_110_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,293]  INFO {org.apache.spark.SparkContext} -  Created broadcast 110 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,293]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[149] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,293]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 84.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,294]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 84.0 (TID 84, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,294]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 84.0 (TID 84) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,299]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,299]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,341]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 84.0 (TID 84). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,346]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 84.0 (TID 84) in 53 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,346]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 84 (collectAsMap at DecisionTree.scala:642) finished in 0.052 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,346]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 84.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,346]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 60 finished: collectAsMap at DecisionTree.scala:642, took 0.088680 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,348]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=458389, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,348]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_111 stored as values in memory (estimated size 2.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,350]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=460629, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,351]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_111_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,352]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_111_piece0 in memory on localhost:48669 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,352]  INFO {org.apache.spark.SparkContext} -  Created broadcast 111 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,369]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 150 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 61 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 86(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 85) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 85) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 85 (MapPartitionsRDD[150] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,373]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28960) called with curMem=460846, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,374]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_112 stored as values in memory (estimated size 28.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,376]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11801) called with curMem=489806, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,377]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_112_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,377]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_112_piece0 in memory on localhost:48669 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,377]  INFO {org.apache.spark.SparkContext} -  Created broadcast 112 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 85 (MapPartitionsRDD[150] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,378]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 85.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,378]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 85.0 (TID 85, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,378]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 85.0 (TID 85) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,382]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_140_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,411]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 85.0 (TID 85). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,414]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 85.0 (TID 85) in 36 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,414]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 85.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,414]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 85 (mapPartitions at DecisionTree.scala:613) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 86) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 86: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 86 (MapPartitionsRDD[152] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,416]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=501607, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,416]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_113 stored as values in memory (estimated size 8.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,418]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4005) called with curMem=510567, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,419]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_113_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,419]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_113_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,419]  INFO {org.apache.spark.SparkContext} -  Created broadcast 113 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,419]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[152] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,419]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 86.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,420]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 86.0 (TID 86, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,421]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 86.0 (TID 86) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,426]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,426]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,471]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 86.0 (TID 86). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,478]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 86 (collectAsMap at DecisionTree.scala:642) finished in 0.057 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,478]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 61 finished: collectAsMap at DecisionTree.scala:642, took 0.108337 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,478]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 86.0 (TID 86) in 57 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,478]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 86.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,479]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=514572, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,480]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_114 stored as values in memory (estimated size 1808.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,482]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=516380, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,482]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_114_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,482]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_114_piece0 in memory on localhost:48669 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,483]  INFO {org.apache.spark.SparkContext} -  Created broadcast 114 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,497]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,504]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 153 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,506]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 62 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,506]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 88(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,506]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 87) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 87) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,508]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 87 (MapPartitionsRDD[153] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,510]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=516553, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,510]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_115 stored as values in memory (estimated size 33.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,512]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13194) called with curMem=550457, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,513]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_115_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,513]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_115_piece0 in memory on localhost:48669 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,514]  INFO {org.apache.spark.SparkContext} -  Created broadcast 115 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,514]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[153] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,514]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 87.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,515]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 87.0 (TID 87, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,516]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 87.0 (TID 87) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,519]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_140_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,532]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 87.0 (TID 87). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 87.0 (TID 87) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 87 (mapPartitions at DecisionTree.scala:613) finished in 0.019 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 87.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 88) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,535]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 88: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,535]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 88 (MapPartitionsRDD[155] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,536]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=563651, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,536]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_116 stored as values in memory (estimated size 8.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,538]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4003) called with curMem=572371, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,538]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_116_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,539]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_116_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,539]  INFO {org.apache.spark.SparkContext} -  Created broadcast 116 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[155] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,539]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 88.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,540]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 88.0 (TID 88, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,540]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 88.0 (TID 88) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,542]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,542]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,568]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 88.0 (TID 88). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,573]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 88.0 (TID 88) in 33 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 88 (collectAsMap at DecisionTree.scala:642) finished in 0.033 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,573]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 88.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,573]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 62 finished: collectAsMap at DecisionTree.scala:642, took 0.076011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,574]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 140 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:21,574]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 140 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,595]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:58:21,595]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.219330989
  total: 0.807930633
  findSplitsBins: 0.024589999
  findBestSplits: 0.565360643
  chooseSplits: 0.563816424 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 14:58:21,595]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 135 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:21,595]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 135 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,602]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,603]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 63 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,603]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 89(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,603]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,604]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,604]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 89 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,606]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28664) called with curMem=454038, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,606]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_117 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,608]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10838) called with curMem=482702, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,609]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_117_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,609]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_117_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,609]  INFO {org.apache.spark.SparkContext} -  Created broadcast 117 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,609]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,609]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 89.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,610]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 89.0 (TID 89, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,610]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 89.0 (TID 89) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,612]  INFO {org.apache.spark.CacheManager} -  Partition rdd_156_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:58:21,614]  INFO {org.apache.spark.CacheManager} -  Partition rdd_136_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:58:21,614]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 14:58:21,663]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=493540, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,663]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_136_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,663]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_136_0 in memory on localhost:48669 (size: 29.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,678]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=523236, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,678]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_156_0 stored as values in memory (estimated size 15.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,678]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_156_0 in memory on localhost:48669 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,680]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 89.0 (TID 89). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,683]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 89.0 (TID 89) in 73 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,683]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 89.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 89 (take at SparkModelUtils.java:215) finished in 0.073 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 63 finished: take at SparkModelUtils.java:215, took 0.080828 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,687]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 64 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 90(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,688]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,688]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 90 (MapPartitionsRDD[136] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,688]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=539364, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,688]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_118 stored as values in memory (estimated size 7.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,691]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3548) called with curMem=546636, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,691]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_118_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,691]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_118_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,691]  INFO {org.apache.spark.SparkContext} -  Created broadcast 118 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,692]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[136] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,692]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 90.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,692]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 90.0 (TID 90, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,707]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 90.0 (TID 90) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,714]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_136_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,716]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 90.0 (TID 90). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 90 (take at SparkModelUtils.java:223) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,721]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 64 finished: take at SparkModelUtils.java:223, took 0.033897 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,722]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 65 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 91(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 91 (ParallelCollectionRDD[157] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,724]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=550184, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,724]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_119 stored as values in memory (estimated size 1320.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,726]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=551504, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,726]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_119_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,726]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_119_piece0 in memory on localhost:48669 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,727]  INFO {org.apache.spark.SparkContext} -  Created broadcast 119 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,727]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 91 (ParallelCollectionRDD[157] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,727]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 91.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,727]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 90.0 (TID 90) in 28 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,727]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 90.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,730]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 91.0 (TID 91, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,730]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 91.0 (TID 91) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,734]  INFO {org.apache.spark.CacheManager} -  Partition rdd_157_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 14:58:21,737]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=552381, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,737]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_157_0 stored as values in memory (estimated size 32.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,738]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_157_0 in memory on localhost:48669 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,748]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 91.0 (TID 91). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,751]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 91.0 (TID 91) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,751]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 91 (count at SparkModelUtils.java:241) finished in 0.021 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,751]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 91.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,751]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 65 finished: count at SparkModelUtils.java:241, took 0.028571 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,753]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,754]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 66 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,754]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 92(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,754]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,754]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,754]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 92 (ParallelCollectionRDD[157] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,755]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=585469, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,755]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_120 stored as values in memory (estimated size 1496.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,757]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=586965, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,757]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_120_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,757]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_120_piece0 in memory on localhost:48669 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,758]  INFO {org.apache.spark.SparkContext} -  Created broadcast 120 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,758]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 92 (ParallelCollectionRDD[157] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,758]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 92.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,761]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 92.0 (TID 92, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,761]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 92.0 (TID 92) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,764]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_157_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,766]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 92.0 (TID 92). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,769]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 92.0 (TID 92) in 11 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,769]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 92 (collect at SparkModelUtils.java:245) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,769]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 92.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,769]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 66 finished: collect at SparkModelUtils.java:245, took 0.015641 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,769]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 157 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:58:21,770]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 157 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,772]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 67 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 93(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,772]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,773]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 93 (MapPartitionsRDD[158] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,774]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=554807, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,774]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_121 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,776]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10912) called with curMem=583519, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,777]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_121_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,777]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_121_piece0 in memory on localhost:48669 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,777]  INFO {org.apache.spark.SparkContext} -  Created broadcast 121 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,777]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[158] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,777]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 93.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,780]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 93.0 (TID 93, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,780]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 93.0 (TID 93) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,783]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_156_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,785]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 93.0 (TID 93). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,786]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 93.0 (TID 93) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,787]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 93.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,787]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 93 (count at SparkModelUtils.java:252) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,787]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 67 finished: count at SparkModelUtils.java:252, took 0.015147 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,788]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 68 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 94(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 94 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,791]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=594431, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,792]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_122 stored as values in memory (estimated size 27.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,794]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10747) called with curMem=622927, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,794]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_122_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,794]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_122_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,795]  INFO {org.apache.spark.SparkContext} -  Created broadcast 122 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,795]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 94.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,795]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 94.0 (TID 94, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,796]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 94.0 (TID 94) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,798]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_156_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,801]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 94.0 (TID 94). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,803]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 94.0 (TID 94) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,803]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 94.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,803]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 94 (count at SparkModelUtils.java:252) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,803]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 68 finished: count at SparkModelUtils.java:252, took 0.014886 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,803]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 136 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:21,804]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 136 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,809]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 69 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 95(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 95 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,811]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28648) called with curMem=603978, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,811]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_123 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,814]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10803) called with curMem=632626, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,814]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_123_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,814]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_123_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,814]  INFO {org.apache.spark.SparkContext} -  Created broadcast 123 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,815]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[156] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,815]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 95.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,816]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 95.0 (TID 95, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,817]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 95.0 (TID 95) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,819]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_156_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,826]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 95.0 (TID 95). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,829]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 95.0 (TID 95) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,829]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 95.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 95 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,830]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 69 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.020464 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,830]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 159 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 14:58:21,833]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 159 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,833]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 156 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 14:58:21,834]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 156 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:58:21,841]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,841]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 160 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,841]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 70 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,841]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 97(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,841]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 96) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 96) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 96 (MapPartitionsRDD[160] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,842]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=627301, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,842]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_124 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,845]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1710) called with curMem=630165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,845]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_124_piece0 stored as bytes in memory (estimated size 1710.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,845]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_124_piece0 in memory on localhost:48669 (size: 1710.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,845]  INFO {org.apache.spark.SparkContext} -  Created broadcast 124 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,846]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[160] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,846]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 96.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,851]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 96.0 (TID 96, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,852]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 96.0 (TID 96) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,861]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 96.0 (TID 96). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 96.0 (TID 96) in 17 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 96 (map at MulticlassMetrics.scala:47) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 96.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 97) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,867]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 97: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,867]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 97 (ShuffledRDD[161] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,867]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=631875, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,868]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_125 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,870]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1394) called with curMem=634171, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,870]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_125_piece0 stored as bytes in memory (estimated size 1394.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,870]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_125_piece0 in memory on localhost:48669 (size: 1394.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,871]  INFO {org.apache.spark.SparkContext} -  Created broadcast 125 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,871]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 97 (ShuffledRDD[161] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,871]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 97.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,871]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 97.0 (TID 97, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,872]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 97.0 (TID 97) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,874]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,874]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,878]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 97.0 (TID 97). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,879]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 97.0 (TID 97) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,880]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 97.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,880]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 97 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,880]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 70 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.039278 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,886]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 162 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 71 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 99(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 98) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 98) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 98 (MapPartitionsRDD[162] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,891]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=635565, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,891]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_126 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,893]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=638437, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,894]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_126_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,894]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_126_piece0 in memory on localhost:48669 (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,894]  INFO {org.apache.spark.SparkContext} -  Created broadcast 126 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,894]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 98 (MapPartitionsRDD[162] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,895]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 98.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,897]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 98.0 (TID 98, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,898]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 98.0 (TID 98) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,924]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 98.0 (TID 98). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 98.0 (TID 98) in 31 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 98 (map at MulticlassMetrics.scala:57) finished in 0.030 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 98.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 99) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,928]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 99: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,928]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 99 (ShuffledRDD[163] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,929]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=640149, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,929]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_127 stored as values in memory (estimated size 2.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,932]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=642453, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,932]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_127_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 14:58:21,933]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_127_piece0 in memory on localhost:48669 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:58:21,933]  INFO {org.apache.spark.SparkContext} -  Created broadcast 127 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 14:58:21,934]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 99 (ShuffledRDD[163] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,934]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 99.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,934]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 99.0 (TID 99, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,934]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 99.0 (TID 99) {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,936]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,936]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 14:58:21,940]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 99.0 (TID 99). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 14:58:21,941]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 99.0 (TID 99) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 14:58:21,941]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 99.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 14:58:21,941]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 99 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.002 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:58:21,942]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 71 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.055792 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 14:59:53,668]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_104_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,670]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_103_piece0 on localhost:48669 in memory (size: 7.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,671]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 21 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,671]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_102_piece0 on localhost:48669 in memory (size: 71.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,675]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 140 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:59:53,679]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 140 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,679]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_101_piece0 on localhost:48669 in memory (size: 3.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,680]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_100_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,685]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_99_piece0 on localhost:48669 in memory (size: 3.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,685]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_127_piece0 on localhost:48669 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,686]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_126_piece0 on localhost:48669 in memory (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,686]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 27 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,687]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_125_piece0 on localhost:48669 in memory (size: 1394.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,688]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_124_piece0 on localhost:48669 in memory (size: 1710.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,694]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 26 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,695]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 159 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:59:53,695]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 159 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,695]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_123_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,696]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_122_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,699]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_121_piece0 on localhost:48669 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,700]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_120_piece0 on localhost:48669 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,701]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_119_piece0 on localhost:48669 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,701]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 157 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:59:53,701]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 157 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,702]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_118_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,702]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_117_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,702]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 156 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:59:53,703]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 156 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,703]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 136 {org.apache.spark.storage.BlockManager}
[2016-06-20 14:59:53,703]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 136 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,704]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_116_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,704]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_115_piece0 on localhost:48669 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,705]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 25 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,711]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_114_piece0 on localhost:48669 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,712]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_113_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,713]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_112_piece0 on localhost:48669 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,713]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 24 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,713]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_111_piece0 on localhost:48669 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,714]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_110_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,715]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_109_piece0 on localhost:48669 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,718]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 23 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,719]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_108_piece0 on localhost:48669 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,720]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_107_piece0 on localhost:48669 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,720]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_106_piece0 on localhost:48669 in memory (size: 8.7 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 14:59:53,721]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 22 {org.apache.spark.ContextCleaner}
[2016-06-20 14:59:53,721]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_105_piece0 on localhost:48669 in memory (size: 113.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:06:05,464]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=170308, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:05,480]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_128 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:06,052]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=208796, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:06,067]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_128_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:06,103]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_128_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:06:06,256]  INFO {org.apache.spark.SparkContext} -  Created broadcast 128 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:06:07,313]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 15:06:08,743]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:06:08,775]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 72 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:08,777]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 100(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:08,779]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:08,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:08,877]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 100 (MapPartitionsRDD[165] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:08,979]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=212885, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:08,994]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_129 stored as values in memory (estimated size 3.2 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:09,324]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1901) called with curMem=216125, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:09,339]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_129_piece0 stored as bytes in memory (estimated size 1901.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:06:09,380]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_129_piece0 in memory on localhost:48669 (size: 1901.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:06:09,541]  INFO {org.apache.spark.SparkContext} -  Created broadcast 129 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:06:09,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[165] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:09,581]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 100.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:06:09,715]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 100.0 (TID 100, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:06:09,744]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 100.0 (TID 100) {org.apache.spark.executor.Executor}
[2016-06-20 15:06:09,949]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466427964209:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:06:10,378]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 100.0 (TID 100). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:06:10,786]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 100 (first at MLUtils.java:91) finished in 1.139 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:06:10,787]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 100.0 (TID 100) in 1103 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:06:10,818]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 100.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:06:10,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 72 finished: first at MLUtils.java:91, took 2.061874 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:34,449]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=218026, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:34,465]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_130 stored as values in memory (estimated size 37.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:35,052]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=256514, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:35,067]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_130_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:35,102]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_130_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:35,266]  INFO {org.apache.spark.SparkContext} -  Created broadcast 130 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 15:07:41,886]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 167 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:41,992]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 167 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:42,614]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 175 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:42,663]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 175 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,090]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 15:07:54,094]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 73 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 101(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,096]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,096]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 101 (MapPartitionsRDD[178] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,097]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=260603, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,097]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_131 stored as values in memory (estimated size 7.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,099]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3677) called with curMem=268211, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,099]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_131_piece0 stored as bytes in memory (estimated size 3.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,100]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_131_piece0 in memory on localhost:48669 (size: 3.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,105]  INFO {org.apache.spark.SparkContext} -  Created broadcast 131 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,106]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[178] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,106]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 101.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,106]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 101.0 (TID 101, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,117]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 101.0 (TID 101) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,119]  INFO {org.apache.spark.CacheManager} -  Partition rdd_176_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:07:54,119]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:07:54,153]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=271888, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,153]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_176_0 stored as values in memory (estimated size 55.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,153]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_176_0 in memory on localhost:48669 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,154]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 101.0 (TID 101). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,160]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 101.0 (TID 101) in 54 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,160]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 101 (take at DecisionTreeMetadata.scala:110) finished in 0.054 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,160]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 101.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,160]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 73 finished: take at DecisionTreeMetadata.scala:110, took 0.066175 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,162]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 74 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 102(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,163]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,163]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 102 (MapPartitionsRDD[178] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,163]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7448) called with curMem=328256, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,164]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_132 stored as values in memory (estimated size 7.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,166]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3587) called with curMem=335704, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,166]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_132_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,167]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_132_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,167]  INFO {org.apache.spark.SparkContext} -  Created broadcast 132 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,167]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[178] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,167]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 102.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,168]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 102.0 (TID 102, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,168]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 102.0 (TID 102) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,170]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_176_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,171]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 102.0 (TID 102). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,173]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 102.0 (TID 102) in 5 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,173]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 102.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,174]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 102 (count at DecisionTreeMetadata.scala:111) finished in 0.006 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,175]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 74 finished: count at DecisionTreeMetadata.scala:111, took 0.012632 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,178]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 75 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 103(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,179]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,180]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 103 (PartitionwiseSampledRDD[179] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,180]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8176) called with curMem=339291, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,181]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_133 stored as values in memory (estimated size 8.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,196]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3917) called with curMem=347467, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,198]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_133_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,199]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_133_piece0 in memory on localhost:48669 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,199]  INFO {org.apache.spark.SparkContext} -  Created broadcast 133 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,199]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 103 (PartitionwiseSampledRDD[179] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,199]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 103.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,200]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 103.0 (TID 103, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,200]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 103.0 (TID 103) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,201]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_176_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,203]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 103.0 (TID 103). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,207]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 103.0 (TID 103) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,207]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 103.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 103 (collect at DecisionTree.scala:977) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 75 finished: collect at DecisionTree.scala:977, took 0.028613 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,215]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=351384, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,215]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_134 stored as values in memory (estimated size 624.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,217]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=352008, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,218]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_134_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,218]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_134_piece0 in memory on localhost:48669 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,218]  INFO {org.apache.spark.SparkContext} -  Created broadcast 134 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,225]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 182 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 76 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 105(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,226]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 104) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,227]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 104) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,228]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 104 (MapPartitionsRDD[182] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,229]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=352079, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,229]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_135 stored as values in memory (estimated size 18.4 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,231]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8088) called with curMem=370927, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,231]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_135_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,231]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_135_piece0 in memory on localhost:48669 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,232]  INFO {org.apache.spark.SparkContext} -  Created broadcast 135 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,232]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 104 (MapPartitionsRDD[182] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,232]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 104.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,233]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 104.0 (TID 104, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,233]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 104.0 (TID 104) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,238]  INFO {org.apache.spark.CacheManager} -  Partition rdd_181_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:07:54,238]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_176_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,246]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=379015, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,246]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_181_0 stored as values in memory (estimated size 64.4 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,247]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_181_0 in memory on localhost:48669 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,253]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 104.0 (TID 104). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 104.0 (TID 104) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 104 (mapPartitions at DecisionTree.scala:613) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 104.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 105) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,255]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 105: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 105 (MapPartitionsRDD[184] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,259]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=444983, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,260]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_136 stored as values in memory (estimated size 7.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,261]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3571) called with curMem=453031, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,262]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_136_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,262]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_136_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,262]  INFO {org.apache.spark.SparkContext} -  Created broadcast 136 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,262]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[184] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,262]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 105.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,263]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 105.0 (TID 105, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,263]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 105.0 (TID 105) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,266]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,266]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,272]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 105.0 (TID 105). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,276]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 105.0 (TID 105) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,277]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 105.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,276]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 105 (collectAsMap at DecisionTree.scala:642) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 76 finished: collectAsMap at DecisionTree.scala:642, took 0.051264 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,278]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=456602, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,278]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_137 stored as values in memory (estimated size 1160.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,280]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=457762, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,280]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_137_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,281]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_137_piece0 in memory on localhost:48669 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,281]  INFO {org.apache.spark.SparkContext} -  Created broadcast 137 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,291]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 185 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 77 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 107(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 106) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 106) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,296]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 106 (MapPartitionsRDD[185] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,297]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20560) called with curMem=457875, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,297]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_138 stored as values in memory (estimated size 20.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,306]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8923) called with curMem=478435, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,310]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_138_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,311]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_138_piece0 in memory on localhost:48669 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,312]  INFO {org.apache.spark.SparkContext} -  Created broadcast 138 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,312]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 106 (MapPartitionsRDD[185] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,312]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 106.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,313]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 106.0 (TID 106, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,313]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 106.0 (TID 106) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,315]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_181_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,324]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 106.0 (TID 106). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,326]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 106.0 (TID 106) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 106.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 106 (mapPartitions at DecisionTree.scala:613) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 107) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 107: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 107 (MapPartitionsRDD[187] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,328]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=487358, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,328]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_139 stored as values in memory (estimated size 8.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,332]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3829) called with curMem=495710, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,332]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_139_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,333]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_139_piece0 in memory on localhost:48669 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,333]  INFO {org.apache.spark.SparkContext} -  Created broadcast 139 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,333]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[187] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,333]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 107.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,334]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 107.0 (TID 107, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,334]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 107.0 (TID 107) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,336]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,336]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,348]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 107.0 (TID 107). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,355]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 107 (collectAsMap at DecisionTree.scala:642) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,356]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 77 finished: collectAsMap at DecisionTree.scala:642, took 0.065158 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,357]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=499539, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,357]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_140 stored as values in memory (estimated size 2.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,357]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 107.0 (TID 107) in 22 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,360]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=501779, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,360]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_140_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,360]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_140_piece0 in memory on localhost:48669 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,361]  INFO {org.apache.spark.SparkContext} -  Created broadcast 140 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,361]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 107.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,372]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 188 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 78 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 109(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 108) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,373]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 108) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 108 (MapPartitionsRDD[188] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,375]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23592) called with curMem=501988, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,375]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_141 stored as values in memory (estimated size 23.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,378]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10114) called with curMem=525580, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,378]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_141_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,379]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_141_piece0 in memory on localhost:48669 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,379]  INFO {org.apache.spark.SparkContext} -  Created broadcast 141 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,379]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[188] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,379]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 108.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,380]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 108.0 (TID 108, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,380]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 108.0 (TID 108) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,383]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_181_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,391]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 108.0 (TID 108). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 108.0 (TID 108) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 108.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 108 (mapPartitions at DecisionTree.scala:613) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 109) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 109: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,394]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 109 (MapPartitionsRDD[190] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,394]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=535694, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,394]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_142 stored as values in memory (estimated size 8.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,397]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4052) called with curMem=544654, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,397]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_142_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,397]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_142_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,398]  INFO {org.apache.spark.SparkContext} -  Created broadcast 142 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,398]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[190] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,398]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 109.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,398]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 109.0 (TID 109, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,411]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 109.0 (TID 109) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,414]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,414]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,428]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 109.0 (TID 109). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,436]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 109.0 (TID 109) in 37 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,436]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 109.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,436]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 109 (collectAsMap at DecisionTree.scala:642) finished in 0.037 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,436]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 78 finished: collectAsMap at DecisionTree.scala:642, took 0.064194 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,437]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=548706, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,438]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_143 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,462]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=550946, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,462]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_143_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,463]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_143_piece0 in memory on localhost:48669 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,463]  INFO {org.apache.spark.SparkContext} -  Created broadcast 143 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,463]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 29 {org.apache.spark.ContextCleaner}
[2016-06-20 15:07:54,464]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_137_piece0 on localhost:48669 in memory (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,554]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_136_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,555]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_135_piece0 on localhost:48669 in memory (size: 7.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,555]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 28 {org.apache.spark.ContextCleaner}
[2016-06-20 15:07:54,556]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_134_piece0 on localhost:48669 in memory (size: 71.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,556]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 191 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 79 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 111(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,558]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,558]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_133_piece0 on localhost:48669 in memory (size: 3.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,559]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 110 (MapPartitionsRDD[191] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,559]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_132_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,560]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28960) called with curMem=487512, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,560]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_144 stored as values in memory (estimated size 28.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,561]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_131_piece0 on localhost:48669 in memory (size: 3.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,563]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11804) called with curMem=500046, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,563]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_144_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,563]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_129_piece0 on localhost:48669 in memory (size: 1901.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,563]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_144_piece0 in memory on localhost:48669 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,564]  INFO {org.apache.spark.SparkContext} -  Created broadcast 144 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,564]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_128_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,564]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[191] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,564]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 110.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,565]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 110.0 (TID 110, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,565]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_142_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,571]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 110.0 (TID 110) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,576]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_181_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,583]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_141_piece0 on localhost:48669 in memory (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,583]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 30 {org.apache.spark.ContextCleaner}
[2016-06-20 15:07:54,584]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_140_piece0 on localhost:48669 in memory (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,585]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_139_piece0 on localhost:48669 in memory (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,586]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_138_piece0 on localhost:48669 in memory (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,587]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 110.0 (TID 110). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 110.0 (TID 110) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 110.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 110 (mapPartitions at DecisionTree.scala:613) finished in 0.024 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,590]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 111: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,590]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 111 (MapPartitionsRDD[193] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,591]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=378442, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,591]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_145 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,593]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4006) called with curMem=387402, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,593]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_145_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,594]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_145_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,594]  INFO {org.apache.spark.SparkContext} -  Created broadcast 145 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,594]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[193] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,594]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 111.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,595]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 111.0 (TID 111, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,596]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 111.0 (TID 111) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,599]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,599]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,631]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 111.0 (TID 111). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,636]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 111.0 (TID 111) in 41 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,636]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 111.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,636]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 111 (collectAsMap at DecisionTree.scala:642) finished in 0.042 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,637]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 79 finished: collectAsMap at DecisionTree.scala:642, took 0.080170 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,638]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=391408, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,638]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_146 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,640]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=393216, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,641]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_146_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,641]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_146_piece0 in memory on localhost:48669 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,641]  INFO {org.apache.spark.SparkContext} -  Created broadcast 146 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,650]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 194 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 80 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 113(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 112) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 112) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,652]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 112 (MapPartitionsRDD[194] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,653]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=393389, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,653]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_147 stored as values in memory (estimated size 33.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,656]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13196) called with curMem=427293, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,656]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_147_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,657]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_147_piece0 in memory on localhost:48669 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,657]  INFO {org.apache.spark.SparkContext} -  Created broadcast 147 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,658]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 112 (MapPartitionsRDD[194] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,658]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 112.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,658]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 112.0 (TID 112, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,659]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 112.0 (TID 112) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,666]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_181_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,684]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 112.0 (TID 112). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,685]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 112.0 (TID 112) in 27 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,685]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 112.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 112 (mapPartitions at DecisionTree.scala:613) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 113) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 113: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 113 (MapPartitionsRDD[196] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,687]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=440489, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,687]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_148 stored as values in memory (estimated size 8.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,690]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4013) called with curMem=449209, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,690]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_148_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,690]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_148_piece0 in memory on localhost:48669 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,691]  INFO {org.apache.spark.SparkContext} -  Created broadcast 148 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,691]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[196] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,692]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 113.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,693]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 113.0 (TID 113, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,693]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 113.0 (TID 113) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,696]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,696]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,710]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 113.0 (TID 113). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,714]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 113.0 (TID 113) in 21 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,714]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 113 (collectAsMap at DecisionTree.scala:642) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,714]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 113.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,714]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 80 finished: collectAsMap at DecisionTree.scala:642, took 0.063812 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,714]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 181 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:54,715]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 181 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,715]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 15:07:54,715]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.131886751
  total: 0.631807768
  findSplitsBins: 0.034272336
  findBestSplits: 0.497579981
  chooseSplits: 0.496608373 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-20 15:07:54,715]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 176 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:54,715]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 176 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,723]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 81 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 114(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,723]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,724]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,724]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 114 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,726]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28664) called with curMem=330886, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,726]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_149 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,729]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10844) called with curMem=359550, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,729]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_149_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,729]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_149_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,729]  INFO {org.apache.spark.SparkContext} -  Created broadcast 149 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,730]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,730]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 114.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,731]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 114.0 (TID 114, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,731]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 114.0 (TID 114) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,733]  INFO {org.apache.spark.CacheManager} -  Partition rdd_197_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:07:54,734]  INFO {org.apache.spark.CacheManager} -  Partition rdd_177_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:07:54,734]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:07:54,755]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=370394, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,755]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_177_0 stored as values in memory (estimated size 29.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,755]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_177_0 in memory on localhost:48669 (size: 29.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,762]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=400090, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,762]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_197_0 stored as values in memory (estimated size 15.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,763]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_197_0 in memory on localhost:48669 (size: 15.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,764]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 114.0 (TID 114). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,766]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 114.0 (TID 114) in 35 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,766]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 114.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,766]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 114 (take at SparkModelUtils.java:215) finished in 0.035 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,766]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 81 finished: take at SparkModelUtils.java:215, took 0.043764 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,770]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,770]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 82 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,770]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 115(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,770]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,771]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,771]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 115 (MapPartitionsRDD[177] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,771]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=416218, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,771]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_150 stored as values in memory (estimated size 7.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,774]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3553) called with curMem=423490, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,774]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_150_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,775]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_150_piece0 in memory on localhost:48669 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,775]  INFO {org.apache.spark.SparkContext} -  Created broadcast 150 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,775]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[177] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,775]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 115.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,776]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 115.0 (TID 115, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,776]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 115.0 (TID 115) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,777]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_177_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,779]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 115.0 (TID 115). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,783]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 115.0 (TID 115) in 7 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,783]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 115.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,783]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 115 (take at SparkModelUtils.java:223) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,784]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 82 finished: take at SparkModelUtils.java:223, took 0.013801 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,786]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,786]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 83 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,786]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 116(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,786]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,787]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,787]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 116 (ParallelCollectionRDD[198] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,787]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=427043, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,788]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_151 stored as values in memory (estimated size 1320.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,790]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=428363, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,790]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_151_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,791]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_151_piece0 in memory on localhost:48669 (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,791]  INFO {org.apache.spark.SparkContext} -  Created broadcast 151 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,791]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 116 (ParallelCollectionRDD[198] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,791]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 116.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,794]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 116.0 (TID 116, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,795]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 116.0 (TID 116) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,798]  INFO {org.apache.spark.CacheManager} -  Partition rdd_198_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:07:54,804]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=429240, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,804]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_198_0 stored as values in memory (estimated size 32.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,805]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_198_0 in memory on localhost:48669 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,806]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 116.0 (TID 116). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,808]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 116.0 (TID 116) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,809]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 116.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 116 (count at SparkModelUtils.java:241) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 83 finished: count at SparkModelUtils.java:241, took 0.023603 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,812]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 84 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 117(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,815]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,815]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 117 (ParallelCollectionRDD[198] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,816]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=462328, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,816]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_152 stored as values in memory (estimated size 1496.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,819]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=463824, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,819]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_152_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,820]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_152_piece0 in memory on localhost:48669 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,820]  INFO {org.apache.spark.SparkContext} -  Created broadcast 152 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,820]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 117 (ParallelCollectionRDD[198] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,820]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 117.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,822]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 117.0 (TID 117, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,823]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 117.0 (TID 117) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,826]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_198_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,828]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 117.0 (TID 117). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,830]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 117.0 (TID 117) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,830]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 117.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,830]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 117 (collect at SparkModelUtils.java:245) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,831]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 84 finished: collect at SparkModelUtils.java:245, took 0.018295 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,831]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 198 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 15:07:54,832]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 198 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,834]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 85 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 118(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,836]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 118 (MapPartitionsRDD[199] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,838]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=431666, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,838]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_153 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,840]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10916) called with curMem=460378, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,841]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_153_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,841]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_153_piece0 in memory on localhost:48669 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,842]  INFO {org.apache.spark.SparkContext} -  Created broadcast 153 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[199] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,842]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 118.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,843]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 118.0 (TID 118, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,843]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 118.0 (TID 118) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,845]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_197_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,847]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 118.0 (TID 118). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,848]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 118.0 (TID 118) in 6 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,848]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 118.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,848]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 118 (count at SparkModelUtils.java:252) finished in 0.005 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,849]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 85 finished: count at SparkModelUtils.java:252, took 0.014187 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,850]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,850]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 86 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,850]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 119(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,850]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,851]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,851]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 119 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,853]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=471294, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,853]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_154 stored as values in memory (estimated size 27.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,856]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10753) called with curMem=499790, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,856]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_154_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,857]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_154_piece0 in memory on localhost:48669 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,857]  INFO {org.apache.spark.SparkContext} -  Created broadcast 154 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,857]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,857]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 119.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,858]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 119.0 (TID 119, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,858]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 119.0 (TID 119) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,860]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_197_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,861]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 119.0 (TID 119). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,863]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 119.0 (TID 119) in 6 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,863]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 119.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 119 (count at SparkModelUtils.java:252) finished in 0.004 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 86 finished: count at SparkModelUtils.java:252, took 0.013933 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,864]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 177 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:54,864]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 177 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,869]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 87 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 120(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,870]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 120 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,873]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28648) called with curMem=480847, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,873]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_155 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,876]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10809) called with curMem=509495, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,876]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_155_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,876]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_155_piece0 in memory on localhost:48669 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,877]  INFO {org.apache.spark.SparkContext} -  Created broadcast 155 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,877]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[197] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,877]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 120.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,877]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 120.0 (TID 120, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,878]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 120.0 (TID 120) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,880]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_197_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,883]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 120.0 (TID 120). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,885]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 120.0 (TID 120) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,886]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 120.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 120 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 87 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.017278 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,887]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 200 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-20 15:07:54,888]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 200 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,888]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 197 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:07:54,888]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 197 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 201 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 88 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 122(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 121) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,895]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 121) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,896]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 121 (MapPartitionsRDD[201] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,897]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=504176, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,897]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_156 stored as values in memory (estimated size 2.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,899]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1710) called with curMem=507040, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,899]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_156_piece0 stored as bytes in memory (estimated size 1710.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,899]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_156_piece0 in memory on localhost:48669 (size: 1710.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,900]  INFO {org.apache.spark.SparkContext} -  Created broadcast 156 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,900]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 121 (MapPartitionsRDD[201] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,900]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 121.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,902]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 121.0 (TID 121, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,902]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 121.0 (TID 121) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,910]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 121.0 (TID 121). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,911]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 121.0 (TID 121) in 11 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 121 (map at MulticlassMetrics.scala:47) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 121.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 122) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 122: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 122 (ShuffledRDD[202] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,912]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=508750, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,913]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_157 stored as values in memory (estimated size 2.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1394) called with curMem=511046, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,915]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_157_piece0 stored as bytes in memory (estimated size 1394.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,916]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_157_piece0 in memory on localhost:48669 (size: 1394.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,916]  INFO {org.apache.spark.SparkContext} -  Created broadcast 157 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,916]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 122 (ShuffledRDD[202] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,916]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 122.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,918]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 122.0 (TID 122, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,918]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 122.0 (TID 122) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,920]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,920]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 1 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,924]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 122.0 (TID 122). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,926]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 122.0 (TID 122) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,926]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 122.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 122 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.006 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 88 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.031512 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,932]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,933]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 203 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,933]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 89 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,933]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 124(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,933]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 123) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,934]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 123) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,934]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 123 (MapPartitionsRDD[203] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,935]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=512440, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,935]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_158 stored as values in memory (estimated size 2.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,937]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=515312, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,937]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_158_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,938]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_158_piece0 in memory on localhost:48669 (size: 1712.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,938]  INFO {org.apache.spark.SparkContext} -  Created broadcast 158 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,938]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[203] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,938]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 123.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,941]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 123.0 (TID 123, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,941]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 123.0 (TID 123) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,953]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 123.0 (TID 123). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,955]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 123.0 (TID 123) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,955]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 123.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,956]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 123 (map at MulticlassMetrics.scala:57) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,956]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,956]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,956]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 124) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,956]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,957]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 124: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,957]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 124 (ShuffledRDD[204] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,957]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=517024, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,958]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_159 stored as values in memory (estimated size 2.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,960]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=519328, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,960]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_159_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:07:54,960]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_159_piece0 in memory on localhost:48669 (size: 1390.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:07:54,960]  INFO {org.apache.spark.SparkContext} -  Created broadcast 159 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:07:54,961]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 124 (ShuffledRDD[204] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,961]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 124.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,961]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 124.0 (TID 124, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,961]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 124.0 (TID 124) {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,964]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,964]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-20 15:07:54,967]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 124.0 (TID 124). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:07:54,969]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 124.0 (TID 124) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:07:54,969]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 124.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:07:54,969]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 124 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:07:54,969]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 89 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.036590 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,394]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=520718, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,394]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_160 stored as values in memory (estimated size 37.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,401]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=559206, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,402]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_160_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,402]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_160_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,403]  INFO {org.apache.spark.SparkContext} -  Created broadcast 160 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,408]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 15:08:20,411]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 90 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 125(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,412]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,413]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 125 (MapPartitionsRDD[206] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,413]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=563295, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,413]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_161 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,417]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=566511, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,417]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_161_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,417]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_161_piece0 in memory on localhost:48669 (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,417]  INFO {org.apache.spark.SparkContext} -  Created broadcast 161 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,418]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[206] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,418]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 125.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,418]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 125.0 (TID 125, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,418]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 125.0 (TID 125) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,419]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,423]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 125.0 (TID 125). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,424]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 125.0 (TID 125) in 6 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,424]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 125 (first at MLUtils.java:91) finished in 0.006 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,424]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 125.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,424]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 90 finished: first at MLUtils.java:91, took 0.012858 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,434]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=568393, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,435]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_162 stored as values in memory (estimated size 37.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,441]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=606881, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,441]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_162_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,441]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_162_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,442]  INFO {org.apache.spark.SparkContext} -  Created broadcast 162 from textFile at MLUtils.java:73 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,448]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 15:08:20,451]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:75 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,453]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 91 (first at MLUtils.java:75) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,454]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 126(first at MLUtils.java:75) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,454]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,454]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,454]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 126 (MapPartitionsRDD[208] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,455]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=610970, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,455]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_163 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,457]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=614186, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,457]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_163_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,457]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_163_piece0 in memory on localhost:48669 (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,458]  INFO {org.apache.spark.SparkContext} -  Created broadcast 163 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,458]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[208] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,458]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 126.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,459]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 126.0 (TID 126, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,460]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 126.0 (TID 126) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,464]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,469]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 126.0 (TID 126). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,472]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 126.0 (TID 126) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,472]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 126.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,472]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 126 (first at MLUtils.java:75) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,473]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 91 finished: first at MLUtils.java:75, took 0.021447 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,475]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:153 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,475]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 92 (first at MLUtils.java:153) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,475]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 127(first at MLUtils.java:153) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,475]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,476]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,476]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 127 (MapPartitionsRDD[208] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,477]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=616068, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,477]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_164 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,479]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=619284, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,479]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_164_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,480]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_164_piece0 in memory on localhost:48669 (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,480]  INFO {org.apache.spark.SparkContext} -  Created broadcast 164 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,480]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[208] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,480]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 127.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,481]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 127.0 (TID 127, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,481]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 127.0 (TID 127) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,482]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,485]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 127.0 (TID 127). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,487]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 127.0 (TID 127) in 6 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,487]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 127.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,488]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 127 (first at MLUtils.java:153) finished in 0.007 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,489]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 92 finished: first at MLUtils.java:153, took 0.013540 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,491]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:163 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,491]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 93 (first at MLUtils.java:163) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,491]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 128(first at MLUtils.java:163) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,491]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 128 (MapPartitionsRDD[208] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,493]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=621166, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,493]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_165 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,495]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=624382, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,495]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_165_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,495]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_165_piece0 in memory on localhost:48669 (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,496]  INFO {org.apache.spark.SparkContext} -  Created broadcast 165 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,496]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[208] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,496]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 128.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,497]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 128.0 (TID 128, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,497]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 128.0 (TID 128) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,498]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,501]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 128.0 (TID 128). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,502]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 128.0 (TID 128) in 6 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,502]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 128.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,502]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 128 (first at MLUtils.java:163) finished in 0.002 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,502]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 93 finished: first at MLUtils.java:163, took 0.011188 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,503]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 209 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:08:20,504]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 209 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:08:20,506]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:187 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 94 (first at MLUtils.java:187) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 129(first at MLUtils.java:187) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,507]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,508]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 129 (MapPartitionsRDD[208] at textFile at MLUtils.java:73), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,508]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=626264, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,508]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_166 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,510]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=629480, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,510]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_166_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,510]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_166_piece0 in memory on localhost:48669 (size: 1882.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,511]  INFO {org.apache.spark.SparkContext} -  Created broadcast 166 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,511]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[208] at textFile at MLUtils.java:73) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,511]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 129.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,511]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 129.0 (TID 129, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,511]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 129.0 (TID 129) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,512]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,514]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 129.0 (TID 129). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,516]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 129.0 (TID 129) in 5 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,516]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 129 (first at MLUtils.java:187) finished in 0.005 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,516]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 129.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,516]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 94 finished: first at MLUtils.java:187, took 0.009629 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,517]  INFO {org.apache.spark.SparkContext} -  Starting job: takeSample at MLUtils.java:194 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,517]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 95 (takeSample at MLUtils.java:194) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,517]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 130(takeSample at MLUtils.java:194) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,517]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,518]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,518]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 130 (MapPartitionsRDD[210] at map at MLUtils.java:167), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,518]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4272) called with curMem=631362, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,518]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_167 stored as values in memory (estimated size 4.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,520]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2525) called with curMem=635634, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,520]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_167_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,521]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_167_piece0 in memory on localhost:48669 (size: 2.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,521]  INFO {org.apache.spark.SparkContext} -  Created broadcast 167 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,521]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[210] at map at MLUtils.java:167) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,521]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 130.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,522]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 130.0 (TID 130, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,522]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 130.0 (TID 130) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,523]  INFO {org.apache.spark.CacheManager} -  Partition rdd_210_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-20 15:08:20,523]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428100393:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:08:20,567]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(362816) called with curMem=638159, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,568]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_210_0 stored as values in memory (estimated size 354.3 KB, free 982.1 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,568]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_210_0 in memory on localhost:48669 (size: 354.3 KB, free: 982.6 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,569]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 130.0 (TID 130). 2331 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,571]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 130.0 (TID 130) in 50 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,571]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 130 (takeSample at MLUtils.java:194) finished in 0.050 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,571]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 130.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,571]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 95 finished: takeSample at MLUtils.java:194, took 0.053973 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,574]  INFO {org.apache.spark.SparkContext} -  Starting job: takeSample at MLUtils.java:194 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 96 (takeSample at MLUtils.java:194) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 131(takeSample at MLUtils.java:194) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 131 (MapPartitionsRDD[210] at map at MLUtils.java:167), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,575]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4416) called with curMem=1000975, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,576]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_168 stored as values in memory (estimated size 4.3 KB, free 982.1 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2576) called with curMem=1005391, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_168_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.1 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_168_piece0 in memory on localhost:48669 (size: 2.5 KB, free: 982.6 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.SparkContext} -  Created broadcast 168 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[210] at map at MLUtils.java:167) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,578]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 131.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,579]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 131.0 (TID 131, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,579]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 131.0 (TID 131) {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,580]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_210_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-20 15:08:20,587]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 131.0 (TID 131). 31102 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:08:20,592]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 131.0 (TID 131) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:08:20,592]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 131.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:08:20,592]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 131 (takeSample at MLUtils.java:194) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,592]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 96 finished: takeSample at MLUtils.java:194, took 0.018534 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:08:20,593]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 210 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:08:20,593]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 210 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:08:51,857]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=645151, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:51,857]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_169 stored as values in memory (estimated size 37.6 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:51,861]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=683639, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:51,861]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_169_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:08:51,861]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_169_piece0 in memory on localhost:48669 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:08:51,862]  INFO {org.apache.spark.SparkContext} -  Created broadcast 169 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 15:08:51,875]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 212 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:08:51,881]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 212 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:08:51,890]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 220 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:08:51,891]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 220 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,234]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 221 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,235]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 221 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,236]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 220 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,236]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 220 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,237]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 212 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,237]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 212 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,237]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_169_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,239]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_168_piece0 on localhost:48669 in memory (size: 2.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,243]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_167_piece0 on localhost:48669 in memory (size: 2.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,245]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_166_piece0 on localhost:48669 in memory (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,246]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 210 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,246]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 210 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,247]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 209 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,251]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 209 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,251]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_165_piece0 on localhost:48669 in memory (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,252]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_164_piece0 on localhost:48669 in memory (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,255]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_163_piece0 on localhost:48669 in memory (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,256]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_162_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,263]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_161_piece0 on localhost:48669 in memory (size: 1882.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,265]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_160_piece0 on localhost:48669 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,267]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_159_piece0 on localhost:48669 in memory (size: 1390.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,268]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_158_piece0 on localhost:48669 in memory (size: 1712.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,268]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 34 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,269]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_157_piece0 on localhost:48669 in memory (size: 1394.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,272]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_156_piece0 on localhost:48669 in memory (size: 1710.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,273]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 33 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,273]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 200 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,274]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 200 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,279]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_155_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,280]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_154_piece0 on localhost:48669 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,281]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_153_piece0 on localhost:48669 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,282]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_152_piece0 on localhost:48669 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,283]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_151_piece0 on localhost:48669 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,284]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 198 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,286]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 198 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,287]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_150_piece0 on localhost:48669 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,289]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_149_piece0 on localhost:48669 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,290]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 197 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,291]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 197 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,292]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 177 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,292]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 177 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,292]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_148_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,293]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_147_piece0 on localhost:48669 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,293]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 32 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,295]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_146_piece0 on localhost:48669 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,295]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_145_piece0 on localhost:48669 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,296]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_144_piece0 on localhost:48669 in memory (size: 11.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,296]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 31 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:09,302]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_143_piece0 on localhost:48669 in memory (size: 217.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:11:09,303]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 181 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:09,303]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 181 {org.apache.spark.ContextCleaner}
[2016-06-20 15:11:15,293]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-20 15:11:15,314]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,315]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,317]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,317]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,318]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,319]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,320]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,320]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,320]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,321]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,321]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,321]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,321]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,322]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,322]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,322]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,322]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,322]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,323]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,324]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 15:11:15,377]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 15:11:15,379]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:11:15,439]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-20 15:11:15,445]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-90679c12-bfa0-4bec-ae48-ebe65f8fb91b/blockmgr-b07195dc-35db-45b1-b99b-1eb00bf7a7b8, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-20 15:11:15,446]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:11:15,446]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-20 15:11:15,447]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 15:11:15,453]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-20 15:11:15,453]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-20 15:11:15,454]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-20 15:11:15,454]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-90679c12-bfa0-4bec-ae48-ebe65f8fb91b {org.apache.spark.util.Utils}
[2016-06-20 15:11:15,465]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 15:11:15,468]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 15:11:15,492]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 15:11:16,357]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 15:11:20,109]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 15:13:50,315]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-20 15:13:50,432]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-20 15:13:50,488]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-20 15:13:50,488]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-20 15:13:50,502]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 15:13:50,503]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 15:13:50,503]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-20 15:13:50,886]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-20 15:13:50,932]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-20 15:13:51,097]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:55324] {Remoting}
[2016-06-20 15:13:51,104]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 55324. {org.apache.spark.util.Utils}
[2016-06-20 15:13:51,132]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-20 15:13:51,146]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-20 15:13:51,170]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-3c5b5b6d-c5a8-45f4-b134-669b1a09477b/blockmgr-d10df25d-f40f-4988-8bf4-f375a836c1eb {org.apache.spark.storage.DiskBlockManager}
[2016-06-20 15:13:51,176]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:13:51,203]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-3c5b5b6d-c5a8-45f4-b134-669b1a09477b/httpd-bf893043-8a52-47cb-9ff7-113e3013e8b2 {org.apache.spark.HttpFileServer}
[2016-06-20 15:13:51,206]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-20 15:13:51,254]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 15:13:51,270]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:43943 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 15:13:51,270]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 43943. {org.apache.spark.util.Utils}
[2016-06-20 15:13:51,280]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-20 15:13:51,363]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 15:13:51,379]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 15:13:51,379]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-20 15:13:51,380]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 15:13:51,438]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-20 15:13:51,737]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38088. {org.apache.spark.util.Utils}
[2016-06-20 15:13:51,738]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 38088 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-20 15:13:51,739]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 15:13:51,742]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:38088 with 983.1 MB RAM, BlockManagerId(driver, localhost, 38088) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 15:13:51,744]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 15:14:21,093]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,097]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,284]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,284]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,288]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:38088 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:14:21,294]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:14:21,421]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 15:14:21,488]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 15:14:21,510]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,511]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,511]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,515]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,521]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,526]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,527]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,551]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,552]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:21,553]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:38088 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:14:21,554]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 15:14:21,561]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,563]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:14:21,612]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:14:21,622]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-20 15:14:21,656]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466428460760:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 15:14:21,669]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 15:14:21,669]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 15:14:21,669]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 15:14:21,669]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 15:14:21,669]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 15:14:21,753]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 15:14:21,771]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 167 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 15:14:21,773]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 15:14:21,780]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.190 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:21,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.306719 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 15:14:53,725]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:53,726]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:53,738]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:53,739]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 15:14:53,740]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:38088 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:14:53,741]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 15:14:53,839]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:14:53,843]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:14:53,861]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 15:14:53,862]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:18:27,084]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:18:27,086]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-20 15:18:27,087]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:18:27,088]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-20 15:18:27,088]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 15:18:27,089]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-20 15:18:27,095]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:38088 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:18:27,097]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:38088 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 15:18:27,100]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:38088 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 17:42:04,993]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:04,993]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,009]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,009]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,010]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:38088 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 17:42:05,010]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 17:42:05,022]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 17:42:05,043]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 17:42:05,046]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,046]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,046]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,046]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,048]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,049]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,053]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,053]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:42:05,054]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:38088 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 17:42:05,054]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 17:42:05,054]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,054]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 17:42:05,059]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 17:42:05,059]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-20 17:42:05,062]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466437324988:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 17:42:05,074]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 17:42:05,077]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 22 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 17:42:05,077]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:42:05,078]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 17:42:05,079]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.035348 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:44:37,891]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-20 17:44:37,907]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,909]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,910]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,911]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,912]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,912]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,913]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,913]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,914]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,914]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,914]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 17:44:37,969]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 17:44:37,971]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 17:44:38,029]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-20 17:44:38,033]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-3c5b5b6d-c5a8-45f4-b134-669b1a09477b/blockmgr-d10df25d-f40f-4988-8bf4-f375a836c1eb, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-20 17:44:38,034]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-20 17:44:38,034]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-20 17:44:38,035]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 17:44:38,037]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-20 17:44:38,037]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-20 17:44:38,037]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-20 17:44:38,040]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-3c5b5b6d-c5a8-45f4-b134-669b1a09477b {org.apache.spark.util.Utils}
[2016-06-20 17:44:38,046]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 17:44:38,047]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 17:44:38,071]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 17:44:38,968]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 17:44:42,671]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 18:12:42,987]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-20 18:12:43,110]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-20 18:12:43,173]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-20 18:12:43,173]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-20 18:12:43,192]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 18:12:43,193]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 18:12:43,194]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-20 18:12:43,618]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-20 18:12:43,652]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-20 18:12:43,790]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:58600] {Remoting}
[2016-06-20 18:12:43,798]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 58600. {org.apache.spark.util.Utils}
[2016-06-20 18:12:43,825]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-20 18:12:43,838]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-20 18:12:43,860]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-358d0548-6e4e-4c86-af91-daf37448b92e/blockmgr-d7644ed0-2243-4147-8e24-ea90c7e5fcee {org.apache.spark.storage.DiskBlockManager}
[2016-06-20 18:12:43,866]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:12:43,894]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-358d0548-6e4e-4c86-af91-daf37448b92e/httpd-bf112e36-6480-456c-8b71-2ea0050a9152 {org.apache.spark.HttpFileServer}
[2016-06-20 18:12:43,896]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-20 18:12:43,933]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 18:12:43,948]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:46257 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 18:12:43,948]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 46257. {org.apache.spark.util.Utils}
[2016-06-20 18:12:43,959]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-20 18:12:44,053]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 18:12:44,063]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 18:12:44,063]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-20 18:12:44,065]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 18:12:44,122]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-20 18:12:44,454]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55705. {org.apache.spark.util.Utils}
[2016-06-20 18:12:44,454]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 55705 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-20 18:12:44,455]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:12:44,459]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:55705 with 983.1 MB RAM, BlockManagerId(driver, localhost, 55705) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 18:12:44,461]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:13:12,889]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:12,892]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,078]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,080]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,082]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:55705 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:13:13,085]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 18:13:13,227]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 18:13:13,292]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 18:13:13,310]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,311]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,311]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,315]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,320]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,325]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,326]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,361]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,361]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:13,362]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:55705 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:13:13,364]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 18:13:13,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,376]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 18:13:13,403]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 18:13:13,415]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-20 18:13:13,439]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466439192533:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 18:13:13,450]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:13:13,450]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:13:13,450]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:13:13,450]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:13:13,450]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:13:13,520]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 18:13:13,550]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.163 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:13,542]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 145 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 18:13:13,552]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 18:13:13,566]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.272920 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:13:45,530]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:45,530]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:45,542]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:45,543]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:13:45,544]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:55705 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:13:45,545]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 18:13:45,646]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 18:13:45,651]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:13:45,679]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 18:13:45,680]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:17:20,707]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:17:20,709]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-20 18:17:20,710]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:17:20,711]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-20 18:17:20,711]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:17:20,711]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-20 18:17:20,717]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:55705 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:17:20,720]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:55705 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:17:20,721]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:55705 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:26:20,066]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-20 18:26:20,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,088]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,093]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,093]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,096]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,098]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,100]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:26:20,154]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 18:26:20,157]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:26:20,218]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-20 18:26:20,225]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-358d0548-6e4e-4c86-af91-daf37448b92e/blockmgr-d7644ed0-2243-4147-8e24-ea90c7e5fcee, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-20 18:26:20,225]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:26:20,226]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-20 18:26:20,227]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:26:20,232]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-20 18:26:20,237]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-20 18:26:20,238]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-20 18:26:20,238]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-358d0548-6e4e-4c86-af91-daf37448b92e {org.apache.spark.util.Utils}
[2016-06-20 18:26:20,247]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:26:20,251]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:26:20,271]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:26:21,180]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 18:26:25,035]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 18:26:58,727]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-20 18:26:58,850]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-20 18:26:58,921]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-20 18:26:58,921]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-20 18:26:58,940]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 18:26:58,941]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-20 18:26:58,941]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-20 18:26:59,381]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-20 18:26:59,431]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-20 18:26:59,557]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:60615] {Remoting}
[2016-06-20 18:26:59,563]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 60615. {org.apache.spark.util.Utils}
[2016-06-20 18:26:59,585]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-20 18:26:59,598]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-20 18:26:59,618]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2638d06d-1d8f-44b6-aeb9-ec5208521a28/blockmgr-ff09cb95-3e94-49ce-86c3-28f00389a679 {org.apache.spark.storage.DiskBlockManager}
[2016-06-20 18:26:59,623]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:26:59,644]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2638d06d-1d8f-44b6-aeb9-ec5208521a28/httpd-d2fe1009-b6eb-47de-ac1b-839bb71ad763 {org.apache.spark.HttpFileServer}
[2016-06-20 18:26:59,647]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-20 18:26:59,684]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 18:26:59,696]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:46656 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 18:26:59,696]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 46656. {org.apache.spark.util.Utils}
[2016-06-20 18:26:59,704]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-20 18:26:59,786]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-20 18:26:59,794]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-20 18:26:59,795]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-20 18:26:59,796]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 18:26:59,865]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-20 18:27:00,192]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53745. {org.apache.spark.util.Utils}
[2016-06-20 18:27:00,192]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 53745 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-20 18:27:00,193]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:27:00,197]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:53745 with 983.1 MB RAM, BlockManagerId(driver, localhost, 53745) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-20 18:27:00,200]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:27:17,015]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,018]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,190]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,191]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,193]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:53745 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:27:17,197]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 18:27:17,304]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-20 18:27:17,366]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-20 18:27:17,385]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,386]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,386]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,391]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,397]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,404]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,410]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,442]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,442]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:17,443]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:53745 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:27:17,444]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-20 18:27:17,449]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,450]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 18:27:17,479]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 18:27:17,487]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-20 18:27:17,511]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466440036718:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-20 18:27:17,523]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:27:17,523]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:27:17,523]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:27:17,523]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:27:17,524]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-20 18:27:17,567]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-20 18:27:17,583]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 111 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-20 18:27:17,586]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.126 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:17,587]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-20 18:27:17,605]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.238476 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:27:49,510]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:49,510]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:49,523]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:49,523]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:27:49,525]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:53745 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:27:49,525]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-20 18:27:49,612]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 18:27:49,617]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:27:49,634]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-20 18:27:49,635]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:31:33,475]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:31:33,475]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-20 18:31:33,484]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-20 18:31:33,486]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:31:33,487]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-20 18:31:33,491]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,491]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,491]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,492]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,492]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,492]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,492]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,493]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,493]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,493]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,493]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,493]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,494]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,494]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,494]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-20 18:31:33,494]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,494]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,495]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,495]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,496]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-20 18:31:33,499]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,507]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:53745 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:31:33,507]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,507]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,507]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,507]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,507]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,508]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-20 18:31:33,518]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:53745 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:31:33,521]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:53745 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-20 18:31:33,562]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-20 18:31:33,565]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-20 18:31:33,624]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-20 18:31:33,628]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2638d06d-1d8f-44b6-aeb9-ec5208521a28/blockmgr-ff09cb95-3e94-49ce-86c3-28f00389a679, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-20 18:31:33,629]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-20 18:31:33,630]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-20 18:31:33,631]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-20 18:31:33,633]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-20 18:31:33,634]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-20 18:31:33,634]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-20 18:31:33,634]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2638d06d-1d8f-44b6-aeb9-ec5208521a28 {org.apache.spark.util.Utils}
[2016-06-20 18:31:33,642]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:31:33,644]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:31:33,667]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-20 18:31:34,550]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-20 18:31:38,335]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 10:34:49,047]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 10:34:49,167]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 10:34:49,233]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 10:34:49,234]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 10:34:49,254]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:34:49,255]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:34:49,256]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 10:34:49,812]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 10:34:49,860]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 10:34:50,007]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:49869] {Remoting}
[2016-06-21 10:34:50,021]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 49869. {org.apache.spark.util.Utils}
[2016-06-21 10:34:50,052]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 10:34:50,067]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 10:34:50,087]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-0f260b1a-b1e0-4a20-b477-ee08c457e6f6/blockmgr-40d093cc-db19-4517-a2f9-0075e83679e2 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 10:34:50,093]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:34:50,115]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-0f260b1a-b1e0-4a20-b477-ee08c457e6f6/httpd-81c70697-7d8a-4f40-9515-799d9641a551 {org.apache.spark.HttpFileServer}
[2016-06-21 10:34:50,117]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 10:34:50,155]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:34:50,176]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:42451 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:34:50,176]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 42451. {org.apache.spark.util.Utils}
[2016-06-21 10:34:50,185]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 10:34:50,281]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:34:50,304]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:34:50,304]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 10:34:50,306]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 10:34:50,379]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 10:34:50,782]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37178. {org.apache.spark.util.Utils}
[2016-06-21 10:34:50,783]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 37178 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 10:34:50,783]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:34:50,787]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:37178 with 983.1 MB RAM, BlockManagerId(driver, localhost, 37178) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 10:34:50,789]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:34:59,716]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:34:59,723]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:34:59,981]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:34:59,981]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:34:59,984]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:37178 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:34:59,988]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:35:00,083]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:35:00,137]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:35:00,153]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,154]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,154]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,158]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,163]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,167]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:00,167]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:00,181]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:00,181]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:00,182]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:37178 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:35:00,183]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:35:00,188]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,190]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:35:00,225]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:35:00,234]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 10:35:00,255]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466498099376:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:35:00,263]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:35:00,263]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:35:00,263]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:35:00,263]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:35:00,263]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:35:00,312]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:35:00,332]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 113 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:35:00,334]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:35:00,335]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.132 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:00,341]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.203189 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:35:32,254]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:32,255]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:32,269]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:32,270]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:35:32,271]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:37178 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:35:32,271]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:35:32,367]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:35:32,375]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:35:32,399]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:35:32,400]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:38:55,170]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,170]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,184]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,185]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,185]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:37178 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:38:55,186]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:38:55,195]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:38:55,200]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:38:55,201]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,201]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,201]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,202]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,202]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,204]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,204]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,232]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,233]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:38:55,235]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:37178 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:38:55,235]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:38:55,236]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,236]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:38:55,239]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:38:55,262]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 10:38:55,267]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:38:55,268]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466498335166:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:38:55,282]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-21 10:38:55,283]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:38:55,284]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:38:55,287]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-21 10:38:55,289]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 52 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:38:55,289]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.053 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,290]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:38:55,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.089799 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:38:55,291]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:38:55,293]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-21 10:38:55,303]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:37178 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:38:55,307]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:37178 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:38:55,309]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:37178 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:39:26,758]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47674, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:39:26,759]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:39:26,773]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86162, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:39:26,773]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:39:26,774]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:37178 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:39:26,776]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:39:26,800]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:39:26,810]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:39:26,832]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:39:26,833]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:46:14,885]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:46:15,764]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-21 10:46:15,764]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:46:15,765]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-21 10:46:15,766]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:46:15,767]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 10:46:15,768]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:37178 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:46:15,769]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:37178 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:46:15,771]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:37178 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:47:17,788]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 10:47:17,806]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,807]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,808]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,808]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,809]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,809]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,815]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,818]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:47:17,871]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 10:47:17,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:17,933]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 10:47:17,939]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-0f260b1a-b1e0-4a20-b477-ee08c457e6f6/blockmgr-40d093cc-db19-4517-a2f9-0075e83679e2, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 10:47:17,939]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:17,943]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 10:47:17,943]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:47:17,950]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 10:47:17,951]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 10:47:17,951]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 10:47:17,952]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-0f260b1a-b1e0-4a20-b477-ee08c457e6f6 {org.apache.spark.util.Utils}
[2016-06-21 10:47:17,958]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:47:17,960]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:47:17,979]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:47:18,832]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 10:47:22,954]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 10:47:50,020]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 10:47:50,123]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 10:47:50,183]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 10:47:50,183]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 10:47:50,195]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:47:50,195]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:47:50,196]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 10:47:50,537]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 10:47:50,570]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 10:47:50,701]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:34631] {Remoting}
[2016-06-21 10:47:50,707]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 34631. {org.apache.spark.util.Utils}
[2016-06-21 10:47:50,731]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 10:47:50,745]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 10:47:50,767]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-a20c83a0-e1c9-45d1-943c-47c36bd896d9/blockmgr-20f8250c-3225-4c82-bd88-a9a206db87b1 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 10:47:50,774]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:50,801]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-a20c83a0-e1c9-45d1-943c-47c36bd896d9/httpd-465fc942-e98c-4562-8f09-db67f4091177 {org.apache.spark.HttpFileServer}
[2016-06-21 10:47:50,804]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 10:47:50,856]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:47:50,875]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:57874 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:47:50,875]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 57874. {org.apache.spark.util.Utils}
[2016-06-21 10:47:50,890]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 10:47:51,066]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:47:51,078]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:47:51,078]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 10:47:51,079]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 10:47:51,147]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 10:47:51,442]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42357. {org.apache.spark.util.Utils}
[2016-06-21 10:47:51,442]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 42357 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 10:47:51,443]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:47:51,449]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:42357 with 983.1 MB RAM, BlockManagerId(driver, localhost, 42357) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 10:47:51,453]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:47:58,758]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:58,761]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:58,928]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:58,928]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:58,931]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:47:58,934]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:47:59,020]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:47:59,073]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:47:59,088]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,089]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,089]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,103]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,110]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:59,112]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:59,144]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:59,144]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:47:59,145]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:42357 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:47:59,146]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:47:59,152]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,154]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:47:59,184]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:47:59,191]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 10:47:59,212]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466498878390:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:47:59,222]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:47:59,222]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:47:59,222]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:47:59,222]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:47:59,222]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:47:59,269]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:47:59,298]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.130 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:47:59,297]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 116 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:47:59,303]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:47:59,308]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.234547 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:48:31,208]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:48:31,209]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:48:31,228]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:48:31,228]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:48:31,230]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:48:31,230]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:48:31,319]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:48:31,324]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:48:31,347]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:48:31,348]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:49:45,366]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,366]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,377]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,377]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,378]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:49:45,379]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:49:45,386]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:49:45,392]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:49:45,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,395]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,395]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,396]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,396]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,400]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,400]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:49:45,401]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:42357 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:49:45,401]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:49:45,402]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,402]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:49:45,403]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:49:45,403]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 10:49:45,412]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466498985363:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:49:45,420]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:49:45,426]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:49:45,427]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.024 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:49:45,427]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:49:45,427]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.034648 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:50:16,903]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137839, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:50:16,904]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:50:16,916]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:50:16,916]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:50:16,917]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:50:16,918]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:51:13,491]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:42357 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:51:13,624]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:42357 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:52:01,878]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:52:01,884]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-21 10:52:01,889]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:52:01,898]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-21 10:52:11,839]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:52:11,840]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-21 10:52:24,231]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:42357 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:52:24,493]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:42357 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:52:24,496]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:42357 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:54:29,790]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:54:29,795]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:54:29,810]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:54:29,811]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:54:49,767]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,767]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,777]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,777]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,778]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:54:49,778]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:54:49,785]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:54:49,788]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:54:49,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,789]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,790]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,791]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,792]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,792]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,797]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=88370, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,798]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:54:49,799]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:42357 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:54:49,799]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:54:49,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,799]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:54:49,800]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:54:49,801]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 10:54:49,809]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466499289762:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:54:49,815]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:54:49,821]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:54:49,821]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 0.021 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:54:49,822]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:54:49,822]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 0.033504 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:55:21,292]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90251, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:55:21,293]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:55:21,302]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128739, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:55:21,303]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:55:21,304]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:42357 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:55:21,304]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:55:21,344]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:55:21,345]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:55:21,349]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 39 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:55:21,349]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:56:48,963]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 10:56:48,984]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,986]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,987]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,987]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,987]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,987]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,988]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,988]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,988]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,988]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,989]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,989]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,990]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,990]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,990]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,990]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,991]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,991]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,991]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,992]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:48,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 10:56:49,045]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 10:56:49,047]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:56:49,106]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 10:56:49,110]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-a20c83a0-e1c9-45d1-943c-47c36bd896d9/blockmgr-20f8250c-3225-4c82-bd88-a9a206db87b1, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 10:56:49,111]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:56:49,112]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 10:56:49,112]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:56:49,115]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 10:56:49,115]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 10:56:49,115]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 10:56:49,116]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-a20c83a0-e1c9-45d1-943c-47c36bd896d9 {org.apache.spark.util.Utils}
[2016-06-21 10:56:49,122]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:56:49,124]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:56:49,145]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 10:56:50,017]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 10:56:53,799]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 10:57:28,368]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 10:57:28,485]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 10:57:28,548]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 10:57:28,548]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 10:57:28,563]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:57:28,564]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 10:57:28,564]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 10:57:28,919]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 10:57:28,965]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 10:57:29,102]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:58367] {Remoting}
[2016-06-21 10:57:29,111]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 58367. {org.apache.spark.util.Utils}
[2016-06-21 10:57:29,139]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 10:57:29,150]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 10:57:29,166]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-b2195fd0-e10c-4deb-84fa-d2456fe3a462/blockmgr-cebb1df2-b9eb-4a28-bf14-721db29f054f {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 10:57:29,171]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:29,198]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-b2195fd0-e10c-4deb-84fa-d2456fe3a462/httpd-be3b421f-4ee7-4ecc-b398-b82ce0601a0b {org.apache.spark.HttpFileServer}
[2016-06-21 10:57:29,200]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 10:57:29,249]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:57:29,269]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:60979 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:57:29,269]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 60979. {org.apache.spark.util.Utils}
[2016-06-21 10:57:29,281]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 10:57:29,388]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 10:57:29,397]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 10:57:29,398]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 10:57:29,399]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 10:57:29,449]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 10:57:29,729]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51130. {org.apache.spark.util.Utils}
[2016-06-21 10:57:29,730]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 51130 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 10:57:29,731]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:57:29,734]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:51130 with 983.1 MB RAM, BlockManagerId(driver, localhost, 51130) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 10:57:29,736]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 10:57:39,261]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,264]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,419]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,420]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,423]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:57:39,427]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:57:39,527]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:57:39,584]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:57:39,615]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,622]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,628]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,633]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,635]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,668]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,668]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:57:39,669]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:51130 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:57:39,670]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:57:39,679]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,680]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:57:39,716]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:57:39,725]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 10:57:39,746]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466499458911:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:57:39,754]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:57:39,754]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:57:39,754]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:57:39,754]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:57:39,754]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 10:57:39,801]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:57:39,817]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.127 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:57:39,827]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 107 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:57:39,830]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:57:39,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.248610 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:58:11,750]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:58:11,750]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:58:11,762]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:58:11,763]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:58:11,764]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:58:11,765]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:58:11,882]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:58:11,889]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:58:11,911]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:58:11,912]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:59:20,341]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,342]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,354]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,355]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,355]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:59:20,356]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:59:20,363]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 10:59:20,367]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 10:59:20,368]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,368]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,368]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,369]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,370]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,371]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,371]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,377]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,377]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:20,378]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:51130 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:59:20,378]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 10:59:20,379]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,379]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:59:20,380]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:59:20,380]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 10:59:20,385]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466499560337:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 10:59:20,392]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 10:59:20,399]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,399]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.032187 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 10:59:20,401]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 10:59:20,402]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 10:59:51,874]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137839, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:51,875]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:51,884]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:51,885]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 10:59:51,885]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 10:59:51,886]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 10:59:51,902]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:59:51,920]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 10:59:51,937]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 10:59:51,938]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,633]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,635]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,636]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,636]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,637]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,637]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,642]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:51130 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:01,645]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:51130 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:01,646]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:51130 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:01,647]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,648]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,648]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,648]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,649]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:00:01,649]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-21 11:00:01,650]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:51130 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:01,651]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:51130 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:01,652]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:51130 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:39,221]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,221]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,228]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,229]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,230]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:39,230]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:00:39,240]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:00:39,245]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:00:39,246]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,246]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,246]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,247]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,249]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,249]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,254]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,254]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:00:39,255]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:51130 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:00:39,256]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:00:39,256]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,256]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:00:39,257]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:00:39,258]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 11:00:39,261]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466499639218:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:00:39,266]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:00:39,271]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,271]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 0.025616 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:00:39,271]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:00:39,271]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:01:10,689]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47674, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:01:10,689]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:01:10,702]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86162, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:01:10,702]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:01:10,703]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:51130 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:01:10,704]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 11:01:10,735]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:01:10,746]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:01:10,751]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 39 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:01:10,755]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:01:28,266]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 11:01:28,289]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,291]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,291]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,291]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,291]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,292]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,292]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,292]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,293]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,293]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,293]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,293]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,294]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,294]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,295]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,295]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,295]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,296]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,296]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,298]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,299]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,299]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,299]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,299]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,300]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:01:28,351]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:01:28,353]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:01:28,414]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 11:01:28,424]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-b2195fd0-e10c-4deb-84fa-d2456fe3a462/blockmgr-cebb1df2-b9eb-4a28-bf14-721db29f054f, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 11:01:28,425]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:01:28,426]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 11:01:28,427]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:01:28,430]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 11:01:28,431]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 11:01:28,431]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 11:01:28,431]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-b2195fd0-e10c-4deb-84fa-d2456fe3a462 {org.apache.spark.util.Utils}
[2016-06-21 11:01:28,440]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:01:28,441]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:01:28,463]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:01:29,318]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:01:33,295]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:06:54,037]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 11:06:54,143]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 11:06:54,202]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 11:06:54,203]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 11:06:54,222]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:06:54,223]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:06:54,223]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 11:06:55,013]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 11:06:55,071]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 11:06:55,274]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:49894] {Remoting}
[2016-06-21 11:06:55,276]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 49894. {org.apache.spark.util.Utils}
[2016-06-21 11:06:55,310]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 11:06:55,332]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 11:06:55,364]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-19eb2775-3427-4bee-95ff-1c8f85439ae2/blockmgr-3e105455-e7ed-42c8-8f20-eac4a551f305 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 11:06:55,373]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:06:55,405]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-19eb2775-3427-4bee-95ff-1c8f85439ae2/httpd-c2257756-9479-4444-b7ae-1635fb936e17 {org.apache.spark.HttpFileServer}
[2016-06-21 11:06:55,408]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 11:06:55,459]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:06:55,477]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:59072 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:06:55,478]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 59072. {org.apache.spark.util.Utils}
[2016-06-21 11:06:55,488]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 11:06:55,579]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:06:55,589]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:06:55,589]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 11:06:55,591]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:06:55,641]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 11:06:55,781]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57786. {org.apache.spark.util.Utils}
[2016-06-21 11:06:55,781]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 57786 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 11:06:55,782]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:06:55,786]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:57786 with 983.1 MB RAM, BlockManagerId(driver, localhost, 57786) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 11:06:55,788]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:07:11,479]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,483]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,641]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,641]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,644]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:57786 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:07:11,648]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:07:11,747]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:07:11,806]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:07:11,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,839]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,848]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,850]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,871]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,871]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:11,872]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:57786 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:07:11,879]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:07:11,892]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:11,894]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:07:11,924]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:07:11,930]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 11:07:11,952]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466500031221:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:07:11,961]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:07:11,961]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:07:11,961]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:07:11,961]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:07:11,961]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:07:12,006]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:07:12,027]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.115 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:12,025]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 108 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:07:12,031]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:07:12,040]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.233193 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:07:43,930]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:43,930]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:43,943]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:43,943]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:07:43,944]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:57786 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:07:43,945]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 11:07:44,042]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:07:44,049]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:07:44,075]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:07:44,076]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:09:07,835]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:09:07,837]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-21 11:09:07,841]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:09:07,842]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-21 11:09:07,843]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:09:07,843]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-21 11:09:07,848]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:57786 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:09:07,850]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:57786 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:09:07,852]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:57786 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:12:55,893]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 11:12:55,913]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,914]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,914]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,916]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,917]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,918]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,918]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,918]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,918]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,918]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,919]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,919]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,919]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,920]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,920]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,920]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:12:55,973]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:12:55,976]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:12:56,038]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 11:12:56,048]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-19eb2775-3427-4bee-95ff-1c8f85439ae2/blockmgr-3e105455-e7ed-42c8-8f20-eac4a551f305, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 11:12:56,049]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:12:56,050]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 11:12:56,051]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:12:56,057]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 11:12:56,058]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 11:12:56,059]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 11:12:56,060]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-19eb2775-3427-4bee-95ff-1c8f85439ae2 {org.apache.spark.util.Utils}
[2016-06-21 11:12:56,069]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:12:56,071]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:12:56,101]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:12:56,944]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:13:00,736]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:14:23,635]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 11:14:23,758]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 11:14:23,857]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 11:14:23,857]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 11:14:23,877]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:14:23,877]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:14:23,878]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 11:14:24,267]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 11:14:24,328]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 11:14:24,479]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:42854] {Remoting}
[2016-06-21 11:14:24,487]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 42854. {org.apache.spark.util.Utils}
[2016-06-21 11:14:24,514]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 11:14:24,527]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 11:14:24,550]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2f4af2ba-89e5-4bf9-9737-027cc98ed1fb/blockmgr-65d833e7-d349-4910-8226-ab1cc2496ac8 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 11:14:24,557]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:24,593]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2f4af2ba-89e5-4bf9-9737-027cc98ed1fb/httpd-30473e11-b92c-47d9-8eb3-9aad4a0ab53c {org.apache.spark.HttpFileServer}
[2016-06-21 11:14:24,595]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 11:14:24,634]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:14:24,648]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:33159 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:14:24,649]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 33159. {org.apache.spark.util.Utils}
[2016-06-21 11:14:24,658]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 11:14:24,744]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:14:24,759]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:14:24,760]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 11:14:24,761]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:14:24,812]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 11:14:25,091]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37067. {org.apache.spark.util.Utils}
[2016-06-21 11:14:25,091]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 37067 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 11:14:25,092]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:14:25,097]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:37067 with 983.1 MB RAM, BlockManagerId(driver, localhost, 37067) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 11:14:25,099]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:14:39,563]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,565]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,736]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,736]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,741]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:37067 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:14:39,748]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:14:39,867]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:14:39,924]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:14:39,940]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,941]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,941]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,946]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,950]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,954]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,955]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,970]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1886) called with curMem=45737, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,970]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1886.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:14:39,971]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:37067 (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:14:39,972]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:14:39,985]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:39,986]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:14:40,015]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:14:40,023]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 11:14:40,042]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466500479243:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:14:40,050]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:14:40,050]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:14:40,050]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:14:40,050]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:14:40,050]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:14:40,100]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:14:40,112]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 106 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:14:40,113]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:14:40,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.119 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:14:40,122]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.197840 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,087]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47623, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,087]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,102]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86111, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,103]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,104]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:37067 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,105]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,192]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:12,197]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:12,213]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:12,214]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:12,233]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:15:12,245]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,256]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,256]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,258]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=90200, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,259]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 7.4 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,265]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3673) called with curMem=97808, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,265]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,266]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:37067 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,267]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,268]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,268]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,269]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,269]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,281]  INFO {org.apache.spark.CacheManager} -  Partition rdd_12_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 11:15:12,281]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:15:12,373]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=101481, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,373]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_12_0 stored as values in memory (estimated size 55.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,374]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_12_0 in memory on localhost:37067 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,385]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,403]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 134 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,403]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (take at DecisionTreeMetadata.scala:110) finished in 0.135 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,403]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,403]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: take at DecisionTreeMetadata.scala:110, took 0.157757 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,408]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,409]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,409]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,409]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,414]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=157849, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,414]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 7.3 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,418]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3583) called with curMem=165289, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,419]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,421]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:37067 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,422]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,422]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,422]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,425]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,425]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,429]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:12,434]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,439]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,440]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,445]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (count at DecisionTreeMetadata.scala:111) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,445]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: count at DecisionTreeMetadata.scala:111, took 0.037428 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,468]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,469]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,469]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,469]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,470]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,471]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,473]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8168) called with curMem=168872, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,473]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 8.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,477]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3909) called with curMem=177040, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,477]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,478]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:37067 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,478]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,479]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,479]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,481]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,481]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,486]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:12,501]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,516]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 37 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,519]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (collect at DecisionTree.scala:977) finished in 0.037 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,519]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,519]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: collect at DecisionTree.scala:977, took 0.051006 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,558]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=180949, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,559]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 624.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,562]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=181573, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,563]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,564]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:37067 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,564]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,608]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,612]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 18 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,612]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,612]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,612]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,614]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,618]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,626]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=181644, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,626]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 18.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,631]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8077) called with curMem=200492, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,631]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,632]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:37067 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,632]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,634]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,635]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,637]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,637]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,649]  INFO {org.apache.spark.CacheManager} -  Partition rdd_17_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 11:15:12,650]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:12,722]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=208569, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,722]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_17_0 stored as values in memory (estimated size 64.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,723]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_17_0 in memory on localhost:37067 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,827]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,841]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 206 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,841]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 4 (mapPartitions at DecisionTree.scala:613) finished in 0.207 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 5) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,846]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 5: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,849]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,850]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=274537, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,850]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,855]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=282585, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,856]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,857]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:37067 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:12,857]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:12,858]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,858]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,859]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,860]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,874]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:12,876]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 7 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:12,973]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:12,985]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 127 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:12,985]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (collectAsMap at DecisionTree.scala:642) finished in 0.127 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,985]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:12,986]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: collectAsMap at DecisionTree.scala:642, took 0.377794 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:12,993]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=286154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,993]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 1160.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,998]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=287314, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,998]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:12,999]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:37067 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,000]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,022]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,024]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 21 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,024]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,024]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 7(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,024]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,025]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,028]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,030]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=287427, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,030]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 20.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,035]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8910) called with curMem=307979, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,035]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,036]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:37067 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,036]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,037]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,037]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,038]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,038]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,043]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,064]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 6 (mapPartitions at DecisionTree.scala:613) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,071]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 33 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,071]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,072]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 7: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,072]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,073]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=316889, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,073]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 8.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,078]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3826) called with curMem=325241, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,079]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,079]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:37067 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,080]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,080]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,080]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,081]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,081]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,086]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,086]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,117]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,125]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 7 (collectAsMap at DecisionTree.scala:642) finished in 0.044 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: collectAsMap at DecisionTree.scala:642, took 0.103526 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,129]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=329067, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,129]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,134]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=331307, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,134]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,134]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 44 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,135]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,135]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:37067 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,135]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,149]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,150]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 24 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 9(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,152]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,154]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,157]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=331516, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,157]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,161]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10104) called with curMem=355100, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,162]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,163]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:37067 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,163]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,163]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,163]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,165]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,165]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,168]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,189]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 28 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 8 (mapPartitions at DecisionTree.scala:613) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,192]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,193]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,193]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 9: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,194]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,195]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=365204, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,195]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,199]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4054) called with curMem=374164, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,199]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,200]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:37067 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,200]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,200]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,200]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,201]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,201]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,205]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,206]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,235]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,244]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 42 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,244]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 9 (collectAsMap at DecisionTree.scala:642) finished in 0.043 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,244]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,244]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: collectAsMap at DecisionTree.scala:642, took 0.095042 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,247]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=378218, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,248]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,252]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=380458, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,252]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,253]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:37067 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,253]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,268]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,270]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 27 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,270]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,270]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 11(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,271]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,272]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,274]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,277]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28952) called with curMem=380675, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,277]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 28.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,282]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11798) called with curMem=409627, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,283]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,284]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:37067 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,284]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,284]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,284]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 10.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,285]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 10.0 (TID 10, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,286]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 10.0 (TID 10) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,290]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,310]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 10.0 (TID 10). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 10.0 (TID 10) in 28 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 10 (mapPartitions at DecisionTree.scala:613) finished in 0.027 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 10.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,313]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,314]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,315]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 11: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,315]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,316]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=421425, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,316]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,320]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4004) called with curMem=430385, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,320]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,321]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:37067 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,322]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,322]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,322]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 11.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,323]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 11.0 (TID 11, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,323]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 11.0 (TID 11) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,327]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,327]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,357]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 11.0 (TID 11). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,364]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 11.0 (TID 11) in 41 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,364]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 11.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 11 (collectAsMap at DecisionTree.scala:642) finished in 0.041 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: collectAsMap at DecisionTree.scala:642, took 0.095452 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,370]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=434389, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,370]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,376]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=436197, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,376]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,377]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:37067 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,378]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,391]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 30 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 13(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,394]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,396]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,398]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=436370, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,399]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 33.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,404]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13188) called with curMem=470274, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,404]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,405]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:37067 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,405]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,406]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,406]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 12.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,406]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 12.0 (TID 12, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,407]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 12.0 (TID 12) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,411]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,430]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 12.0 (TID 12). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 12.0 (TID 12) in 27 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 12 (mapPartitions at DecisionTree.scala:613) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 12.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,434]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 13: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,435]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=483462, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,436]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 8.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,439]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4009) called with curMem=492182, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,440]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,440]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:37067 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,441]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,441]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,441]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 13.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,442]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 13.0 (TID 13, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,442]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 13.0 (TID 13) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,448]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,448]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,467]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 13.0 (TID 13). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,473]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 13 (collectAsMap at DecisionTree.scala:642) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,474]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 13.0 (TID 13) in 31 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,474]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 13.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,474]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: collectAsMap at DecisionTree.scala:642, took 0.082387 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,475]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:13,477]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,480]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 11:15:13,481]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.321372967
  total: 1.251618346
  findSplitsBins: 0.083376685
  findBestSplits: 0.914316004
  chooseSplits: 0.906998372 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 11:15:13,485]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 12 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:13,487]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,508]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,509]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,509]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 14(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,509]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,513]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,513]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,515]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28656) called with curMem=373855, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,515]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,519]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10840) called with curMem=402511, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,519]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,520]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_21_piece0 in memory on localhost:37067 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,520]  INFO {org.apache.spark.SparkContext} -  Created broadcast 21 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,521]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,521]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 14.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,522]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 14.0 (TID 14, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,522]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 14.0 (TID 14) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,528]  INFO {org.apache.spark.CacheManager} -  Partition rdd_33_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 11:15:13,529]  INFO {org.apache.spark.CacheManager} -  Partition rdd_13_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 11:15:13,529]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:15:13,546]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=413351, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,547]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_13_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,547]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_13_0 in memory on localhost:37067 (size: 29.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,563]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=443047, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,563]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_33_0 stored as values in memory (estimated size 15.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,563]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_33_0 in memory on localhost:37067 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,567]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 14.0 (TID 14). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,570]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 14.0 (TID 14) in 49 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,570]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 14 (take at SparkModelUtils.java:215) finished in 0.049 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,570]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 14.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,571]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: take at SparkModelUtils.java:215, took 0.062294 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,576]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 10 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 15(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,578]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,578]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=459175, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,579]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22 stored as values in memory (estimated size 7.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,583]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3551) called with curMem=466447, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,583]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,584]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_22_piece0 in memory on localhost:37067 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,585]  INFO {org.apache.spark.SparkContext} -  Created broadcast 22 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,585]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,585]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 15.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,586]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 15.0 (TID 15, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,586]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 15.0 (TID 15) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,588]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_13_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,592]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 15.0 (TID 15). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,598]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 15.0 (TID 15) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,598]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 15.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,599]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 15 (take at SparkModelUtils.java:223) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,599]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 10 finished: take at SparkModelUtils.java:223, took 0.023022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,606]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,606]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 11 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,606]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 16(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,606]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,610]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=469998, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,611]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23 stored as values in memory (estimated size 1320.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,615]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=471318, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,615]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,616]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_23_piece0 in memory on localhost:37067 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,616]  INFO {org.apache.spark.SparkContext} -  Created broadcast 23 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,617]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 16.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,625]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 16.0 (TID 16, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,625]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 16.0 (TID 16) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,636]  INFO {org.apache.spark.CacheManager} -  Partition rdd_34_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 11:15:13,640]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=472195, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,640]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_34_0 stored as values in memory (estimated size 32.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,641]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_34_0 in memory on localhost:37067 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,644]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 16.0 (TID 16). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,647]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 16.0 (TID 16) in 30 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,647]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 16.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,648]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 16 (count at SparkModelUtils.java:241) finished in 0.031 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,648]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 11 finished: count at SparkModelUtils.java:241, took 0.041877 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,650]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 12 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 17(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,651]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,652]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,652]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,652]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=505283, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,652]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24 stored as values in memory (estimated size 1496.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,656]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=506779, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,656]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,657]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_24_piece0 in memory on localhost:37067 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,657]  INFO {org.apache.spark.SparkContext} -  Created broadcast 24 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,657]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,658]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 17.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,662]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 17.0 (TID 17, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,663]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 17.0 (TID 17) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,668]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_34_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,672]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 17.0 (TID 17). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,676]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 17 (collect at SparkModelUtils.java:245) finished in 0.018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,676]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 12 finished: collect at SparkModelUtils.java:245, took 0.025716 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,676]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 17.0 (TID 17) in 18 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,676]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 17.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,677]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 34 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 11:15:13,678]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,684]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 13 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 18(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,685]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,688]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=474621, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,689]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,693]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10912) called with curMem=503333, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,693]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,694]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_25_piece0 in memory on localhost:37067 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,694]  INFO {org.apache.spark.SparkContext} -  Created broadcast 25 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,694]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,694]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 18.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,695]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 18.0 (TID 18, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,695]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 18.0 (TID 18) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,699]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,701]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 18.0 (TID 18). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,704]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 18.0 (TID 18) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,704]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 18.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,705]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 18 (count at SparkModelUtils.java:252) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,705]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 13 finished: count at SparkModelUtils.java:252, took 0.021218 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,707]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,707]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 14 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,708]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 19(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,708]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,708]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,709]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,711]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=514245, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,711]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26 stored as values in memory (estimated size 27.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,715]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10749) called with curMem=542741, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,715]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,716]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_26_piece0 in memory on localhost:37067 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,716]  INFO {org.apache.spark.SparkContext} -  Created broadcast 26 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,716]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,716]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 19.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,717]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 19.0 (TID 19, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,717]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 19.0 (TID 19) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,721]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,724]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 19.0 (TID 19). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,726]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 19.0 (TID 19) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,726]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 19.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,726]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 19 (count at SparkModelUtils.java:252) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,727]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 14 finished: count at SparkModelUtils.java:252, took 0.019922 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,727]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:13,728]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,734]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,735]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 15 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,735]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 20(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,735]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,736]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,736]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,738]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28640) called with curMem=523794, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,738]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,742]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10805) called with curMem=552434, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,742]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,743]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_27_piece0 in memory on localhost:37067 (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,743]  INFO {org.apache.spark.SparkContext} -  Created broadcast 27 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,743]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,743]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 20.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,745]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 20.0 (TID 20, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,745]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 20.0 (TID 20) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,749]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,753]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 20.0 (TID 20). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,756]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 20.0 (TID 20) in 12 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,756]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 20.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,757]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 20 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.012 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,757]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 15 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.023069 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,758]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 36 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 11:15:13,759]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,761]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 33 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:15:13,762]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 33 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:13,773]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,773]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 37 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,774]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 16 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,774]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 22(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,774]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,774]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,775]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,776]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=547111, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,777]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,780]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1708) called with curMem=549975, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,780]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28_piece0 stored as bytes in memory (estimated size 1708.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,781]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:37067 (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,782]  INFO {org.apache.spark.SparkContext} -  Created broadcast 28 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,782]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,782]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 21.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,786]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 21.0 (TID 21, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,786]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 21.0 (TID 21) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,804]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 21.0 (TID 21). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 21.0 (TID 21) in 25 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 21.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 21 (map at MulticlassMetrics.scala:47) finished in 0.025 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 22) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,808]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 22: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,808]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,809]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=551683, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,809]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,812]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1392) called with curMem=553979, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,812]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29_piece0 stored as bytes in memory (estimated size 1392.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,813]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:37067 (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,814]  INFO {org.apache.spark.SparkContext} -  Created broadcast 29 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,814]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,814]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 22.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,815]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 22.0 (TID 22, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,815]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 22.0 (TID 22) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,820]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,820]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,827]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 22.0 (TID 22). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,830]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 22.0 (TID 22) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,830]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 22 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,830]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 22.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,830]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 16 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.057123 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,842]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 39 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,844]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 17 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,844]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 24(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,844]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,844]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,846]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,846]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=555371, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,846]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,850]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=558243, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,850]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,851]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:37067 (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,851]  INFO {org.apache.spark.SparkContext} -  Created broadcast 30 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,852]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,852]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 23.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,856]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 23.0 (TID 23, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,856]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 23.0 (TID 23) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,873]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 23.0 (TID 23). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 23.0 (TID 23) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 23 (map at MulticlassMetrics.scala:57) finished in 0.021 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 23.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,875]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,876]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 24: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,876]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,877]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=559955, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,877]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31 stored as values in memory (estimated size 2.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,881]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=562259, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,882]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:15:13,882]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:37067 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:13,883]  INFO {org.apache.spark.SparkContext} -  Created broadcast 31 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:15:13,883]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,883]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 24.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,884]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 24.0 (TID 24, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,884]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 24.0 (TID 24) {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,887]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,887]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 11:15:13,891]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 24.0 (TID 24). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:15:13,893]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 24.0 (TID 24) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:15:13,893]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 24.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:15:13,893]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 24 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:13,894]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 17 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.051177 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:15:14,147]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 3 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,154]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_31_piece0 on localhost:37067 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,157]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_30_piece0 on localhost:37067 in memory (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,163]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 6 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,164]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_29_piece0 on localhost:37067 in memory (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,165]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_28_piece0 on localhost:37067 in memory (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,167]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 5 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,168]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_27_piece0 on localhost:37067 in memory (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,170]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_26_piece0 on localhost:37067 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,172]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_25_piece0 on localhost:37067 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,179]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_24_piece0 on localhost:37067 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,187]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_23_piece0 on localhost:37067 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,190]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:14,196]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 34 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,204]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_22_piece0 on localhost:37067 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,212]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_21_piece0 on localhost:37067 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,220]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:37067 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,224]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:37067 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,225]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 4 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,227]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:37067 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,229]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:37067 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,230]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:37067 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,232]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_15_piece0 on localhost:37067 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,234]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_14_piece0 on localhost:37067 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,235]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:37067 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,236]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 2 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,237]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:37067 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,239]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:37067 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,241]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:37067 in memory (size: 8.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,242]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 1 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,244]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:37067 in memory (size: 113.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,246]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:37067 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,247]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:37067 in memory (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,249]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 0 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,250]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:37067 in memory (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,251]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:15:14,251]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 11:15:14,253]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:37067 in memory (size: 3.8 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,255]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:37067 in memory (size: 3.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,258]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:37067 in memory (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,260]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:37067 in memory (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:15:14,262]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:37067 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:17:44,841]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 11:17:44,864]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,867]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,868]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,868]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,868]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,869]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,869]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,869]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,869]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,870]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,870]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,870]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,870]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,871]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,871]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,871]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,872]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,872]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,872]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,873]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,873]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,873]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,873]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,873]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,874]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 11:17:44,928]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:17:44,931]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:17:44,989]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 11:17:44,993]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2f4af2ba-89e5-4bf9-9737-027cc98ed1fb/blockmgr-65d833e7-d349-4910-8226-ab1cc2496ac8, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 11:17:44,993]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:17:44,993]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 11:17:44,994]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:17:44,996]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 11:17:44,996]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 11:17:44,997]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 11:17:44,997]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-2f4af2ba-89e5-4bf9-9737-027cc98ed1fb {org.apache.spark.util.Utils}
[2016-06-21 11:17:45,006]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:17:45,008]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:17:45,031]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 11:17:45,930]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:17:49,520]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 11:18:17,334]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 11:18:17,440]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 11:18:17,487]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 11:18:17,487]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 11:18:17,500]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:18:17,501]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 11:18:17,501]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 11:18:17,898]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 11:18:17,949]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 11:18:18,087]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:58622] {Remoting}
[2016-06-21 11:18:18,094]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 58622. {org.apache.spark.util.Utils}
[2016-06-21 11:18:18,117]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 11:18:18,133]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 11:18:18,156]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bcba7165-03bb-420e-883d-863f6debe139/blockmgr-e7d46073-dd07-4d2a-b569-f219a6f8762a {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 11:18:18,162]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:18,189]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bcba7165-03bb-420e-883d-863f6debe139/httpd-c9cec6af-1d48-47c9-9d2f-e954e542f873 {org.apache.spark.HttpFileServer}
[2016-06-21 11:18:18,192]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 11:18:18,237]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:18:18,251]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:37021 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:18:18,251]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 37021. {org.apache.spark.util.Utils}
[2016-06-21 11:18:18,259]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 11:18:18,361]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 11:18:18,376]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 11:18:18,377]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 11:18:18,378]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 11:18:18,446]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 11:18:18,727]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58175. {org.apache.spark.util.Utils}
[2016-06-21 11:18:18,727]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 58175 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 11:18:18,728]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:18:18,732]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:58175 with 983.1 MB RAM, BlockManagerId(driver, localhost, 58175) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 11:18:18,735]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 11:18:24,387]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,391]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,631]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,632]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,635]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:18:24,638]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:18:24,724]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:18:24,777]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:18:24,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,805]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,810]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,811]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,843]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,844]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:18:24,845]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:58175 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:18:24,845]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:18:24,852]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,853]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:18:24,878]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:18:24,884]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 11:18:24,904]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466500704083:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:18:24,912]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:18:24,912]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:18:24,912]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:18:24,912]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:18:24,912]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 11:18:24,956]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:18:24,969]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 97 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:18:24,971]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:18:24,972]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.112 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:18:24,979]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.201491 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:19:20,013]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:19:20,029]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:19:20,638]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:19:20,656]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:19:20,693]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:19:20,863]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 11:19:30,137]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:19:30,334]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:19:33,165]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:19:33,335]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:25:33,348]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:33,364]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:35,158]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:35,210]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:35,312]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:35,688]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:25:36,673]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:25:37,056]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:25:37,089]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:37,092]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:37,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:37,158]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:37,201]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:37,303]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:37,317]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:37,939]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:37,958]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:25:38,002]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:58175 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:38,063]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:25:38,075]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:38,078]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:25:38,133]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:25:38,151]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 11:25:38,298]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466501132052:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:25:38,714]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:25:39,442]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 1301 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:25:39,448]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 1.323 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:39,464]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:25:39,488]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 2.413368 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:25:48,837]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:58175 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:49,152]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:58175 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:49,438]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:58175 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:49,594]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:25:49,670]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-21 11:25:49,832]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:25:49,880]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-21 11:25:50,053]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:25:50,111]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-21 11:25:50,404]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:58175 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:25:50,699]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:58175 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:30:07,889]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:07,890]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:07,912]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:07,912]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:07,915]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:30:07,916]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 11:30:07,982]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:30:07,984]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:30:07,994]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 11:30:07,994]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 11:30:37,874]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:37,966]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:40,020]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:40,036]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:40,075]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:30:40,240]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:30:41,170]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 11:30:41,544]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 11:30:41,575]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:41,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:41,579]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:41,634]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:41,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:41,831]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:41,873]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:42,346]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=88370, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:42,400]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 11:30:42,511]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:58175 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 11:30:42,630]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 11:30:42,642]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:42,644]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:30:42,692]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:30:42,709]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 11:30:42,872]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466501436644:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 11:30:43,306]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 11:30:43,941]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 1.236 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 11:30:43,962]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 1214 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 11:30:44,013]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 11:30:44,020]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 2.433550 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:50:51,559]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:58175 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:50:51,680]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:58175 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:50:51,710]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:58175 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:50:51,731]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:50:51,751]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-21 12:50:51,752]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:50:51,761]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-21 12:50:51,762]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:50:51,763]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 12:51:03,875]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:51:03,876]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:51:03,893]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:51:03,894]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:51:03,895]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:58175 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:51:03,896]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 12:51:03,953]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:51:03,954]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:51:03,980]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 39 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:51:03,981]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:52:16,021]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 12:52:16,036]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,036]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,037]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,039]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,040]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,041]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,041]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,042]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,042]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,043]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,043]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,044]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,044]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,044]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,044]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,045]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,045]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,045]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,045]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,046]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 12:52:16,098]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 12:52:16,100]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:52:16,156]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 12:52:16,160]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bcba7165-03bb-420e-883d-863f6debe139/blockmgr-e7d46073-dd07-4d2a-b569-f219a6f8762a, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 12:52:16,160]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:52:16,161]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 12:52:16,161]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 12:52:16,164]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 12:52:16,164]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 12:52:16,164]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 12:52:16,164]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bcba7165-03bb-420e-883d-863f6debe139 {org.apache.spark.util.Utils}
[2016-06-21 12:52:16,172]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 12:52:16,173]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 12:52:16,191]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 12:52:17,078]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 12:52:20,540]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 12:52:49,914]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 12:52:50,027]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 12:52:50,090]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 12:52:50,090]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 12:52:50,100]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 12:52:50,101]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 12:52:50,101]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 12:52:50,483]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 12:52:50,538]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 12:52:50,671]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:34824] {Remoting}
[2016-06-21 12:52:50,677]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 34824. {org.apache.spark.util.Utils}
[2016-06-21 12:52:50,714]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 12:52:50,736]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 12:52:50,760]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-268134e7-2545-441c-b0d6-8fda105f5664/blockmgr-3b9afae9-8e38-4c3f-ba13-91ca53f4f254 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 12:52:50,766]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:52:50,791]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-268134e7-2545-441c-b0d6-8fda105f5664/httpd-623eab44-54ea-407f-b3d2-ebb715bf0e5d {org.apache.spark.HttpFileServer}
[2016-06-21 12:52:50,794]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 12:52:50,832]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 12:52:50,843]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:39939 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 12:52:50,843]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 39939. {org.apache.spark.util.Utils}
[2016-06-21 12:52:50,851]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 12:52:50,934]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 12:52:50,943]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 12:52:50,943]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 12:52:50,945]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 12:52:51,005]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 12:52:51,320]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40982. {org.apache.spark.util.Utils}
[2016-06-21 12:52:51,320]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 40982 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 12:52:51,321]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 12:52:51,327]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:40982 with 983.1 MB RAM, BlockManagerId(driver, localhost, 40982) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 12:52:51,330]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 12:53:41,848]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:41,850]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,018]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,019]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,020]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:53:42,024]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 12:53:42,137]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 12:53:42,188]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 12:53:42,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,207]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,213]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,216]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,221]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,222]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,243]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,245]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:53:42,246]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:40982 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:53:42,247]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 12:53:42,254]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,255]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 12:53:42,287]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 12:53:42,295]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 12:53:42,316]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466506421500:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 12:53:42,323]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 12:53:42,323]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 12:53:42,323]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 12:53:42,323]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 12:53:42,323]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 12:53:42,360]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 12:53:42,375]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 93 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 12:53:42,376]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 12:53:42,377]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.109 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:53:42,386]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.197909 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:54:15,382]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:54:15,382]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:54:15,396]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:54:15,396]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:54:15,397]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:54:15,398]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 12:54:15,489]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:54:15,494]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:54:15,510]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:54:15,511]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:57:19,774]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,774]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,785]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,786]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,786]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:19,788]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 12:57:19,795]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 12:57:19,800]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 12:57:19,802]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,802]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,802]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,803]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,803]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,806]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,806]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,811]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,811]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:19,812]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:40982 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:19,812]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 12:57:19,812]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,812]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 12:57:19,814]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 12:57:19,814]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 12:57:19,817]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466506639771:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 12:57:19,824]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 12:57:19,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,829]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 12:57:19,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.028386 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 12:57:19,830]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 12:57:26,983]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:40982 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:26,985]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:26,989]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:40982 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:26,992]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:51,270]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:51,271]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:51,285]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:51,285]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 12:57:51,286]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 12:57:51,287]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 12:57:51,337]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:57:51,351]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 12:57:51,356]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 12:57:51,357]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:02:41,056]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,056]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,065]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=123642, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,066]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,067]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:02:41,067]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:02:41,077]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:02:41,083]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:02:41,084]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,084]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,084]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,085]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,085]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,086]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=127731, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,086]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,091]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=130947, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,092]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:02:41,093]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:40982 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:02:41,093]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:02:41,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,094]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:02:41,095]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:02:41,096]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 13:02:41,103]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466506961051:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:02:41,112]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:02:41,116]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:02:41,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:02:41,117]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:02:41,117]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 0.034037 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:03:12,524]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=132828, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:03:12,525]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:03:12,535]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=171316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:03:12,536]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:03:12,537]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:03:12,537]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 13:03:12,569]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:03:12,584]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:03:12,602]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 39 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:03:12,603]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:09:54,531]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 193147 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-21 13:09:54,536] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 193147 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:09:54,559]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:09:54,560]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:09:54,573]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 40982) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:09:54,574]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-21 13:09:54,574]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-21 13:09:54,573]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:09:54,574]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:10:17,922]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:40982 with 983.1 MB RAM, BlockManagerId(driver, localhost, 40982) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:10:17,928]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:10:17,943]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 10 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-21 13:10:17,945]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:17,947]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:17,948]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:17,948]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:40982 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:17,949]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:17,950]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,888]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=175405, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,888]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,897]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=213893, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,898]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,898]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,899]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:10:58,905]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:10:58,911]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:10:58,911]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,913]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,913]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[43] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,914]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3240) called with curMem=217982, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,914]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 3.2 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,946]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1901) called with curMem=221222, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,946]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 1901.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:10:58,947]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:40982 (size: 1901.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,953]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:10:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[43] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,953]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:10:58,968]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:10:58,969]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:40982 in memory (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,969]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-21 13:10:58,973]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466507458886:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:10:58,984]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,985]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:10:58,989]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-21 13:10:58,990]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:10:58,990]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:10:58,990]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-21 13:10:58,991]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:10:58,991]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 13:10:58,992]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:10:58,999]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (first at MLUtils.java:91) finished in 0.045 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,999]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: first at MLUtils.java:91, took 0.088643 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:10:58,999]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 45 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:10:59,000]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:11:30,338]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=132872, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:11:30,339]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:11:30,343]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=171360, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:11:30,344]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:11:30,344]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:11:30,345]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 13:11:30,362]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 45 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:11:30,364]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 45 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:11:30,370]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 53 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:11:30,371]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 53 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:55,029]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:15:55,124]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 4(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,129]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,129]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 4 (MapPartitionsRDD[56] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,130]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=175449, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,131]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 7.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,135]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3666) called with curMem=183057, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,136]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,136]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:40982 (size: 3.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:55,139]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,139]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[56] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,139]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,140]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,141]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,152]  INFO {org.apache.spark.CacheManager} -  Partition rdd_54_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:15:55,153]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:15:55,262]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=186723, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,262]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_54_0 stored as values in memory (estimated size 55.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,263]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_54_0 in memory on localhost:40982 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:55,280]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,295]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 155 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,295]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 4 (take at DecisionTreeMetadata.scala:110) finished in 0.155 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,295]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,296]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: take at DecisionTreeMetadata.scala:110, took 0.171768 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,323]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[56] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,330]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=243091, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,331]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 7.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,335]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3576) called with curMem=250531, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,335]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,336]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:40982 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:55,336]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,336]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[56] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,336]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,340]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,340]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,343]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_54_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:55,346]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,349]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,349]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,349]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (count at DecisionTreeMetadata.scala:111) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,351]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: count at DecisionTreeMetadata.scala:111, took 0.026206 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,459]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,460]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 6(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,461]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,462]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 6 (PartitionwiseSampledRDD[57] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,463]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8176) called with curMem=254107, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,463]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 8.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,468]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3903) called with curMem=262283, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,468]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,469]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:40982 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:55,469]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:55,469]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 6 (PartitionwiseSampledRDD[57] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,469]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,471]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,471]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,475]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_54_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:55,489]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:55,502]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 6 (collect at DecisionTree.scala:977) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,503]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 32 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:55,503]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:55,503]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: collect at DecisionTree.scala:977, took 0.043736 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:55,738]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=266186, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,739]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 624.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,772]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=266810, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,774]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:55,775]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:40982 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:55,780]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:15:56,168]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_14_piece0 on localhost:40982 in memory (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,174]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:40982 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,176]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:40982 in memory (size: 3.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,180]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:40982 in memory (size: 1901.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,184]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,278]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:15:56,282]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 60 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,282]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,282]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 8(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,283]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,283]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,286]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 7 (MapPartitionsRDD[60] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,293]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=184794, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,293]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 18.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,298]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8070) called with curMem=203642, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,299]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,299]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:40982 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,300]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:56,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[60] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,302]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:56,305]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:56,305]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:56,316]  INFO {org.apache.spark.CacheManager} -  Partition rdd_59_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:15:56,316]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_54_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:56,410]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=211712, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,410]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_59_0 stored as values in memory (estimated size 64.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,411]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_59_0 in memory on localhost:40982 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,522]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:56,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 7 (mapPartitions at DecisionTree.scala:613) finished in 0.233 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,536]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 232 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:56,536]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:56,537]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,538]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,543]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 8: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,547]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 8 (MapPartitionsRDD[62] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,548]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=277680, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,548]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,553]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=285728, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,553]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,554]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:40982 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,554]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:56,555]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[62] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,555]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:56,557]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:56,557]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:56,573]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:56,574]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 7 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:56,661]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:56,669]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 114 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:56,670]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 8 (collectAsMap at DecisionTree.scala:642) finished in 0.108 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,670]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:56,671]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: collectAsMap at DecisionTree.scala:642, took 0.392187 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:56,716]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=289297, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,719]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 1160.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,780]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=290457, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,783]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:56,785]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:40982 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:56,796]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,027]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,029]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 63 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,030]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,030]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 10(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,030]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,030]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 9 (MapPartitionsRDD[63] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,033]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=290570, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,033]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 20.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,040]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8903) called with curMem=311122, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,040]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,041]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:40982 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,041]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,041]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[63] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,041]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,042]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,043]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,045]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_59_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:57,067]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,070]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 9 (mapPartitions at DecisionTree.scala:613) finished in 0.028 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 28 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,071]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,072]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 10: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,072]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 10 (MapPartitionsRDD[65] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,073]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=320025, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,073]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 8.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,077]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3826) called with curMem=328377, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,078]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,078]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:40982 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,079]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,079]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[65] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,079]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 10.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,080]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 10.0 (TID 10, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,080]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 10.0 (TID 10) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,084]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,084]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,122]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 10.0 (TID 10). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,129]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 10 (collectAsMap at DecisionTree.scala:642) finished in 0.049 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,130]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 10.0 (TID 10) in 49 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,130]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 10.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,130]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: collectAsMap at DecisionTree.scala:642, took 0.102263 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,177]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=332203, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,180]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,221]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=334443, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,222]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,224]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_21_piece0 in memory on localhost:40982 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,229]  INFO {org.apache.spark.SparkContext} -  Created broadcast 21 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,394]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,401]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 66 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,401]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,401]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 12(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,401]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,402]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,403]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 11 (MapPartitionsRDD[66] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,405]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=334652, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,405]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,409]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10097) called with curMem=358236, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,409]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,410]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_22_piece0 in memory on localhost:40982 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,410]  INFO {org.apache.spark.SparkContext} -  Created broadcast 22 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[66] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,411]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 11.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,411]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 11.0 (TID 11, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,412]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 11.0 (TID 11) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,415]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_59_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:57,434]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 11.0 (TID 11). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 11 (mapPartitions at DecisionTree.scala:613) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 11.0 (TID 11) in 26 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 11.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,438]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 12: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,438]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 12 (MapPartitionsRDD[68] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,440]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=368333, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,440]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,444]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4134) called with curMem=377293, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,444]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,445]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_23_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,445]  INFO {org.apache.spark.SparkContext} -  Created broadcast 23 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,446]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[68] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,446]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 12.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,447]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 12.0 (TID 12, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,447]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 12.0 (TID 12) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,450]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,450]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,481]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 12.0 (TID 12). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,487]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 12 (collectAsMap at DecisionTree.scala:642) finished in 0.040 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,488]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: collectAsMap at DecisionTree.scala:642, took 0.093430 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,496]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 12.0 (TID 12) in 41 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,496]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 12.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,535]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=381427, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,537]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,570]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=383667, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,572]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,573]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_24_piece0 in memory on localhost:40982 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,578]  INFO {org.apache.spark.SparkContext} -  Created broadcast 24 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,758]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 69 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 10 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 14(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,762]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 13 (MapPartitionsRDD[69] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,764]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28960) called with curMem=383884, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,764]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25 stored as values in memory (estimated size 28.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,769]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11791) called with curMem=412844, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,769]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,769]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_25_piece0 in memory on localhost:40982 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,770]  INFO {org.apache.spark.SparkContext} -  Created broadcast 25 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,770]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[69] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,770]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 13.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,771]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 13.0 (TID 13, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,771]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 13.0 (TID 13) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,775]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_59_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:57,790]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 13.0 (TID 13). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,793]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 13.0 (TID 13) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,793]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 13 (mapPartitions at DecisionTree.scala:613) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,793]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 13.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,793]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,793]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 14) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 14: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 14 (MapPartitionsRDD[71] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,795]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=424635, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,795]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,798]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4006) called with curMem=433595, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,799]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,799]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_26_piece0 in memory on localhost:40982 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,799]  INFO {org.apache.spark.SparkContext} -  Created broadcast 26 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:57,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[71] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,800]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 14.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,800]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 14.0 (TID 14, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,801]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 14.0 (TID 14) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,804]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,804]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:57,827]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 14.0 (TID 14). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:57,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 14 (collectAsMap at DecisionTree.scala:642) finished in 0.033 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,835]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 14.0 (TID 14) in 33 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:57,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 10 finished: collectAsMap at DecisionTree.scala:642, took 0.075930 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:57,835]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 14.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:57,877]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=437601, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,879]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=439409, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,917]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:57,918]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_27_piece0 in memory on localhost:40982 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:57,923]  INFO {org.apache.spark.SparkContext} -  Created broadcast 27 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,114]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 72 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 11 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 16(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,116]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 15) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,117]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 15) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,118]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 15 (MapPartitionsRDD[72] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,120]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=439582, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,120]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28 stored as values in memory (estimated size 33.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,123]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13181) called with curMem=473486, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,123]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,124]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:40982 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,124]  INFO {org.apache.spark.SparkContext} -  Created broadcast 28 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,124]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[72] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,124]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 15.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,125]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 15.0 (TID 15, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,126]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 15.0 (TID 15) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,130]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_59_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,144]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 15.0 (TID 15). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 15.0 (TID 15) in 23 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 15.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 15 (mapPartitions at DecisionTree.scala:613) finished in 0.023 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 16) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,148]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 16: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 16 (MapPartitionsRDD[74] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,152]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=486667, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,152]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29 stored as values in memory (estimated size 8.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,155]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4013) called with curMem=495387, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,155]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,156]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:40982 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,156]  INFO {org.apache.spark.SparkContext} -  Created broadcast 29 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,157]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[74] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,157]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 16.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,157]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 16.0 (TID 16, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,157]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 16.0 (TID 16) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,161]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:58,161]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:58,183]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 16.0 (TID 16). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,190]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 16 (collectAsMap at DecisionTree.scala:642) finished in 0.032 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,191]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 11 finished: collectAsMap at DecisionTree.scala:642, took 0.076070 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,197]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 16.0 (TID 16) in 32 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,197]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 16.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,199]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 59 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:15:58,202]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 59 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,207]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 13:15:58,212]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.719801753
  total: 3.214479589
  findSplitsBins: 0.2429917
  findBestSplits: 2.41335623
  chooseSplits: 2.376760845 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 13:15:58,223]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 54 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:15:58,224]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 54 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,479]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,480]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 12 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,481]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 17(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,481]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,484]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 17 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,486]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28664) called with curMem=377064, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,487]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,490]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10831) called with curMem=405728, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,491]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,491]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:40982 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,492]  INFO {org.apache.spark.SparkContext} -  Created broadcast 30 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,492]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 17.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,493]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 17.0 (TID 17, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,493]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 17.0 (TID 17) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,501]  INFO {org.apache.spark.CacheManager} -  Partition rdd_75_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:15:58,502]  INFO {org.apache.spark.CacheManager} -  Partition rdd_55_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:15:58,502]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:15:58,528]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=416559, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,528]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_55_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,529]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_55_0 in memory on localhost:40982 (size: 29.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,551]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=446255, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,551]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_75_0 stored as values in memory (estimated size 15.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,551]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_75_0 in memory on localhost:40982 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,555]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 17.0 (TID 17). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,557]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 17.0 (TID 17) in 64 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 17 (take at SparkModelUtils.java:215) finished in 0.065 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,557]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 17.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,558]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 12 finished: take at SparkModelUtils.java:215, took 0.078683 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,662]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,665]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 13 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,665]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 18(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,665]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,666]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,666]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 18 (MapPartitionsRDD[55] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,667]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=462383, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,667]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31 stored as values in memory (estimated size 7.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,670]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3546) called with curMem=469655, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,670]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,671]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:40982 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,671]  INFO {org.apache.spark.SparkContext} -  Created broadcast 31 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,672]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[55] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,672]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 18.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,673]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 18.0 (TID 18, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,673]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 18.0 (TID 18) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,675]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_55_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,678]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 18.0 (TID 18). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,686]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 18.0 (TID 18) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,686]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 18.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 18 (take at SparkModelUtils.java:223) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 13 finished: take at SparkModelUtils.java:223, took 0.023822 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,758]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 14 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 19(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 19 (ParallelCollectionRDD[76] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,764]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=473201, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,765]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32 stored as values in memory (estimated size 1320.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,768]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=474521, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,769]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,769]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:40982 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,770]  INFO {org.apache.spark.SparkContext} -  Created broadcast 32 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,771]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 19 (ParallelCollectionRDD[76] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,771]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 19.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,780]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 19.0 (TID 19, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,781]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 19.0 (TID 19) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,790]  INFO {org.apache.spark.CacheManager} -  Partition rdd_76_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:15:58,794]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=475398, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,794]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_76_0 stored as values in memory (estimated size 32.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,795]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_76_0 in memory on localhost:40982 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,798]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 19.0 (TID 19). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,800]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 19.0 (TID 19) in 29 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 19 (count at SparkModelUtils.java:241) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,800]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 19.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,801]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 14 finished: count at SparkModelUtils.java:241, took 0.042055 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,841]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 15 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 20(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,842]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 20 (ParallelCollectionRDD[76] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,844]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=508486, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,844]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33 stored as values in memory (estimated size 1496.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,847]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=509982, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,847]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,848]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:40982 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,848]  INFO {org.apache.spark.SparkContext} -  Created broadcast 33 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,848]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 20 (ParallelCollectionRDD[76] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,849]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 20.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,851]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 20.0 (TID 20, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,851]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 20.0 (TID 20) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,855]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_76_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,859]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 20.0 (TID 20). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,862]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 20.0 (TID 20) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,862]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 20.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,862]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 20 (collect at SparkModelUtils.java:245) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,863]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 15 finished: collect at SparkModelUtils.java:245, took 0.021356 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,867]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 76 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 13:15:58,868]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 76 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,910]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 16 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 21(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,912]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,913]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,913]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 21 (MapPartitionsRDD[77] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=477824, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,915]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,919]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10903) called with curMem=506536, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,919]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,920]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_34_piece0 in memory on localhost:40982 (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,920]  INFO {org.apache.spark.SparkContext} -  Created broadcast 34 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,921]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[77] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,921]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 21.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,922]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 21.0 (TID 21, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,922]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 21.0 (TID 21) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,925]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_75_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,927]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 21.0 (TID 21). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,930]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 21.0 (TID 21) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,930]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 21 (count at SparkModelUtils.java:252) finished in 0.005 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,930]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 21.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,931]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 16 finished: count at SparkModelUtils.java:252, took 0.020058 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,951]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 17 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 22(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,953]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 22 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,955]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=517439, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,955]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_35 stored as values in memory (estimated size 27.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,958]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10740) called with curMem=545935, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,958]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:58,959]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_35_piece0 in memory on localhost:40982 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:58,960]  INFO {org.apache.spark.SparkContext} -  Created broadcast 35 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:58,960]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,960]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 22.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,960]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 22.0 (TID 22, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,961]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 22.0 (TID 22) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,964]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_75_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:58,966]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 22.0 (TID 22). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:58,968]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 22.0 (TID 22) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:58,968]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 22.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:58,969]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 22 (count at SparkModelUtils.java:252) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,970]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 17 finished: count at SparkModelUtils.java:252, took 0.017847 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:58,971]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 55 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:15:58,972]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 55 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:59,128]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,130]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 18 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,130]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 23(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,130]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 23 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,132]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28648) called with curMem=526979, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,132]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_36 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,135]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10796) called with curMem=555627, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,135]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_36_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,136]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_36_piece0 in memory on localhost:40982 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:59,136]  INFO {org.apache.spark.SparkContext} -  Created broadcast 36 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,136]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[75] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,136]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 23.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,139]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 23.0 (TID 23, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,139]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 23.0 (TID 23) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,141]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_75_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:59,143]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 23.0 (TID 23). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,145]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 23.0 (TID 23) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,145]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 23.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,145]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 23 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,146]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 18 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.016514 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,163]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 78 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 13:15:59,165]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 78 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:59,166]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 75 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:15:59,168]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 75 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:15:59,262]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 79 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 19 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 25(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,265]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,267]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 24 (MapPartitionsRDD[79] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,267]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=550295, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,267]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_37 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,270]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1708) called with curMem=553159, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,270]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_37_piece0 stored as bytes in memory (estimated size 1708.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,271]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_37_piece0 in memory on localhost:40982 (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:59,272]  INFO {org.apache.spark.SparkContext} -  Created broadcast 37 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,272]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[79] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,272]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 24.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,280]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 24.0 (TID 24, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,280]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 24.0 (TID 24) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,299]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 24.0 (TID 24). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 24.0 (TID 24) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 24 (map at MulticlassMetrics.scala:47) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 24.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 25) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 25: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 25 (ShuffledRDD[80] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,303]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=554867, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,303]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_38 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,306]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1392) called with curMem=557163, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,306]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_38_piece0 stored as bytes in memory (estimated size 1392.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,307]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_38_piece0 in memory on localhost:40982 (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:59,307]  INFO {org.apache.spark.SparkContext} -  Created broadcast 38 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,307]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[80] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,307]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 25.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,308]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 25.0 (TID 25, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,308]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 25.0 (TID 25) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,311]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:59,311]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:59,314]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 25.0 (TID 25). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,317]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 25.0 (TID 25) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,317]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 25 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,317]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 25.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 19 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.054607 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,415]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,416]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 81 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,416]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 20 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,416]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 27(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,416]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 26) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 26) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 26 (MapPartitionsRDD[81] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,418]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=558555, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,418]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_39 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,420]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=561427, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,420]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_39_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,421]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_39_piece0 in memory on localhost:40982 (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:59,421]  INFO {org.apache.spark.SparkContext} -  Created broadcast 39 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,421]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[81] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,421]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 26.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,424]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 26.0 (TID 26, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,424]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 26.0 (TID 26) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,441]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 26.0 (TID 26). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,443]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 26.0 (TID 26) in 21 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,443]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 26.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 26 (map at MulticlassMetrics.scala:57) finished in 0.022 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 27) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 27: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 27 (ShuffledRDD[82] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,445]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=563139, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,445]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_40 stored as values in memory (estimated size 2.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,447]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=565443, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,447]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_40_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:15:59,447]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_40_piece0 in memory on localhost:40982 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:15:59,448]  INFO {org.apache.spark.SparkContext} -  Created broadcast 40 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:15:59,448]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[82] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,448]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 27.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,449]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 27.0 (TID 27, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,449]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 27.0 (TID 27) {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,452]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:59,452]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:15:59,455]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 27.0 (TID 27). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:15:59,457]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 27.0 (TID 27) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:15:59,457]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 27 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:15:59,457]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 27.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:15:59,458]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 20 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.042289 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,774]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=566833, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,774]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_41 stored as values in memory (estimated size 37.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,780]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=605321, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,780]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_41_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,780]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_41_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:19:28,781]  INFO {org.apache.spark.SparkContext} -  Created broadcast 41 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:19:28,790]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:19:28,794]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:19:28,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 21 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,794]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 28(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,795]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 28 (MapPartitionsRDD[84] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,796]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=609410, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,796]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_42 stored as values in memory (estimated size 3.1 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,799]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=612626, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,799]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_42_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:19:28,799]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_42_piece0 in memory on localhost:40982 (size: 1881.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:19:28,800]  INFO {org.apache.spark.SparkContext} -  Created broadcast 42 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:19:28,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[84] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,800]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 28.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:19:28,801]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 28.0 (TID 28, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:19:28,802]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 28.0 (TID 28) {org.apache.spark.executor.Executor}
[2016-06-21 13:19:28,804]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466507968773:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:19:28,807]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 28.0 (TID 28). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:19:28,809]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 28.0 (TID 28) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:19:28,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 28 (first at MLUtils.java:91) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:19:28,810]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 28.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:19:28,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 21 finished: first at MLUtils.java:91, took 0.015713 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:20:00,162]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=614507, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:20:00,162]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_43 stored as values in memory (estimated size 37.6 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:20:00,167]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=652995, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:20:00,167]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_43_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.4 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:20:00,167]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_43_piece0 in memory on localhost:40982 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,168]  INFO {org.apache.spark.SparkContext} -  Created broadcast 43 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 13:20:00,185]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_26_piece0 on localhost:40982 in memory (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,190]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:40982 in memory (size: 7.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,201]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 86 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:20:00,204]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 86 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:20:00,205]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_42_piece0 on localhost:40982 in memory (size: 1881.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,223]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 94 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:20:00,226]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_41_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,226]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 94 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:20:00,314]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_40_piece0 on localhost:40982 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:20:00,339]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_39_piece0 on localhost:40982 in memory (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:00,326]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 6 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:00,339]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_38_piece0 on localhost:40982 in memory (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:00,465]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_37_piece0 on localhost:40982 in memory (size: 1708.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:00,472]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 5 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:00,472]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 78 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:21:00,472]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 78 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:00,473]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_36_piece0 on localhost:40982 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,698]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_35_piece0 on localhost:40982 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,704]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_34_piece0 on localhost:40982 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,732]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_33_piece0 on localhost:40982 in memory (size: 930.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,737]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_32_piece0 on localhost:40982 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,738]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 76 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:21:23,739]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 76 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:23,743]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_31_piece0 on localhost:40982 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,745]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_30_piece0 on localhost:40982 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:23,746]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 75 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:21:23,749]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 75 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:48,310]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_29_piece0 on localhost:40982 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:48,319]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_28_piece0 on localhost:40982 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:48,327]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 4 {org.apache.spark.ContextCleaner}
[2016-06-21 13:21:52,643]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_27_piece0 on localhost:40982 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:21:52,648]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_25_piece0 on localhost:40982 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,023]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 3 {org.apache.spark.ContextCleaner}
[2016-06-21 13:22:05,029]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_24_piece0 on localhost:40982 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,032]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_23_piece0 on localhost:40982 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,035]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_22_piece0 on localhost:40982 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,040]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 2 {org.apache.spark.ContextCleaner}
[2016-06-21 13:22:05,041]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_21_piece0 on localhost:40982 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,042]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:40982 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,044]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:40982 in memory (size: 8.7 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,051]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 1 {org.apache.spark.ContextCleaner}
[2016-06-21 13:22:05,052]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:40982 in memory (size: 113.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:22:05,054]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:40982 in memory (size: 3.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:42:47,055]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 13:42:47,072]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,076]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,077]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,078]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,079]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,079]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,080]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,081]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,086]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,086]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,086]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,087]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,088]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,088]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,089]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,090]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,090]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,091]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,091]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,091]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,091]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,091]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 13:42:47,144]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 13:42:47,146]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:42:47,205]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 13:42:47,208]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-268134e7-2545-441c-b0d6-8fda105f5664/blockmgr-3b9afae9-8e38-4c3f-ba13-91ca53f4f254, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 13:42:47,208]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:42:47,209]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 13:42:47,210]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:42:47,211]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 13:42:47,212]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 13:42:47,212]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 13:42:47,212]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-268134e7-2545-441c-b0d6-8fda105f5664 {org.apache.spark.util.Utils}
[2016-06-21 13:42:47,220]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 13:42:47,221]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 13:42:47,239]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 13:42:48,115]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 13:42:51,715]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 13:43:22,173]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 13:43:22,286]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 13:43:22,350]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 13:43:22,350]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 13:43:22,368]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 13:43:22,369]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 13:43:22,369]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 13:43:22,741]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 13:43:22,779]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 13:43:22,897]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:57608] {Remoting}
[2016-06-21 13:43:22,899]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 57608. {org.apache.spark.util.Utils}
[2016-06-21 13:43:22,917]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 13:43:22,931]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 13:43:22,948]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-475d5841-ca42-44b6-818c-fd1faa577633/blockmgr-8b0efbc2-fc8e-4848-937d-204698d85461 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 13:43:22,952]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:22,972]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-475d5841-ca42-44b6-818c-fd1faa577633/httpd-93d284f4-313c-49a7-a940-8606a2e30ddf {org.apache.spark.HttpFileServer}
[2016-06-21 13:43:22,974]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 13:43:23,008]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 13:43:23,021]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:60013 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 13:43:23,021]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 60013. {org.apache.spark.util.Utils}
[2016-06-21 13:43:23,029]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 13:43:23,111]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 13:43:23,121]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 13:43:23,123]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 13:43:23,124]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 13:43:23,184]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 13:43:23,337]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42987. {org.apache.spark.util.Utils}
[2016-06-21 13:43:23,337]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 42987 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 13:43:23,338]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:43:23,343]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:42987 with 983.1 MB RAM, BlockManagerId(driver, localhost, 42987) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:43:23,345]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:43:46,809]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:46,812]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:46,983]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:46,984]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:46,986]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:43:46,989]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:43:47,078]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:43:47,140]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:43:47,159]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,160]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,160]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,165]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,169]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,174]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:47,175]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:47,187]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1886) called with curMem=45737, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:47,187]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1886.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:43:47,188]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:42987 (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:43:47,189]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:43:47,202]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,204]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:43:47,241]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:43:47,252]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 13:43:47,277]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466509426530:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:43:47,286]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 13:43:47,286]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 13:43:47,286]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 13:43:47,287]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 13:43:47,287]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 13:43:47,339]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:43:47,355]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.135 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:43:47,358]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 123 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:43:47,360]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:43:47,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.224540 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:44:19,208]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47623, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:44:19,208]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:44:19,220]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86111, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:44:19,221]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:44:19,221]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:44:19,222]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 13:44:19,318]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:44:19,323]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:44:19,340]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:44:19,341]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:16,905]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:46:17,013]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,026]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,026]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,030]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=90200, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,030]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 7.4 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,037]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3675) called with curMem=97808, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,037]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,038]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:42987 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:17,039]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,040]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,040]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,042]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,042]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,062]  INFO {org.apache.spark.CacheManager} -  Partition rdd_12_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:46:17,063]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:46:17,187]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=101483, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,187]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_12_0 stored as values in memory (estimated size 55.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,188]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_12_0 in memory on localhost:42987 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:17,203]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,236]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (take at DecisionTreeMetadata.scala:110) finished in 0.196 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,237]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 196 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,237]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: take at DecisionTreeMetadata.scala:110, took 0.223151 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,238]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,267]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,269]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,269]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,269]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,272]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,272]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,275]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=157851, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,275]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 7.3 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,281]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3585) called with curMem=165291, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,282]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,282]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:42987 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:17,283]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,283]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,283]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,288]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,289]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,294]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:17,299]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,305]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (count at DecisionTreeMetadata.scala:111) finished in 0.018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,305]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 19 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,307]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,308]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: count at DecisionTreeMetadata.scala:111, took 0.038256 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,433]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,436]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,437]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,440]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8168) called with curMem=168876, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,440]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 8.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,445]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3910) called with curMem=177044, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,446]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,447]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:42987 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:17,447]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:17,448]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (PartitionwiseSampledRDD[15] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,448]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,450]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,450]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,456]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:17,472]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:17,487]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 38 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:17,487]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:17,487]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (collect at DecisionTree.scala:977) finished in 0.039 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,488]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: collect at DecisionTree.scala:977, took 0.054619 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:17,742]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=180954, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,744]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 624.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,776]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=181578, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,777]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:17,779]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:42987 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:17,784]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,059]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,063]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 18 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,064]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,064]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,064]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,065]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 4) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,068]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,076]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=181649, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,076]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 18.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,082]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8081) called with curMem=200497, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,083]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,083]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:42987 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,084]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,086]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[18] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,086]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,088]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,089]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,098]  INFO {org.apache.spark.CacheManager} -  Partition rdd_17_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:46:18,098]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_12_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:18,164]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=208578, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,165]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_17_0 stored as values in memory (estimated size 64.4 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,165]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_17_0 in memory on localhost:42987 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,253]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,263]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 176 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,263]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,263]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 4 (mapPartitions at DecisionTree.scala:613) finished in 0.176 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,263]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,264]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,264]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 5) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,264]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,266]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 5: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,268]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,270]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=274546, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,270]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 7.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,275]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=282594, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,275]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,276]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:42987 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,276]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,277]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,279]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,279]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,297]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:18,299]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 9 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:18,399]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,410]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 132 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,410]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (collectAsMap at DecisionTree.scala:642) finished in 0.126 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,410]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,411]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: collectAsMap at DecisionTree.scala:642, took 0.350867 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,447]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=286163, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,450]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 1160.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,506]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=287323, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,509]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,512]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:42987 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,520]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,810]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 21 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 7(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,813]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,814]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,816]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,818]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=287436, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,818]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 20.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,824]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8914) called with curMem=307988, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,824]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,824]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:42987 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,825]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,825]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,826]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,826]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,832]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:18,860]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 6 (mapPartitions at DecisionTree.scala:613) finished in 0.038 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,864]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,865]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 38 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,865]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,866]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 7: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,866]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,867]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=316902, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,868]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 8.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,872]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3826) called with curMem=325254, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,872]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,873]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:42987 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:18,874]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:18,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,874]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,875]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,875]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,879]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:18,880]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:18,913]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:18,921]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 46 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:18,921]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 7 (collectAsMap at DecisionTree.scala:642) finished in 0.038 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,921]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:18,922]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: collectAsMap at DecisionTree.scala:642, took 0.111010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:18,965]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=329080, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:18,968]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,021]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=331320, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,024]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,026]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:42987 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,035]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,203]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,205]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 24 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,205]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 9(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,208]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,210]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=331529, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,211]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 23.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,215]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10108) called with curMem=355113, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,216]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,217]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:42987 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,217]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,218]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[24] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,218]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,219]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,219]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,223]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:19,245]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 30 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 8 (mapPartitions at DecisionTree.scala:613) finished in 0.029 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,248]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 9: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,250]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,251]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=365221, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,251]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,256]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4054) called with curMem=374181, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,257]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,257]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,258]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,258]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,258]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,259]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,259]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,265]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:19,265]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:19,300]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,307]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 9 (collectAsMap at DecisionTree.scala:642) finished in 0.043 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,307]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 47 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,307]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,307]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: collectAsMap at DecisionTree.scala:642, took 0.103712 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,335]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=378235, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,337]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 2.2 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,372]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=380475, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,373]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,375]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:42987 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,381]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,558]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 27 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 11(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,564]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,565]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,566]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28952) called with curMem=380692, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,567]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 28.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,572]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11802) called with curMem=409644, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,573]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,573]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:42987 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,574]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[27] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,574]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 10.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,575]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 10.0 (TID 10, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,575]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 10.0 (TID 10) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,578]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:19,596]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 10.0 (TID 10). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,599]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 10.0 (TID 10) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,599]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 10 (mapPartitions at DecisionTree.scala:613) finished in 0.024 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,599]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,600]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 10.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,600]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,600]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,600]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,601]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 11: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,601]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,603]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=421446, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,603]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,607]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4004) called with curMem=430406, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,608]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,609]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:42987 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,609]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,610]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[29] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,610]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 11.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,611]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 11.0 (TID 11, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,611]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 11.0 (TID 11) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,616]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:19,616]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:19,648]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 11.0 (TID 11). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,655]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 11 (collectAsMap at DecisionTree.scala:642) finished in 0.044 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,657]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: collectAsMap at DecisionTree.scala:642, took 0.097084 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,660]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 11.0 (TID 11) in 45 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,660]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 11.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,699]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=434410, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,702]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,734]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=436218, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,735]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,737]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:42987 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,742]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,921]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 30 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 13(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,927]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,928]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,930]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=436391, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,930]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 33.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,935]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13192) called with curMem=470295, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,935]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,936]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:42987 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,936]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,936]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[30] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,936]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 12.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,937]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 12.0 (TID 12, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,937]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 12.0 (TID 12) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,942]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_17_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:19,960]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 12.0 (TID 12). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 12 (mapPartitions at DecisionTree.scala:613) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,964]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 12.0 (TID 12) in 26 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,964]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 12.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 13: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,965]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,965]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=483487, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,966]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 8.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,969]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4009) called with curMem=492207, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,969]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:19,970]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:42987 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:19,970]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:19,971]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[32] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:19,971]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 13.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:19,971]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 13.0 (TID 13, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:19,972]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 13.0 (TID 13) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:19,978]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:19,978]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:20,000]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 13.0 (TID 13). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,007]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 13 (collectAsMap at DecisionTree.scala:642) finished in 0.036 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,009]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: collectAsMap at DecisionTree.scala:642, took 0.086701 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,011]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 13.0 (TID 13) in 36 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,011]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 13.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,017]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:46:20,021]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,027]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 13:46:20,031]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.89518182
  total: 3.205163216
  findSplitsBins: 0.27434414
  findBestSplits: 2.239636922
  chooseSplits: 2.208626058 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-21 13:46:20,041]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 12 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:46:20,043]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,351]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,354]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,354]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 14(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,354]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,360]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,360]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,362]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28656) called with curMem=373880, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,362]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,366]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10841) called with curMem=402536, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,367]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,367]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_21_piece0 in memory on localhost:42987 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,368]  INFO {org.apache.spark.SparkContext} -  Created broadcast 21 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,368]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,368]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 14.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,369]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 14.0 (TID 14, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,369]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 14.0 (TID 14) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,378]  INFO {org.apache.spark.CacheManager} -  Partition rdd_33_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:46:20,379]  INFO {org.apache.spark.CacheManager} -  Partition rdd_13_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:46:20,379]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:46:20,406]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=413377, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,406]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_13_0 stored as values in memory (estimated size 29.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,407]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_13_0 in memory on localhost:42987 (size: 29.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,425]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=443073, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,425]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_33_0 stored as values in memory (estimated size 15.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,425]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_33_0 in memory on localhost:42987 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,429]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 14.0 (TID 14). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 14 (take at SparkModelUtils.java:215) finished in 0.064 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: take at SparkModelUtils.java:215, took 0.081585 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,439]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 14.0 (TID 14) in 64 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,439]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 14.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,536]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 10 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 15(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,539]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,540]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,540]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,541]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=459201, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,541]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22 stored as values in memory (estimated size 7.1 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,546]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3553) called with curMem=466473, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,546]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,546]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_22_piece0 in memory on localhost:42987 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,547]  INFO {org.apache.spark.SparkContext} -  Created broadcast 22 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,547]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[13] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,547]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 15.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,548]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 15.0 (TID 15, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,548]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 15.0 (TID 15) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,551]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_13_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,555]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 15.0 (TID 15). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,561]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 15.0 (TID 15) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,561]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 15.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,562]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 15 (take at SparkModelUtils.java:223) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,563]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 10 finished: take at SparkModelUtils.java:223, took 0.025660 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,614]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 11 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 16(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,618]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=470026, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,619]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23 stored as values in memory (estimated size 1320.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,621]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=471346, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,622]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,622]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_23_piece0 in memory on localhost:42987 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,623]  INFO {org.apache.spark.SparkContext} -  Created broadcast 23 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,623]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 16 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,623]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 16.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,632]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 16.0 (TID 16, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,632]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 16.0 (TID 16) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,640]  INFO {org.apache.spark.CacheManager} -  Partition rdd_34_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-21 13:46:20,643]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=472223, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,643]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_34_0 stored as values in memory (estimated size 32.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,644]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_34_0 in memory on localhost:42987 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,646]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 16.0 (TID 16). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,648]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 16.0 (TID 16) in 24 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,648]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 16.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,649]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 16 (count at SparkModelUtils.java:241) finished in 0.025 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,650]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 11 finished: count at SparkModelUtils.java:241, took 0.034591 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,681]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 12 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 17(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,683]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,684]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=505311, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,684]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24 stored as values in memory (estimated size 1496.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,687]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(931) called with curMem=506807, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,688]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24_piece0 stored as bytes in memory (estimated size 931.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,688]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_24_piece0 in memory on localhost:42987 (size: 931.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,688]  INFO {org.apache.spark.SparkContext} -  Created broadcast 24 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,689]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 17 (ParallelCollectionRDD[34] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,689]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 17.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,692]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 17.0 (TID 17, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,692]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 17.0 (TID 17) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,696]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_34_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,698]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 17.0 (TID 17). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,702]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 17 (collect at SparkModelUtils.java:245) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,702]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 17.0 (TID 17) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,702]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 17.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,703]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 12 finished: collect at SparkModelUtils.java:245, took 0.021298 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,709]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 34 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 13:46:20,710]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,759]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,760]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 13 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 18(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,762]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,763]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=474650, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,763]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,767]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10913) called with curMem=503362, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,767]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,768]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_25_piece0 in memory on localhost:42987 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,768]  INFO {org.apache.spark.SparkContext} -  Created broadcast 25 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,769]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[35] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,769]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 18.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,770]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 18.0 (TID 18, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,770]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 18.0 (TID 18) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,774]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,776]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 18.0 (TID 18). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,778]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 18.0 (TID 18) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 18 (count at SparkModelUtils.java:252) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,779]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 18.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,779]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 13 finished: count at SparkModelUtils.java:252, took 0.019811 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,797]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 14 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 19(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,799]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,800]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,802]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=514275, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,802]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26 stored as values in memory (estimated size 27.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,805]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10750) called with curMem=542771, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,805]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:20,806]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_26_piece0 in memory on localhost:42987 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:20,806]  INFO {org.apache.spark.SparkContext} -  Created broadcast 26 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:20,807]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,807]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 19.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,807]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 19.0 (TID 19, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,808]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 19.0 (TID 19) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,810]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:20,812]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 19.0 (TID 19). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:20,815]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 19.0 (TID 19) in 8 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:20,816]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 19 (count at SparkModelUtils.java:252) finished in 0.008 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,816]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 19.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:20,817]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 14 finished: count at SparkModelUtils.java:252, took 0.018328 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:20,817]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:46:20,819]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:21,014]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 15 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 20(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,017]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,018]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28640) called with curMem=523825, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,018]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27 stored as values in memory (estimated size 28.0 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,021]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10806) called with curMem=552465, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,022]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,022]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_27_piece0 in memory on localhost:42987 (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:21,023]  INFO {org.apache.spark.SparkContext} -  Created broadcast 27 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,023]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[33] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,023]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 20.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,024]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 20.0 (TID 20, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,024]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 20.0 (TID 20) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,027]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_33_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:21,031]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 20.0 (TID 20). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,034]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 20.0 (TID 20) in 11 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,034]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 20 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,034]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 20.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,035]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 15 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.019591 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,062]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 36 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-21 13:46:21,064]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:21,068]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 33 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:46:21,069]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 33 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:46:21,229]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 37 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 16 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 22(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,234]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 21) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,235]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,236]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=547143, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,236]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,239]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1709) called with curMem=550007, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,239]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28_piece0 stored as bytes in memory (estimated size 1709.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,240]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:42987 (size: 1709.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:21,240]  INFO {org.apache.spark.SparkContext} -  Created broadcast 28 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[37] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,240]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 21.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,244]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 21.0 (TID 21, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,244]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 21.0 (TID 21) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,258]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 21.0 (TID 21). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 21.0 (TID 21) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 21.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 21 (map at MulticlassMetrics.scala:47) finished in 0.018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 22) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,262]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 22: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,262]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,263]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=551716, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,263]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29 stored as values in memory (estimated size 2.2 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,266]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1392) called with curMem=554012, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,266]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29_piece0 stored as bytes in memory (estimated size 1392.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,267]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:42987 (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:21,267]  INFO {org.apache.spark.SparkContext} -  Created broadcast 29 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,267]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 22 (ShuffledRDD[38] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,267]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 22.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,268]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 22.0 (TID 22, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,268]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 22.0 (TID 22) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,271]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:21,271]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:21,276]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 22.0 (TID 22). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,278]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 22.0 (TID 22) in 10 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 22 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.011 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,278]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 22.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,280]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 16 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.049483 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,376]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 39 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 17 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 24(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,379]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,380]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=555404, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,380]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30 stored as values in memory (estimated size 2.8 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,382]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1713) called with curMem=558276, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,382]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30_piece0 stored as bytes in memory (estimated size 1713.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,383]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:42987 (size: 1713.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:21,383]  INFO {org.apache.spark.SparkContext} -  Created broadcast 30 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,383]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,383]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 23.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,386]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 23.0 (TID 23, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,386]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 23.0 (TID 23) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,398]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 23.0 (TID 23). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 23.0 (TID 23) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 23.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 23 (map at MulticlassMetrics.scala:57) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 24: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,400]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,401]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=559989, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,401]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31 stored as values in memory (estimated size 2.3 KB, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,404]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=562293, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,405]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.5 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:46:21,405]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:42987 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:46:21,405]  INFO {org.apache.spark.SparkContext} -  Created broadcast 31 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:46:21,406]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,406]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 24.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,406]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 24.0 (TID 24, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,406]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 24.0 (TID 24) {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,409]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:21,409]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-21 13:46:21,413]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 24.0 (TID 24). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:46:21,415]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 24.0 (TID 24) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:46:21,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 24 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:46:21,415]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 24.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:46:21,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 17 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.039312 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:47:22,471]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:42987 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,907]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_31_piece0 on localhost:42987 in memory (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,922]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_30_piece0 on localhost:42987 in memory (size: 1713.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,927]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 6 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,929]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_29_piece0 on localhost:42987 in memory (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,935]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_28_piece0 on localhost:42987 in memory (size: 1709.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,936]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 5 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,937]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:47:31,938]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 36 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,939]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_27_piece0 on localhost:42987 in memory (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,940]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_26_piece0 on localhost:42987 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,941]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_25_piece0 on localhost:42987 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,942]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_24_piece0 on localhost:42987 in memory (size: 931.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,944]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_23_piece0 on localhost:42987 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,946]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 34 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:47:31,946]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 34 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,947]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_22_piece0 on localhost:42987 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,950]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_21_piece0 on localhost:42987 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,951]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 33 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:47:31,951]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 33 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,953]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:42987 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,957]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:42987 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,959]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 4 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,961]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:42987 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,962]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:42987 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,964]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:42987 in memory (size: 11.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,965]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 3 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,966]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_15_piece0 on localhost:42987 in memory (size: 217.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,967]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_14_piece0 on localhost:42987 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,968]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:42987 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,969]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 2 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,970]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:42987 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,972]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:42987 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,976]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:42987 in memory (size: 8.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,977]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 1 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,978]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:42987 in memory (size: 113.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,979]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:42987 in memory (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,980]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 0 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,981]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:42987 in memory (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,982]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:47:31,982]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-21 13:47:31,983]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:42987 in memory (size: 3.8 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,985]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:42987 in memory (size: 3.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,986]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:42987 in memory (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,988]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:42987 in memory (size: 1886.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:47:31,989]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:42987 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:50:03,657]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,657]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,664]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,664]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,665]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:50:03,665]  INFO {org.apache.spark.SparkContext} -  Created broadcast 32 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:50:03,673]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 13:50:03,678]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 13:50:03,679]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 18 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,679]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 25(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,679]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,680]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,680]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 25 (MapPartitionsRDD[42] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,681]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,681]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,684]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=88370, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,684]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:03,685]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:42987 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:50:03,685]  INFO {org.apache.spark.SparkContext} -  Created broadcast 33 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 13:50:03,686]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[42] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,686]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 25.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:50:03,686]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 25.0 (TID 25, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:50:03,687]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 25.0 (TID 25) {org.apache.spark.executor.Executor}
[2016-06-21 13:50:03,689]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466509803655:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 13:50:03,693]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 25.0 (TID 25). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 13:50:03,695]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 25.0 (TID 25) in 9 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 13:50:03,695]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 25.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:50:03,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 25 (first at MLUtils.java:91) finished in 0.009 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:03,696]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 18 finished: first at MLUtils.java:91, took 0.017599 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:50:35,110]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90251, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:35,110]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:35,120]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128739, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:35,120]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 13:50:35,121]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_34_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:50:35,127]  INFO {org.apache.spark.SparkContext} -  Created broadcast 34 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 13:50:35,158]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 44 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:50:35,165]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 44 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:50:35,190]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 52 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 13:50:35,190]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 52 {org.apache.spark.storage.BlockManager}
[2016-06-21 13:53:37,681]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 131203 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-21 13:53:37,681] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 131203 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 13:53:37,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:53:37,684]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:53:40,367]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 42987) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:53:40,368]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-21 13:53:40,368]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-21 13:53:40,369]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:53:40,374]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:42987 with 983.1 MB RAM, BlockManagerId(driver, localhost, 42987) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 13:53:40,384]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:53:40,392]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 8 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-21 13:53:40,389]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 13:53:40,403]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 13:53:40,403]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:53:40,404]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:42987 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:53:41,731]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:53:41,733]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_34_piece0 in memory on localhost:42987 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:57:10,967]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_33_piece0 on localhost:42987 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 13:57:10,970]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_32_piece0 on localhost:42987 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:06:51,093]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 14:06:51,133]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,146]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,149]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,149]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,150]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,151]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,151]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,151]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,152]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,152]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,153]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,153]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,153]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,155]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,156]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,157]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,160]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,174]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,184]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,184]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,184]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,188]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,188]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,188]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:06:51,240]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:06:51,242]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:06:51,309]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 14:06:51,318]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-475d5841-ca42-44b6-818c-fd1faa577633/blockmgr-8b0efbc2-fc8e-4848-937d-204698d85461, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 14:06:51,318]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:06:51,319]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 14:06:51,323]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:06:51,332]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 14:06:51,332]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 14:06:51,333]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-475d5841-ca42-44b6-818c-fd1faa577633 {org.apache.spark.util.Utils}
[2016-06-21 14:06:51,345]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 14:06:51,357]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:06:51,359]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:06:51,432]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:06:52,234]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:06:56,442]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:08:44,588]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 14:08:44,728]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 14:08:44,784]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 14:08:44,784]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 14:08:44,799]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:08:44,800]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:08:44,800]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 14:08:45,307]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 14:08:45,370]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 14:08:45,663]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:53203] {Remoting}
[2016-06-21 14:08:45,667]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 53203. {org.apache.spark.util.Utils}
[2016-06-21 14:08:45,701]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 14:08:45,726]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 14:08:45,752]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bef93103-9eba-40cb-b826-1a2b8ffe49e3/blockmgr-27d44227-37df-47cb-bbe5-751ad04d24aa {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 14:08:45,757]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:08:45,782]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bef93103-9eba-40cb-b826-1a2b8ffe49e3/httpd-a7ffc8d1-e452-44f2-a224-c781834b8ef3 {org.apache.spark.HttpFileServer}
[2016-06-21 14:08:45,785]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 14:08:45,856]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:08:45,884]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:56932 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:08:45,885]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 56932. {org.apache.spark.util.Utils}
[2016-06-21 14:08:45,901]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 14:08:46,051]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:08:46,080]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:08:46,080]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 14:08:46,083]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:08:46,159]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 14:08:46,678]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35595. {org.apache.spark.util.Utils}
[2016-06-21 14:08:46,678]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 35595 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 14:08:46,680]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:08:46,686]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:35595 with 983.1 MB RAM, BlockManagerId(driver, localhost, 35595) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:08:46,692]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:08:59,226]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:08:59,230]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:08:59,615]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:08:59,616]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:08:59,618]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:08:59,628]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:08:59,835]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 14:08:59,982]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:09:00,008]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,009]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,009]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,042]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,078]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:00,081]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:00,149]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:00,150]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:00,151]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:35595 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:09:00,158]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 14:09:00,175]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,182]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:09:00,245]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:09:00,275]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 14:09:00,351]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466510938901:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 14:09:00,364]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:09:00,364]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:09:00,364]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:09:00,364]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:09:00,364]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:09:00,452]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 14:09:00,504]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 281 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:09:00,506]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:09:00,512]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.311 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:00,526]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.543101 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:09:32,723]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:32,723]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:32,736]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:32,737]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:09:32,738]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:09:32,739]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 14:09:32,915]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:09:32,920]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:09:32,949]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:09:32,949]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:12:28,848]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,848]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,865]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,865]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,866]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:12:28,868]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:12:28,878]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 14:12:28,884]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:12:28,885]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,886]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,887]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,888]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,889]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,889]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,895]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,895]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:12:28,896]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:35595 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:12:28,896]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 14:12:28,897]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,897]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:12:28,899]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:12:28,899]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 14:12:28,902]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466511148845:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 14:12:28,909]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 14:12:28,913]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:12:28,915]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:12:28,915]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:28,916]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.032018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:12:34,039]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:35595 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:12:34,043]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:35595 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:12:34,046]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:35595 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:12:34,047]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:35595 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:13:00,311]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:13:00,311]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:13:00,320]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:13:00,321]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:13:00,322]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:13:00,322]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 14:13:00,381]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:13:00,384]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:13:00,387]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:13:00,388]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:15:52,926]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 154733 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-21 14:15:52,945] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 154733 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:15:52,950]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:15:52,951]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:15:52,953]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 35595) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:15:52,955]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:15:52,964]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:15:52,966]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-21 14:15:52,969]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-21 14:15:52,969]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:15:52,970]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:35595 with 983.1 MB RAM, BlockManagerId(driver, localhost, 35595) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:15:52,972]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:15:52,972]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 4 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-21 14:15:52,980]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:15:52,981]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:35595 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:16:15,779]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 14:16:15,799]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,808]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,809]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,809]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,812]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,812]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,812]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,813]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,813]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,814]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,815]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,818]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,818]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,820]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:16:15,879]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:16:15,884]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:16:15,947]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 14:16:15,973]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bef93103-9eba-40cb-b826-1a2b8ffe49e3/blockmgr-27d44227-37df-47cb-bbe5-751ad04d24aa, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 14:16:15,974]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:16:15,975]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 14:16:15,979]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:16:15,983]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 14:16:15,983]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 14:16:15,986]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 14:16:15,990]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-bef93103-9eba-40cb-b826-1a2b8ffe49e3 {org.apache.spark.util.Utils}
[2016-06-21 14:16:16,004]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:16:16,006]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:16:16,093]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:16:16,925]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:16:21,277]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:21:46,000]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 14:21:46,174]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 14:21:46,290]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 14:21:46,290]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 14:21:46,314]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:21:46,315]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:21:46,315]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 14:21:46,933]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 14:21:46,976]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 14:21:47,189]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:46646] {Remoting}
[2016-06-21 14:21:47,198]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 46646. {org.apache.spark.util.Utils}
[2016-06-21 14:21:47,234]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 14:21:47,251]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 14:21:47,277]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-df82ee0d-df7c-4692-8d87-8b9c4936a33c/blockmgr-30377624-4e1d-4ea9-b91d-51a5d30b378e {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 14:21:47,284]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:21:47,318]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-df82ee0d-df7c-4692-8d87-8b9c4936a33c/httpd-5a72931d-0745-437a-b30c-8c3d4c5cc10f {org.apache.spark.HttpFileServer}
[2016-06-21 14:21:47,322]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 14:21:47,378]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:21:47,437]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:59300 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:21:47,437]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 59300. {org.apache.spark.util.Utils}
[2016-06-21 14:21:47,445]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 14:21:47,530]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:21:47,539]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:21:47,540]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 14:21:47,543]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:21:47,610]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 14:21:47,960]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59513. {org.apache.spark.util.Utils}
[2016-06-21 14:21:47,960]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 59513 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 14:21:47,961]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:21:47,965]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:59513 with 983.1 MB RAM, BlockManagerId(driver, localhost, 59513) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:21:47,967]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:22:29,578]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:29,582]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,056]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,057]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,064]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:59513 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:22:30,068]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:22:30,261]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 14:22:30,353]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:22:30,381]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,382]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,382]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,395]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,401]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,419]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,420]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,448]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,449]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:22:30,450]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:59513 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:22:30,450]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 14:22:30,459]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,460]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:22:30,521]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:22:30,540]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 14:22:30,591]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466511749182:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 14:22:30,600]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:22:30,601]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:22:30,601]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:22:30,601]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:22:30,601]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:22:30,665]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 14:22:30,684]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 200 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:22:30,687]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:22:30,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.214 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:22:30,720]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.366123 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:23:02,777]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:23:02,778]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:23:02,794]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:23:02,794]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:23:02,795]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:59513 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:23:02,796]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 14:23:02,895]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:23:02,899]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:23:02,933]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:23:02,934]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:29:17,542]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:59513 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:29:17,545]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:59513 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:29:56,026]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 14:29:56,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,062]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,063]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,063]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,063]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,064]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,064]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,065]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,065]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,065]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,065]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,066]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 14:29:56,119]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:29:56,123]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:29:56,184]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 14:29:56,192]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-df82ee0d-df7c-4692-8d87-8b9c4936a33c/blockmgr-30377624-4e1d-4ea9-b91d-51a5d30b378e, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 14:29:56,193]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:29:56,194]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 14:29:56,195]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:29:56,198]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 14:29:56,198]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 14:29:56,208]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 14:29:56,210]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-df82ee0d-df7c-4692-8d87-8b9c4936a33c {org.apache.spark.util.Utils}
[2016-06-21 14:29:56,221]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:29:56,228]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 14:29:57,114]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:30:01,638]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 14:30:51,062]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-21 14:30:51,261]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-21 14:30:51,394]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-21 14:30:51,394]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-21 14:30:51,415]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:30:51,416]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-21 14:30:51,417]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-21 14:30:51,961]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-21 14:30:52,043]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-21 14:30:52,304]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:54772] {Remoting}
[2016-06-21 14:30:52,312]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 54772. {org.apache.spark.util.Utils}
[2016-06-21 14:30:52,350]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-21 14:30:52,368]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-21 14:30:52,391]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-102bbaa7-adca-46a6-b398-9da68eec1c34/blockmgr-d7a22a71-b68e-4582-b854-e64062cc78c6 {org.apache.spark.storage.DiskBlockManager}
[2016-06-21 14:30:52,397]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:30:52,432]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-102bbaa7-adca-46a6-b398-9da68eec1c34/httpd-044eb1f9-d357-4e5e-9d55-aa823dff3178 {org.apache.spark.HttpFileServer}
[2016-06-21 14:30:52,436]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-21 14:30:52,491]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:30:52,510]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:57141 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:30:52,511]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 57141. {org.apache.spark.util.Utils}
[2016-06-21 14:30:52,525]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-21 14:30:52,640]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-21 14:30:52,655]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-21 14:30:52,656]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-21 14:30:52,662]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 14:30:52,752]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-21 14:30:53,227]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33985. {org.apache.spark.util.Utils}
[2016-06-21 14:30:53,227]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 33985 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-21 14:30:53,228]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:30:53,243]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:33985 with 983.1 MB RAM, BlockManagerId(driver, localhost, 33985) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:30:53,250]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:31:05,335]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:05,342]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:05,729]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:05,730]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:05,742]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:31:05,760]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:31:06,051]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 14:31:06,167]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:31:06,197]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,199]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,206]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,215]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,225]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:06,225]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:06,249]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:06,249]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:06,251]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:33985 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:31:06,253]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 14:31:06,264]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,267]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:31:06,356]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:31:06,371]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-21 14:31:06,404]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466512264719:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 14:31:06,415]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:31:06,415]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:31:06,415]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:31:06,415]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:31:06,415]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-21 14:31:06,512]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 14:31:06,545]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.241 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,552]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.384631 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:31:06,542]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 205 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:31:06,565]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:31:38,630]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:38,631]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:38,643]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:38,644]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:31:38,645]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:31:38,646]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 14:31:38,819]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:31:38,833]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:31:38,885]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:31:38,886]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:36:42,798]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,799]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,809]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,810]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,811]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:36:42,812]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:36:42,821]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-21 14:36:42,827]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-21 14:36:42,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,829]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,830]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,832]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,832]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,838]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,838]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:36:42,839]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:33985 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:36:42,839]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-21 14:36:42,840]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,840]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:36:42,841]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:36:42,842]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-21 14:36:42,845]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466512602794:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-21 14:36:42,854]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-21 14:36:42,866]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,867]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.040386 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:36:42,867]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 26 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-21 14:36:42,868]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:37:14,759]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137839, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:37:14,760]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:37:14,779]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:37:14,783]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-21 14:37:14,784]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:37:14,785]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-21 14:37:14,864]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:37:14,881]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:37:14,903]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-21 14:37:14,907]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:41:15,316]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 130701 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-21 14:41:33,443] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 130701 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-21 14:41:33,559]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:41:33,580]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:49,328]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 33985) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:49,332]  WARN {org.apache.spark.rpc.akka.AkkaRpcEndpointRef} -  Error sending message [message = UpdateBlockInfo(BlockManagerId(driver, localhost, 33985),broadcast_4_piece0,StorageLevel(false, false, false, false, 1),0,0,0)] in 1 attempts {org.apache.spark.rpc.akka.AkkaRpcEndpointRef}
java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.storage.BlockManagerMaster.updateBlockInfo(BlockManagerMaster.scala:59)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$tryToReportBlockStatus(BlockManager.scala:380)
	at org.apache.spark.storage.BlockManager.reportBlockStatus(BlockManager.scala:356)
	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1117)
	at org.apache.spark.storage.BlockManager$$anonfun$removeBroadcast$2.apply(BlockManager.scala:1093)
	at org.apache.spark.storage.BlockManager$$anonfun$removeBroadcast$2.apply(BlockManager.scala:1093)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:94)
	at org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1093)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply$mcI$sp(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$1.apply(BlockManagerSlaveEndpoint.scala:78)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-21 14:54:49,335] ERROR {org.apache.spark.ContextCleaner} -  Error cleaning broadcast 4 {org.apache.spark.ContextCleaner}
java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:135)
	at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:228)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)
	at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:66)
	at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:214)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:170)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:161)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:161)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1215)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:154)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:67)
[2016-06-21 14:54:49,335]  WARN {org.apache.spark.rpc.akka.AkkaRpcEndpointRef} -  Error sending message [message = RemoveExecutor(driver)] in 1 attempts {org.apache.spark.rpc.akka.AkkaRpcEndpointRef}
java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.storage.BlockManagerMaster.tell(BlockManagerMaster.scala:224)
	at org.apache.spark.storage.BlockManagerMaster.removeExecutor(BlockManagerMaster.scala:40)
	at org.apache.spark.scheduler.DAGScheduler.handleExecutorLost(DAGScheduler.scala:1178)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[2016-06-21 14:54:49,335]  WARN {org.apache.spark.rpc.akka.AkkaRpcEndpointRef} -  Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7acf3eba,BlockManagerId(driver, localhost, 33985))] in 1 attempts {org.apache.spark.rpc.akka.AkkaRpcEndpointRef}
java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:444)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:464)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:464)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-21 14:54:49,334]  WARN {org.apache.spark.rpc.akka.AkkaRpcEndpointRef} -  Error sending message [message = BlockManagerHeartbeat(BlockManagerId(driver, localhost, 33985))] in 1 attempts {org.apache.spark.rpc.akka.AkkaRpcEndpointRef}
java.util.concurrent.TimeoutException: Futures timed out after [600 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.scheduler.DAGScheduler.executorHeartbeatReceived(DAGScheduler.scala:172)
	at org.apache.spark.scheduler.TaskSchedulerImpl.executorHeartbeatReceived(TaskSchedulerImpl.scala:367)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2$$anonfun$run$2.apply$mcV$sp(HeartbeatReceiver.scala:107)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1264)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2.run(HeartbeatReceiver.scala:106)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-21 14:54:49,372]  WARN {org.apache.spark.storage.BlockManagerMaster} -  Failed to remove broadcast 4 with removeFromMaster = true - Ask timed out on [Actor[akka://sparkDriver/user/BlockManagerEndpoint1#-21581358]] after [120000 ms]} {org.apache.spark.storage.BlockManagerMaster}
akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://sparkDriver/user/BlockManagerEndpoint1#-21581358]] after [120000 ms]
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)
	at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-21 14:54:52,355]  INFO {org.apache.spark.storage.BlockManager} -  Got told to re-register updating block broadcast_4_piece0 {org.apache.spark.storage.BlockManager}
[2016-06-21 14:54:52,356]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-21 14:54:52,357]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:54:52,359]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:33985 with 983.1 MB RAM, BlockManagerId(driver, localhost, 33985) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:52,359]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:54:52,360]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 10 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-21 14:54:52,361]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,361]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,362]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,362]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,363]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:33985 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,370]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:52,371]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 33985) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:52,371]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:54:52,372]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-21 14:54:52,372]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-21 14:54:52,372]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:54:52,373]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:33985 with 983.1 MB RAM, BlockManagerId(driver, localhost, 33985) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-21 14:54:52,373]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 14:54:52,373]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 10 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-21 14:54:52,373]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 14:54:52,373]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,374]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,374]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,376]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:33985 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 14:54:52,376]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:33985 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-21 15:48:10,078]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-21 15:48:10,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,094]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,095]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,096]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,096]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,097]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,098]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,098]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,099]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,099]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,099]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,101]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,103]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,105]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,107]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,107]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,109]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,109]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,109]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,109]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,109]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,110]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-21 15:48:10,181]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-21 15:48:10,182]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-21 15:48:10,238]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-21 15:48:10,244]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-102bbaa7-adca-46a6-b398-9da68eec1c34/blockmgr-d7a22a71-b68e-4582-b854-e64062cc78c6, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-21 15:48:10,245]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-21 15:48:10,245]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-21 15:48:10,246]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-21 15:48:10,248]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-21 15:48:10,248]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-21 15:48:10,249]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/WSO2/wso2ml-1.1.0/tmp/spark-102bbaa7-adca-46a6-b398-9da68eec1c34 {org.apache.spark.util.Utils}
[2016-06-21 15:48:10,257]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-21 15:48:10,263]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 15:48:10,267]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 15:48:10,295]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-21 15:48:11,165]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-21 15:48:14,690]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 09:16:53,920]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 09:16:54,050]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 09:16:54,126]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 09:16:54,126]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 09:16:54,137]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 09:16:54,137]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 09:16:54,138]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 09:16:54,519]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 09:16:54,559]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 09:16:54,754]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:44992] {Remoting}
[2016-06-22 09:16:54,758]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 44992. {org.apache.spark.util.Utils}
[2016-06-22 09:16:54,784]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 09:16:54,801]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 09:16:54,820]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-f752b24c-3974-41fb-b956-5482073d3c86/blockmgr-7eb3c187-b26b-43f8-bf85-3f232beda126 {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 09:16:54,825]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:16:54,867]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-f752b24c-3974-41fb-b956-5482073d3c86/httpd-d8558048-8028-4367-93b4-525934229708 {org.apache.spark.HttpFileServer}
[2016-06-22 09:16:54,869]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 09:16:54,916]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 09:16:54,935]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:53370 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 09:16:54,935]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 53370. {org.apache.spark.util.Utils}
[2016-06-22 09:16:54,948]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 09:16:55,098]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 09:16:55,113]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 09:16:55,114]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 09:16:55,115]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 09:16:55,191]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 09:16:55,527]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49679. {org.apache.spark.util.Utils}
[2016-06-22 09:16:55,527]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 49679 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 09:16:55,528]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 09:16:55,533]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:49679 with 983.1 MB RAM, BlockManagerId(driver, localhost, 49679) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 09:16:55,536]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 09:45:08,171]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 09:45:08,190]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,192]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,193]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,193]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,194]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,194]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,195]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,195]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,196]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,196]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,196]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,196]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,197]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,197]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,197]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,197]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,198]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,198]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,198]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,199]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 09:45:08,254]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 09:45:08,260]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:45:08,326]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 09:45:08,336]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-f752b24c-3974-41fb-b956-5482073d3c86/blockmgr-7eb3c187-b26b-43f8-bf85-3f232beda126, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 09:45:08,337]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:45:08,338]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 09:45:08,339]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 09:45:08,342]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 09:45:08,342]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 09:45:08,342]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 09:45:08,343]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-f752b24c-3974-41fb-b956-5482073d3c86 {org.apache.spark.util.Utils}
[2016-06-22 09:45:08,352]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 09:45:08,354]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 09:45:08,383]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 09:45:09,293]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 09:45:13,434]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 09:45:46,889]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 09:45:47,013]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 09:45:47,071]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 09:45:47,071]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 09:45:47,083]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 09:45:47,084]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 09:45:47,084]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 09:45:47,508]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 09:45:47,545]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 09:45:47,660]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:43898] {Remoting}
[2016-06-22 09:45:47,666]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 43898. {org.apache.spark.util.Utils}
[2016-06-22 09:45:47,692]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 09:45:47,705]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 09:45:47,727]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c0f27ac0-882c-40bb-9f83-52758a955e8d/blockmgr-66b67c91-c16b-45d3-b5f6-befa8f1e1aef {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 09:45:47,733]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:45:47,772]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c0f27ac0-882c-40bb-9f83-52758a955e8d/httpd-7db5beba-64a5-47ff-99ad-55fba6502fb0 {org.apache.spark.HttpFileServer}
[2016-06-22 09:45:47,775]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 09:45:47,824]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 09:45:47,840]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:44585 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 09:45:47,840]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 44585. {org.apache.spark.util.Utils}
[2016-06-22 09:45:47,848]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 09:45:47,931]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 09:45:47,941]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 09:45:47,942]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 09:45:47,943]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 09:45:47,996]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 09:45:48,344]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39487. {org.apache.spark.util.Utils}
[2016-06-22 09:45:48,344]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 39487 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 09:45:48,345]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 09:45:48,350]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:39487 with 983.1 MB RAM, BlockManagerId(driver, localhost, 39487) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 09:45:48,352]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 09:46:30,761]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:30,764]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:30,923]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:30,924]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:30,925]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:39487 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 09:46:30,929]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 09:46:31,006]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 09:46:31,063]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 09:46:31,080]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,082]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,082]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,088]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,093]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,097]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:31,098]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:31,119]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:31,120]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 09:46:31,121]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:39487 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 09:46:31,121]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 09:46:31,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,127]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 09:46:31,155]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 09:46:31,168]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 09:46:31,191]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466581590452:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 09:46:31,201]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 09:46:31,201]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 09:46:31,201]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 09:46:31,201]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 09:46:31,201]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 09:46:31,245]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 09:46:31,261]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.125 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:46:31,264]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 113 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 09:46:31,274]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 09:46:31,274]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.210927 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 09:52:03,718]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:39487 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 09:52:03,725]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:39487 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:00:42,522]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 10:00:42,540]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,540]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,540]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,541]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,541]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,541]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,544]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,544]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,545]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,545]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,545]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,545]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:00:42,599]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 10:00:42,603]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:00:42,660]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 10:00:42,665]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c0f27ac0-882c-40bb-9f83-52758a955e8d/blockmgr-66b67c91-c16b-45d3-b5f6-befa8f1e1aef, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 10:00:42,665]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:00:42,671]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 10:00:42,672]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:00:42,674]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 10:00:42,678]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 10:00:42,678]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 10:00:42,679]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c0f27ac0-882c-40bb-9f83-52758a955e8d {org.apache.spark.util.Utils}
[2016-06-22 10:00:42,686]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:00:42,687]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:00:42,707]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:00:43,619]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 10:00:47,578]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 10:03:06,442]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 10:03:06,590]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 10:03:06,655]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 10:03:06,656]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 10:03:06,670]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 10:03:06,670]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 10:03:06,671]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 10:03:07,346]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 10:03:07,385]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 10:03:07,539]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:55444] {Remoting}
[2016-06-22 10:03:07,545]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 55444. {org.apache.spark.util.Utils}
[2016-06-22 10:03:07,567]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 10:03:07,580]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 10:03:07,600]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-36e738b8-31e9-4d4a-8887-525fd3fbcd18/blockmgr-b2241381-9ccb-4055-9998-606394f8ac88 {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 10:03:07,605]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:07,633]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-36e738b8-31e9-4d4a-8887-525fd3fbcd18/httpd-442e443b-1655-4c2b-a788-9c1343cb82fa {org.apache.spark.HttpFileServer}
[2016-06-22 10:03:07,635]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 10:03:07,676]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 10:03:07,689]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:54677 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 10:03:07,690]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 54677. {org.apache.spark.util.Utils}
[2016-06-22 10:03:07,700]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 10:03:07,800]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 10:03:07,814]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 10:03:07,815]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 10:03:07,816]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 10:03:07,912]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 10:03:08,114]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57775. {org.apache.spark.util.Utils}
[2016-06-22 10:03:08,114]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 57775 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 10:03:08,115]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:03:08,120]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:57775 with 983.1 MB RAM, BlockManagerId(driver, localhost, 57775) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 10:03:08,123]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:03:19,022]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,026]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,174]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,174]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,177]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:57775 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:03:19,181]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:03:19,273]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 10:03:19,352]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:03:19,390]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,392]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,393]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,406]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,418]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,427]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,432]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,463]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,468]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:03:19,470]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:57775 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:03:19,475]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 10:03:19,484]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,491]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:03:19,527]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:03:19,536]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 10:03:19,564]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466582598696:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 10:03:19,572]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:03:19,572]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:03:19,572]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:03:19,572]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:03:19,572]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:03:19,630]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 10:03:19,645]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 124 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:03:19,648]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.139 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:03:19,657]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:03:19,662]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.309774 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:06:11,346]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:57775 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:06:11,350]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:57775 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:09:57,155]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 10:09:57,175]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,175]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,176]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,176]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,177]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,177]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,177]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,178]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,178]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,178]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,178]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,178]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,179]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,179]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,179]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,179]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,180]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,180]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,180]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,181]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 10:09:57,233]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 10:09:57,235]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:09:57,292]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 10:09:57,296]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-36e738b8-31e9-4d4a-8887-525fd3fbcd18/blockmgr-b2241381-9ccb-4055-9998-606394f8ac88, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 10:09:57,297]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:09:57,298]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 10:09:57,300]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:09:57,302]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 10:09:57,303]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 10:09:57,303]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 10:09:57,304]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-36e738b8-31e9-4d4a-8887-525fd3fbcd18 {org.apache.spark.util.Utils}
[2016-06-22 10:09:57,312]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:09:57,313]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:09:57,335]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 10:09:58,251]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 10:10:01,841]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 10:10:26,659]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 10:10:26,774]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 10:10:26,835]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 10:10:26,835]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 10:10:26,849]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 10:10:26,850]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 10:10:26,850]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 10:10:27,300]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 10:10:27,361]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 10:10:27,539]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:54630] {Remoting}
[2016-06-22 10:10:27,547]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 54630. {org.apache.spark.util.Utils}
[2016-06-22 10:10:27,574]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 10:10:27,591]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 10:10:27,617]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e7323077-e2df-49b8-af67-a14cbc781561/blockmgr-9d620688-ea98-4d76-a6a1-a49c2aa582ad {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 10:10:27,624]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:27,653]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e7323077-e2df-49b8-af67-a14cbc781561/httpd-4135b7a7-7f7e-4972-b1e4-e21a09c60e8e {org.apache.spark.HttpFileServer}
[2016-06-22 10:10:27,656]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 10:10:27,713]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 10:10:27,732]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:51909 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 10:10:27,732]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 51909. {org.apache.spark.util.Utils}
[2016-06-22 10:10:27,744]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 10:10:27,854]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 10:10:27,871]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 10:10:27,871]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 10:10:27,874]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 10:10:27,934]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 10:10:28,311]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34939. {org.apache.spark.util.Utils}
[2016-06-22 10:10:28,312]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 34939 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 10:10:28,313]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:10:28,317]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:34939 with 983.1 MB RAM, BlockManagerId(driver, localhost, 34939) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 10:10:28,319]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 10:10:49,201]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,203]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,380]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,380]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,382]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:10:49,387]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:10:49,497]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 10:10:49,555]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:10:49,581]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,582]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,582]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,588]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,592]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,597]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,602]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,621]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,622]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:10:49,623]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:34939 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:10:49,632]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 10:10:49,638]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,639]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:10:49,668]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:10:49,674]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 10:10:49,695]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466583048840:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 10:10:49,703]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:10:49,703]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:10:49,703]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:10:49,703]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:10:49,703]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 10:10:49,750]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 10:10:49,768]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.118 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:10:49,767]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 106 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:10:49,775]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:10:49,776]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.220162 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:15:32,283]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:34939 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:15:32,287]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:19:30,825]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,825]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,842]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,842]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,843]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:19:30,844]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:19:30,851]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 10:19:30,858]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 10:19:30,859]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,859]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,859]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,860]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,860]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,862]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,862]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,868]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1878) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,869]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 10:19:30,870]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:34939 (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:19:30,870]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 10:19:30,871]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,871]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:19:30,871]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:19:30,872]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-22 10:19:30,875]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466583570820:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 10:19:30,882]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 10:19:30,888]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.017 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,889]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.030971 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 10:19:30,889]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 10:19:30,889]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 10:21:52,112]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:34939 in memory (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 10:21:52,163]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:03:13,524]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:13,540]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:15,657]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:15,674]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:15,712]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:03:15,872]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:03:16,762]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 11:03:18,833]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:03:18,931]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:18,936]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:18,943]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:19,085]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:19,156]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[5] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:19,519]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:19,603]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:20,586]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1878) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:20,602]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 1878.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:03:20,636]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:34939 (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:03:20,697]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 11:03:20,711]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:20,714]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:03:20,894]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:03:20,911]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-22 11:03:21,195]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466586192431:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 11:03:22,108]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 11:03:22,778]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 1.993 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:03:22,784]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 1972 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:03:22,820]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:03:22,841]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 3.970606 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:30,802]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47671, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:30,817]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:31,692]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86159, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:31,707]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:31,742]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:05:31,900]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:05:32,630]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 11:05:33,285]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:05:33,320]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,322]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,324]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,378]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,426]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[7] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,510]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=90248, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:33,527]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:33,860]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1878) called with curMem=93464, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:33,875]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1878.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:05:33,910]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:34939 (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:05:33,969]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 11:05:33,981]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:33,985]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:05:34,044]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:05:34,080]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-22 11:05:34,532]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466586329564:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 11:05:34,943]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 11:05:35,443]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (first at MLUtils.java:91) finished in 1.420 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:35,464]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 1403 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:05:35,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: first at MLUtils.java:91, took 2.174233 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:05:35,497]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:06:57,588]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=95342, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:06:57,604]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:06:58,207]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=133830, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:06:58,226]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:06:58,262]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:06:58,658]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 11:07:06,642]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 9 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 11:07:06,717]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 9 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:07:07,743]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 11:07:07,793]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:08:50,233]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 18 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:08:50,733]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 18 {org.apache.spark.ContextCleaner}
[2016-06-22 11:08:50,734]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:08:50,738]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-22 11:08:50,739]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 9 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:08:50,739]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 9 {org.apache.spark.ContextCleaner}
[2016-06-22 11:08:50,740]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:08:50,743]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:34939 in memory (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:08:50,745]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:08:50,747]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:34939 in memory (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:08:50,751]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:09:13,300]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:13,315]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:14,996]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:15,057]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:15,192]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:09:15,839]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:09:17,530]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 11:09:17,945]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 11:09:17,980]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:17,984]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 4(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:17,988]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:18,048]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:18,119]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 4 (MapPartitionsRDD[21] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:18,351]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:18,438]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:18,862]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:18,879]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:09:18,917]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:34939 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:09:18,976]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 11:09:18,992]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:18,995]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:09:19,061]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:09:19,082]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-22 11:09:19,238]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466586552296:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 11:09:20,022]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 11:09:20,501]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 4 (first at MLUtils.java:91) finished in 1.463 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:20,519]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 1438 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 11:09:20,544]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: first at MLUtils.java:91, took 2.574338 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 11:09:20,552]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 11:10:29,813]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:10:30,131]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:34939 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:10:40,929]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:10:40,945]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:10:41,851]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:10:41,867]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 11:10:41,904]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:10:42,078]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 11:10:48,855]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 23 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 11:10:49,006]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 23 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:10:49,857]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 11:10:49,905]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:24:31,805]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 11:24:32,011]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 32 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:24:32,061]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 32 {org.apache.spark.ContextCleaner}
[2016-06-22 11:24:32,208]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:24:32,262]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 31 {org.apache.spark.ContextCleaner}
[2016-06-22 11:24:32,414]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 23 {org.apache.spark.storage.BlockManager}
[2016-06-22 11:24:32,464]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 23 {org.apache.spark.ContextCleaner}
[2016-06-22 12:46:44,906]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:44,956]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:45,848]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:45,864]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:45,900]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:46:46,070]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 12:46:47,116]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 12:46:49,063]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 12:46:49,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:49,331]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:49,337]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:49,478]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:49,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[35] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:49,671]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:49,687]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:50,042]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1879) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:50,058]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 1879.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:46:50,095]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:34939 (size: 1879.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:46:50,160]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 12:46:50,173]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[35] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:50,176]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 12:46:50,368]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 12:46:50,387]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-22 12:46:50,603]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466592403743:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 12:46:51,724]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 12:46:52,637]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (first at MLUtils.java:91) finished in 2.403 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:46:52,645]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 2380 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 12:46:52,686]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 12:46:52,702]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: first at MLUtils.java:91, took 3.602140 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:48:12,106]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47672, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:48:12,121]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:48:13,016]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86160, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:48:13,036]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:48:13,070]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:34939 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:48:13,230]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 12:48:22,884]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 37 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 12:48:22,950]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 37 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:48:23,841]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 45 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 12:48:23,954]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 45 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:50:53,236]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:34939 in memory (size: 1879.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:50:53,238]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:34939 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:51:42,795]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 12:51:42,816]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,820]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,822]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,823]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,823]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,825]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,825]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 12:51:42,881]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 12:51:42,883]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:51:42,942]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 12:51:42,950]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e7323077-e2df-49b8-af67-a14cbc781561/blockmgr-9d620688-ea98-4d76-a6a1-a49c2aa582ad, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 12:51:42,951]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:51:42,952]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 12:51:42,953]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 12:51:42,957]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 12:51:42,958]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 12:51:42,958]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 12:51:42,959]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e7323077-e2df-49b8-af67-a14cbc781561 {org.apache.spark.util.Utils}
[2016-06-22 12:51:42,967]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 12:51:42,970]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 12:51:43,001]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 12:51:43,850]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 12:51:47,713]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 12:52:43,827]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 12:52:43,933]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 12:52:43,985]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 12:52:43,985]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 12:52:43,998]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 12:52:43,999]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 12:52:43,999]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 12:52:44,416]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 12:52:44,465]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 12:52:44,611]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:44304] {Remoting}
[2016-06-22 12:52:44,616]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 44304. {org.apache.spark.util.Utils}
[2016-06-22 12:52:44,636]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 12:52:44,650]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 12:52:44,666]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-da62e542-eb3b-4f0a-83a9-f125da472e49/blockmgr-305bf27d-ffe4-4546-ba96-89f73d2efd11 {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 12:52:44,670]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:44,694]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-da62e542-eb3b-4f0a-83a9-f125da472e49/httpd-8afae4f5-63f5-45c3-b715-6c287c84541d {org.apache.spark.HttpFileServer}
[2016-06-22 12:52:44,697]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 12:52:44,773]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 12:52:44,785]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:33705 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 12:52:44,785]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 33705. {org.apache.spark.util.Utils}
[2016-06-22 12:52:44,794]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 12:52:44,884]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 12:52:44,896]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 12:52:44,896]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 12:52:44,898]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 12:52:44,968]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 12:52:45,284]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58816. {org.apache.spark.util.Utils}
[2016-06-22 12:52:45,284]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 58816 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 12:52:45,285]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 12:52:45,290]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:58816 with 983.1 MB RAM, BlockManagerId(driver, localhost, 58816) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 12:52:45,293]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 12:52:54,960]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:54,963]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,214]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,215]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,217]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:58816 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:52:55,221]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 12:52:55,338]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 12:52:55,403]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 12:52:55,422]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,423]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,423]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,429]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,445]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,449]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,459]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,477]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,479]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:52:55,481]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:58816 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:52:55,482]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 12:52:55,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,493]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 12:52:55,532]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 12:52:55,542]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 12:52:55,577]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466592774655:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 12:52:55,589]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 12:52:55,589]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 12:52:55,589]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 12:52:55,589]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 12:52:55,589]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 12:52:55,647]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 12:52:55,672]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.164 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,684]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.280412 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 12:52:55,684]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 146 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 12:52:55,690]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 12:53:27,556]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:53:27,556]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:53:27,569]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:53:27,570]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 12:53:27,571]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:58816 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:53:27,572]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 12:53:27,694]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 12:53:27,703]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:53:27,724]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 12:53:27,724]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:58:46,618]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:58:46,620]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-22 12:58:46,620]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:58:46,621]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-22 12:58:46,621]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 12:58:46,622]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-22 12:58:46,627]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:58816 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:58:46,630]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:58816 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 12:58:46,631]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:58816 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:12:48,859]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 13:12:48,875]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,876]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,876]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,877]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,890]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,891]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,892]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,892]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,895]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,904]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,907]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,908]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,908]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,909]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,909]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,910]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,910]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,911]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,911]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,915]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:12:48,969]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 13:12:48,974]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:12:49,053]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 13:12:49,079]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-da62e542-eb3b-4f0a-83a9-f125da472e49/blockmgr-305bf27d-ffe4-4546-ba96-89f73d2efd11, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 13:12:49,080]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:12:49,081]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 13:12:49,084]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:12:49,096]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 13:12:49,096]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 13:12:49,098]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 13:12:49,099]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-da62e542-eb3b-4f0a-83a9-f125da472e49 {org.apache.spark.util.Utils}
[2016-06-22 13:12:49,131]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:12:49,139]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:12:49,211]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:12:49,995]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 13:12:55,488]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 13:18:18,678]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 13:18:18,803]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 13:18:18,863]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 13:18:18,864]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 13:18:18,878]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 13:18:18,879]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 13:18:18,880]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 13:18:19,336]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 13:18:19,416]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 13:18:19,541]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:40499] {Remoting}
[2016-06-22 13:18:19,546]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 40499. {org.apache.spark.util.Utils}
[2016-06-22 13:18:19,565]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 13:18:19,576]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 13:18:19,592]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-b0a3d31a-bbd6-4bbc-93de-c5e7082a0d84/blockmgr-d38d6638-3229-47a9-a57a-c50283d3c03a {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 13:18:19,596]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:19,618]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-b0a3d31a-bbd6-4bbc-93de-c5e7082a0d84/httpd-638129ae-751e-4314-aeb6-1630ca78fddd {org.apache.spark.HttpFileServer}
[2016-06-22 13:18:19,621]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 13:18:19,666]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 13:18:19,679]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:53287 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 13:18:19,680]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 53287. {org.apache.spark.util.Utils}
[2016-06-22 13:18:19,688]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 13:18:19,772]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 13:18:19,788]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 13:18:19,788]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 13:18:19,790]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 13:18:19,849]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 13:18:20,237]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59260. {org.apache.spark.util.Utils}
[2016-06-22 13:18:20,238]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 59260 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 13:18:20,238]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:18:20,245]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:59260 with 983.1 MB RAM, BlockManagerId(driver, localhost, 59260) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 13:18:20,248]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:18:35,059]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,062]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,211]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,212]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,214]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:59260 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:18:35,218]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:18:35,322]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 13:18:35,374]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:18:35,395]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,396]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,397]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,402]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,421]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,425]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,435]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,446]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,446]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:18:35,447]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:59260 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:18:35,448]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 13:18:35,456]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,458]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:18:35,491]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:18:35,504]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 13:18:35,528]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466594314771:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 13:18:35,537]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:18:35,537]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:18:35,537]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:18:35,537]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:18:35,537]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:18:35,583]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 13:18:35,602]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.135 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,606]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.231569 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:18:35,607]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 120 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:18:35,612]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:19:07,522]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:19:07,522]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:19:07,541]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:19:07,542]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:19:07,543]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:59260 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:19:07,544]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 13:19:07,679]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:19:07,685]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:19:07,705]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:19:07,706]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:20:21,777]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,777]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,789]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,789]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,790]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:59260 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:20:21,791]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:20:21,799]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 13:20:21,808]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:20:21,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,811]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,811]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,812]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,813]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,817]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,817]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:21,818]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:59260 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:20:21,818]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 13:20:21,819]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,819]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:20:21,820]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:20:21,820]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-22 13:20:21,823]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466594421775:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 13:20:21,828]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 13:20:21,832]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 13 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:20:21,832]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:20:21,832]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.013 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:21,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.024099 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:20:53,287]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137839, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:53,289]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:53,304]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:53,304]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:20:53,305]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:59260 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:20:53,306]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 13:20:53,327]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:20:53,330]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:20:53,363]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:20:53,364]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,702]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,703]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,705]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,705]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,706]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,710]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 17 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,716]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:59260 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:21:20,719]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:59260 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:21:20,720]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:59260 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:21:20,721]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,722]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,722]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,722]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,723]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:21:20,723]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-22 13:21:20,724]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:59260 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:21:20,726]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:59260 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:21:20,727]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:59260 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:22:55,400]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 13:22:55,421]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,425]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,425]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,426]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,426]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,426]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,427]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,427]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,427]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,427]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,428]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,428]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,428]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,429]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,429]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,429]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,430]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,430]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,430]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,431]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,431]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,431]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,432]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,432]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,432]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 13:22:55,488]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 13:22:55,491]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:22:55,581]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 13:22:55,586]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-b0a3d31a-bbd6-4bbc-93de-c5e7082a0d84/blockmgr-d38d6638-3229-47a9-a57a-c50283d3c03a, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 13:22:55,587]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:22:55,588]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 13:22:55,589]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:22:55,592]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 13:22:55,593]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 13:22:55,593]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-b0a3d31a-bbd6-4bbc-93de-c5e7082a0d84 {org.apache.spark.util.Utils}
[2016-06-22 13:22:55,597]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 13:22:55,606]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:22:55,608]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:22:55,634]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 13:22:56,513]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 13:23:01,919]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 13:32:58,677]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-22 13:32:58,787]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-22 13:32:58,859]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-22 13:32:58,859]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-22 13:32:58,881]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 13:32:58,882]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-22 13:32:58,883]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-22 13:32:59,282]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-22 13:32:59,325]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-22 13:32:59,505]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:40846] {Remoting}
[2016-06-22 13:32:59,514]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 40846. {org.apache.spark.util.Utils}
[2016-06-22 13:32:59,548]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-22 13:32:59,564]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-22 13:32:59,586]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-5bafe678-1c82-4e51-9d82-f0e043378f97/blockmgr-5d18af35-79bc-4e0f-a671-542fb64c02ed {org.apache.spark.storage.DiskBlockManager}
[2016-06-22 13:32:59,592]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:32:59,625]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-5bafe678-1c82-4e51-9d82-f0e043378f97/httpd-1ff78ec8-5394-4dec-96d7-8b5008b492a7 {org.apache.spark.HttpFileServer}
[2016-06-22 13:32:59,628]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-22 13:32:59,679]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 13:32:59,694]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:33105 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 13:32:59,695]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 33105. {org.apache.spark.util.Utils}
[2016-06-22 13:32:59,704]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-22 13:32:59,807]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-22 13:32:59,826]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-22 13:32:59,826]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-22 13:32:59,828]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 13:32:59,899]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-22 13:33:00,211]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50263. {org.apache.spark.util.Utils}
[2016-06-22 13:33:00,211]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 50263 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-22 13:33:00,212]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:33:00,218]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:50263 with 983.1 MB RAM, BlockManagerId(driver, localhost, 50263) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-22 13:33:00,220]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 13:33:32,015]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,018]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,185]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,185]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,187]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:50263 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:33:32,190]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:33:32,266]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-22 13:33:32,313]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-22 13:33:32,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,327]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,328]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,331]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,335]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,339]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,340]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,355]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,355]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:33:32,356]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:50263 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:33:32,356]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-22 13:33:32,362]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,362]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:33:32,383]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:33:32,388]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-22 13:33:32,404]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466595211723:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-22 13:33:32,410]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:33:32,411]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:33:32,411]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:33:32,411]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:33:32,411]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-22 13:33:32,445]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-22 13:33:32,457]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 80 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-22 13:33:32,458]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-22 13:33:32,460]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.090 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:33:32,466]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.152356 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 13:34:04,346]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:34:04,346]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:34:04,359]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:34:04,360]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-22 13:34:04,361]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:50263 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:34:04,362]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-22 13:34:04,482]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:34:04,496]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:34:04,516]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-22 13:34:04,518]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:39:20,822]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:39:20,883]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-22 13:39:20,906]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:39:20,924]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-22 13:39:20,925]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-22 13:39:20,928]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-22 13:39:20,935]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:50263 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:39:20,940]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:50263 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 13:39:20,956]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:50263 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-22 14:27:47,584]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-22 14:27:47,602]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,603]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,603]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,604]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,604]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,605]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,605]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,605]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,606]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,606]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,606]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,606]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,607]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,607]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,607]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,608]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,610]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,610]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,610]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,611]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,611]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,611]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,611]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,611]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,612]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-22 14:27:47,664]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-22 14:27:47,666]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-22 14:27:47,723]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-22 14:27:47,727]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-5bafe678-1c82-4e51-9d82-f0e043378f97/blockmgr-5d18af35-79bc-4e0f-a671-542fb64c02ed, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-22 14:27:47,727]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-22 14:27:47,728]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-22 14:27:47,729]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-22 14:27:47,731]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-22 14:27:47,731]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-22 14:27:47,731]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-22 14:27:47,735]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-5bafe678-1c82-4e51-9d82-f0e043378f97 {org.apache.spark.util.Utils}
[2016-06-22 14:27:47,739]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 14:27:47,741]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 14:27:47,766]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-22 14:27:48,636]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-22 14:27:52,480]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-23 15:54:47,601]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-23 15:54:47,712]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-23 15:54:47,774]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-23 15:54:47,774]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-23 15:54:47,787]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 15:54:47,788]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 15:54:47,788]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-23 15:54:48,188]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-23 15:54:48,232]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-23 15:54:48,375]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:49117] {Remoting}
[2016-06-23 15:54:48,390]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 49117. {org.apache.spark.util.Utils}
[2016-06-23 15:54:48,410]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-23 15:54:48,422]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-23 15:54:48,442]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c7a0df50-5f43-4ebb-9c59-51490e998639/blockmgr-34527f91-b655-405a-8e3d-ac9e2e0159fc {org.apache.spark.storage.DiskBlockManager}
[2016-06-23 15:54:48,447]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:54:48,468]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c7a0df50-5f43-4ebb-9c59-51490e998639/httpd-110a0799-8f96-4529-9612-5137dd28a08b {org.apache.spark.HttpFileServer}
[2016-06-23 15:54:48,470]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-23 15:54:48,512]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 15:54:48,534]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:59240 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 15:54:48,535]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 59240. {org.apache.spark.util.Utils}
[2016-06-23 15:54:48,546]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-23 15:54:48,645]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 15:54:48,654]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 15:54:48,655]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-23 15:54:48,656]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-23 15:54:48,716]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-23 15:54:49,043]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44046. {org.apache.spark.util.Utils}
[2016-06-23 15:54:49,044]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 44046 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-23 15:54:49,045]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 15:54:49,051]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:44046 with 983.1 MB RAM, BlockManagerId(driver, localhost, 44046) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-23 15:54:49,054]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 15:56:03,903]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:03,906]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,056]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,056]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,058]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:44046 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 15:56:04,062]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 15:56:04,155]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 15:56:04,215]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 15:56:04,235]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,236]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,236]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,245]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,250]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,251]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,271]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,272]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 15:56:04,273]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:44046 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 15:56:04,274]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 15:56:04,281]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,282]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 15:56:04,309]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 15:56:04,319]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-23 15:56:04,341]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466690163644:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 15:56:04,350]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 15:56:04,351]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 15:56:04,351]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 15:56:04,351]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 15:56:04,351]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 15:56:04,401]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 15:56:04,418]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 115 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 15:56:04,422]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.130 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 15:56:04,423]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 15:56:04,441]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.226138 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,222]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,222]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,235]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,235]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,236]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:44046 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:00:02,237]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:00:02,245]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:00:02,251]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:00:02,252]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,252]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,252]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,254]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,254]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,256]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3232) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,256]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,262]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1897) called with curMem=93397, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,263]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 1897.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:00:02,263]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:44046 (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:00:02,264]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:00:02,264]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,264]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:00:02,265]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:00:02,266]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-23 16:00:02,270]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466690402216:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:00:02,278]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:00:02,282]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:00:02,283]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:00:02,282]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.018 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:00:02,287]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.035800 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:43,933]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=95294, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:43,934]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:43,987]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=133782, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:43,987]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:43,988]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:44046 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:43,995]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,028]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:44046 in memory (size: 1897.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,040]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:44046 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,050]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:44046 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,052]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:44046 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,106]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 5 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:44,109]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 5 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:44,124]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:44,125]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:44,143]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:03:44,160]  INFO {org.apache.spark.SparkContext} -  Starting job: take at DecisionTreeMetadata.scala:110 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(take at DecisionTreeMetadata.scala:110) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,162]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,172]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,173]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[16] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,175]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7608) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,176]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 7.4 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,182]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3675) called with curMem=50185, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,183]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,184]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:44046 (size: 3.6 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,184]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,185]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,185]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,186]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,187]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,201]  INFO {org.apache.spark.CacheManager} -  Partition rdd_14_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-23 16:03:44,202]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:03:44,332]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(56368) called with curMem=53860, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,332]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_14_0 stored as values in memory (estimated size 55.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,333]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_14_0 in memory on localhost:44046 (size: 55.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,347]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 2564 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,368]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (take at DecisionTreeMetadata.scala:110) finished in 0.182 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,368]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 181 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,369]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,369]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: take at DecisionTreeMetadata.scala:110, took 0.209089 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,373]  INFO {org.apache.spark.SparkContext} -  Starting job: count at DecisionTreeMetadata.scala:111 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (count at DecisionTreeMetadata.scala:111) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(count at DecisionTreeMetadata.scala:111) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,376]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,376]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[16] at retag at RandomForest.scala:137), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,379]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7440) called with curMem=110228, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,379]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 7.3 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,385]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3585) called with curMem=117668, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,386]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,386]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:44046 (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,387]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,387]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at retag at RandomForest.scala:137) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,387]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,390]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,391]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,394]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_14_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:44,399]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,403]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,403]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (count at DecisionTreeMetadata.scala:111) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,404]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,404]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: count at DecisionTreeMetadata.scala:111, took 0.030617 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,429]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at DecisionTree.scala:977 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,430]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (collect at DecisionTree.scala:977) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,431]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 4(collect at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,431]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,432]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,433]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 4 (PartitionwiseSampledRDD[17] at sample at DecisionTree.scala:977), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,435]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8168) called with curMem=121253, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,435]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 8.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,441]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3911) called with curMem=129421, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,441]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.8 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,443]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:44046 (size: 3.8 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,443]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,444]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 4 (PartitionwiseSampledRDD[17] at sample at DecisionTree.scala:977) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,444]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,446]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1605 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,446]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,452]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_14_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:44,474]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 48869 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,502]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 4 (collect at DecisionTree.scala:977) finished in 0.058 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,503]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 58 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,503]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,503]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: collect at DecisionTree.scala:977, took 0.073332 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,558]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(624) called with curMem=133332, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,558]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 624.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,571]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(71) called with curMem=133956, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,572]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 71.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,575]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:44046 (size: 71.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,575]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,657]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,661]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 20 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,662]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,662]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 6(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,662]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 5) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,664]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 5) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,675]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 5 (MapPartitionsRDD[20] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,688]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(18848) called with curMem=134027, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,688]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 18.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,713]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8081) called with curMem=152875, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,713]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,723]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:44046 (size: 7.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:44,724]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:44,726]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[20] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:44,727]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:44,730]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:44,730]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:44,780]  INFO {org.apache.spark.CacheManager} -  Partition rdd_19_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-23 16:03:44,780]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_14_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:44,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(65968) called with curMem=160956, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,916]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_19_0 stored as values in memory (estimated size 64.4 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:44,917]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_19_0 in memory on localhost:44046 (size: 64.4 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,456]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:44046 in memory (size: 3.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,461]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:44046 in memory (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,464]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_5_piece0 on localhost:44046 in memory (size: 3.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,616]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 2510 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:45,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 5 (mapPartitions at DecisionTree.scala:613) finished in 0.907 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,635]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,636]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 6) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,636]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,640]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 6: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,643]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 6 (MapPartitionsRDD[22] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,646]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8048) called with curMem=192537, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,647]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 905 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:45,647]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:45,655]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 7.9 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,660]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3569) called with curMem=200585, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,661]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,669]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:44046 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,670]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:45,671]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,671]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:45,680]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:45,680]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:45,713]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:45,714]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 9 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:45,833]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 1839 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:45,843]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 165 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:45,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 6 (collectAsMap at DecisionTree.scala:642) finished in 0.163 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,843]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:45,845]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: collectAsMap at DecisionTree.scala:642, took 1.187092 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,851]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1160) called with curMem=204154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,851]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 1160.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,863]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(113) called with curMem=205314, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,864]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 113.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,866]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:44046 (size: 113.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,867]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-23 16:03:45,899]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-23 16:03:45,900]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 23 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,901]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,901]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 8(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,901]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,902]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 7) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,905]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,908]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(20552) called with curMem=205427, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,908]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 20.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,914]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8912) called with curMem=225979, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,920]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:45,921]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:44046 (size: 8.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:45,927]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:45,927]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,927]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 7.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:45,929]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 7.0 (TID 7, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:45,931]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 7.0 (TID 7) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:45,949]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_19_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:45,990]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 7.0 (TID 7). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:45,997]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 7 (mapPartitions at DecisionTree.scala:613) finished in 0.067 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,997]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,997]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,997]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 8) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,997]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,998]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 8: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,999]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 8 (MapPartitionsRDD[25] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:45,999]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8352) called with curMem=234891, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,000]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 8.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,004]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 7.0 (TID 7) in 67 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,005]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 7.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,007]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3838) called with curMem=243243, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,007]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.7 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,010]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:44046 (size: 3.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,011]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,012]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,012]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 8.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,013]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 8.0 (TID 8, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,013]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 8.0 (TID 8) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,022]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,022]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 4 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,086]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 8.0 (TID 8). 2504 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,110]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 8 (collectAsMap at DecisionTree.scala:642) finished in 0.093 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,111]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: collectAsMap at DecisionTree.scala:642, took 0.211559 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,112]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 8.0 (TID 8) in 98 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,112]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 8.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,115]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=247081, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,116]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,123]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(209) called with curMem=249321, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,125]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 209.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,126]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:44046 (size: 209.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,127]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,149]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,150]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 26 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,150]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 7 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,150]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 10(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,150]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,154]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 9) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,156]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 9 (MapPartitionsRDD[26] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,159]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(23584) called with curMem=249530, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,159]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 23.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,165]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10108) called with curMem=273114, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,165]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 9.9 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,166]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:44046 (size: 9.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,166]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,167]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[26] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,167]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 9.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,168]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 9.0 (TID 9, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,168]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 9.0 (TID 9) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,173]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_19_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:46,209]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 9.0 (TID 9). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,217]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 9 (mapPartitions at DecisionTree.scala:613) finished in 0.046 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,217]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,217]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,217]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 10) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,218]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,220]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 10: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,220]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 10 (MapPartitionsRDD[28] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,221]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=283222, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,221]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 8.8 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,231]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 9.0 (TID 9) in 49 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,231]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 9.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,234]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4053) called with curMem=292182, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,235]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,239]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:44046 (size: 4.0 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,239]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,240]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[28] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,240]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 10.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,241]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 10.0 (TID 10, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,241]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 10.0 (TID 10) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,247]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,247]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,314]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 10.0 (TID 10). 3959 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 10 (collectAsMap at DecisionTree.scala:642) finished in 0.081 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,325]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 7 finished: collectAsMap at DecisionTree.scala:642, took 0.176106 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,325]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 10.0 (TID 10) in 84 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,325]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 10.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,329]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2240) called with curMem=296235, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,329]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 2.2 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,335]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(217) called with curMem=298475, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,335]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 217.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,336]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:44046 (size: 217.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,339]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,369]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 29 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 8 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 12(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,371]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,372]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 11) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,374]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 11 (MapPartitionsRDD[29] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,376]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28952) called with curMem=298692, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,376]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 28.3 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,390]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(11801) called with curMem=327644, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,392]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,392]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:44046 (size: 11.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,414]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[29] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,415]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 11.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,416]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 11.0 (TID 11, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,416]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 11.0 (TID 11) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,421]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_19_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:46,448]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 11.0 (TID 11). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,452]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 11 (mapPartitions at DecisionTree.scala:613) finished in 0.036 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,452]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,452]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,453]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 12) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,453]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,453]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 11.0 (TID 11) in 37 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,453]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 11.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,463]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 12: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,463]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 12 (MapPartitionsRDD[31] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,464]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8960) called with curMem=339445, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,465]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 8.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,469]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4006) called with curMem=348405, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,469]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,473]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:44046 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,474]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,476]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[31] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,476]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 12.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,477]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 12.0 (TID 12, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,479]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 12.0 (TID 12) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,484]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,485]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 4 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,528]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 12.0 (TID 12). 3919 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 12 (collectAsMap at DecisionTree.scala:642) finished in 0.052 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,536]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 8 finished: collectAsMap at DecisionTree.scala:642, took 0.167103 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,539]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1808) called with curMem=352411, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,540]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 1808.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,544]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(173) called with curMem=354219, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,545]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 12.0 (TID 12) in 58 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,545]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 12.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,545]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 173.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,546]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:44046 (size: 173.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,548]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from broadcast at DecisionTree.scala:592 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,573]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at DecisionTree.scala:642 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 32 (mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 9 (collectAsMap at DecisionTree.scala:642) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 14(collectAsMap at DecisionTree.scala:642) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,574]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,577]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 13) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,581]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 13 (MapPartitionsRDD[32] at mapPartitions at DecisionTree.scala:613), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,583]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33904) called with curMem=354392, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,584]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21 stored as values in memory (estimated size 33.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,592]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(13192) called with curMem=388296, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,592]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,593]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_21_piece0 in memory on localhost:44046 (size: 12.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,593]  INFO {org.apache.spark.SparkContext} -  Created broadcast 21 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,603]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[32] at mapPartitions at DecisionTree.scala:613) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,603]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 13.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,604]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 13.0 (TID 13, localhost, PROCESS_LOCAL, 1485 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,604]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 13.0 (TID 13) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,609]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_19_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:46,668]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 13.0 (TID 13). 1930 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 13.0 (TID 13) in 70 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 13.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 13 (mapPartitions at DecisionTree.scala:613) finished in 0.061 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 14) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,674]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,677]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 14: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,678]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 14 (MapPartitionsRDD[34] at map at DecisionTree.scala:633), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,679]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(8720) called with curMem=401488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,679]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22 stored as values in memory (estimated size 8.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,684]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4013) called with curMem=410208, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,685]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.9 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,685]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_22_piece0 in memory on localhost:44046 (size: 3.9 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,686]  INFO {org.apache.spark.SparkContext} -  Created broadcast 22 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,687]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at map at DecisionTree.scala:633) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,687]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 14.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,687]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 14.0 (TID 14, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,689]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 14.0 (TID 14) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,693]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,694]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 3 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:46,749]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 14.0 (TID 14). 3281 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 14 (collectAsMap at DecisionTree.scala:642) finished in 0.073 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,762]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 9 finished: collectAsMap at DecisionTree.scala:642, took 0.188495 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,762]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 14.0 (TID 14) in 73 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,762]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 14.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,763]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 19 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:46,765]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 19 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:46,771]  INFO {org.apache.spark.mllib.tree.RandomForest} -  Internal timing for DecisionTree: {org.apache.spark.mllib.tree.RandomForest}
[2016-06-23 16:03:46,772]  INFO {org.apache.spark.mllib.tree.RandomForest} -    init: 0.400058893
  total: 2.632612281
  findSplitsBins: 0.109796116
  findBestSplits: 2.210203841
  chooseSplits: 2.198636477 {org.apache.spark.mllib.tree.RandomForest}
[2016-06-23 16:03:46,777]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 14 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:46,780]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 14 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:46,821]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:215 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,822]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 10 (take at SparkModelUtils.java:215) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,823]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 15(take at SparkModelUtils.java:215) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,823]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,828]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 15 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,832]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28656) called with curMem=291885, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,832]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23 stored as values in memory (estimated size 28.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,838]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10841) called with curMem=320541, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,838]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,842]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_23_piece0 in memory on localhost:44046 (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,843]  INFO {org.apache.spark.SparkContext} -  Created broadcast 23 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:46,845]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,845]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 15.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,847]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 15.0 (TID 15, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,849]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 15.0 (TID 15) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,875]  INFO {org.apache.spark.CacheManager} -  Partition rdd_35_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-23 16:03:46,879]  INFO {org.apache.spark.CacheManager} -  Partition rdd_15_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-23 16:03:46,879]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/WSO2/wso2ml-1.1.0/datasets/breastCancerWisconsin-random-forest-classification-dataset.-1234.1466423570935:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:03:46,935]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(29696) called with curMem=331382, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,935]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_15_0 stored as values in memory (estimated size 29.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,936]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_15_0 in memory on localhost:44046 (size: 29.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,972]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(16128) called with curMem=361078, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,972]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_35_0 stored as values in memory (estimated size 15.8 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:46,973]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_35_0 in memory on localhost:44046 (size: 15.8 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:46,982]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 15.0 (TID 15). 6663 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:46,990]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 15.0 (TID 15) in 143 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:46,990]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 15.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:46,990]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 15 (take at SparkModelUtils.java:215) finished in 0.142 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:46,991]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 10 finished: take at SparkModelUtils.java:215, took 0.169463 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,001]  INFO {org.apache.spark.SparkContext} -  Starting job: take at SparkModelUtils.java:223 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 11 (take at SparkModelUtils.java:223) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 16(take at SparkModelUtils.java:223) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,002]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,004]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,004]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 16 (MapPartitionsRDD[15] at randomSplit at SupervisedSparkModelBuilder.java:136), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,005]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(7272) called with curMem=377206, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,006]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24 stored as values in memory (estimated size 7.1 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,009]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3553) called with curMem=384478, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,012]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.5 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,012]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_24_piece0 in memory on localhost:44046 (size: 3.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,013]  INFO {org.apache.spark.SparkContext} -  Created broadcast 24 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,013]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[15] at randomSplit at SupervisedSparkModelBuilder.java:136) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,014]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 16.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,014]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 16.0 (TID 16, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,015]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 16.0 (TID 16) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,017]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_15_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,027]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 16.0 (TID 16). 22309 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,035]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 16 (take at SparkModelUtils.java:223) finished in 0.019 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,035]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 16.0 (TID 16) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,035]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 16.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,039]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 11 finished: take at SparkModelUtils.java:223, took 0.037440 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,054]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:241 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,059]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 12 (count at SparkModelUtils.java:241) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,059]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 17(count at SparkModelUtils.java:241) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,059]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,060]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,060]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 17 (ParallelCollectionRDD[36] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,064]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1320) called with curMem=388031, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,065]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25 stored as values in memory (estimated size 1320.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,069]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(877) called with curMem=389351, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,069]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_25_piece0 stored as bytes in memory (estimated size 877.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,071]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_25_piece0 in memory on localhost:44046 (size: 877.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,075]  INFO {org.apache.spark.SparkContext} -  Created broadcast 25 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,076]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 17 (ParallelCollectionRDD[36] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,076]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 17.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,086]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 17.0 (TID 17, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,091]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 17.0 (TID 17) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,119]  INFO {org.apache.spark.CacheManager} -  Partition rdd_36_0 not found, computing it {org.apache.spark.CacheManager}
[2016-06-23 16:03:47,123]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(33088) called with curMem=390228, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,124]  INFO {org.apache.spark.storage.MemoryStore} -  Block rdd_36_0 stored as values in memory (estimated size 32.3 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,124]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added rdd_36_0 in memory on localhost:44046 (size: 32.3 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,130]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 17.0 (TID 17). 1209 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,134]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 17.0 (TID 17) in 57 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,134]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 17.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,139]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 17 (count at SparkModelUtils.java:241) finished in 0.062 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,140]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 12 finished: count at SparkModelUtils.java:241, took 0.085566 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,145]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SparkModelUtils.java:245 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,145]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 13 (collect at SparkModelUtils.java:245) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,145]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 18(collect at SparkModelUtils.java:245) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,145]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,146]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,146]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 18 (ParallelCollectionRDD[36] at parallelize at SparkModelUtils.java:238), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,147]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1496) called with curMem=423316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,147]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26 stored as values in memory (estimated size 1496.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,153]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(930) called with curMem=424812, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,153]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_26_piece0 stored as bytes in memory (estimated size 930.0 B, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,154]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_26_piece0 in memory on localhost:44046 (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,155]  INFO {org.apache.spark.SparkContext} -  Created broadcast 26 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,155]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 18 (ParallelCollectionRDD[36] at parallelize at SparkModelUtils.java:238) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,155]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 18.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,160]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 18.0 (TID 18, localhost, PROCESS_LOCAL, 23573 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,162]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 18.0 (TID 18) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,171]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_36_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,179]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 18.0 (TID 18). 24040 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,184]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 18 (collect at SparkModelUtils.java:245) finished in 0.027 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,184]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 18.0 (TID 18) in 27 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,184]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 18.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,185]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 13 finished: collect at SparkModelUtils.java:245, took 0.040306 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,186]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 36 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-23 16:03:47,186]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,197]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 14 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 19(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,198]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,200]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,200]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 19 (MapPartitionsRDD[37] at filter at SparkModelUtils.java:252), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,202]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28712) called with curMem=392654, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,203]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27 stored as values in memory (estimated size 28.0 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,208]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10913) called with curMem=421366, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,208]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.7 KB, free 982.7 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,210]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_27_piece0 in memory on localhost:44046 (size: 10.7 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,211]  INFO {org.apache.spark.SparkContext} -  Created broadcast 27 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,211]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[37] at filter at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,211]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 19.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,216]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 19.0 (TID 19, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,218]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 19.0 (TID 19) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,221]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_35_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,229]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 19.0 (TID 19). 1750 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 19 (count at SparkModelUtils.java:252) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,233]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 19.0 (TID 19) in 18 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,233]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 19.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,234]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 14 finished: count at SparkModelUtils.java:252, took 0.036808 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,236]  INFO {org.apache.spark.SparkContext} -  Starting job: count at SparkModelUtils.java:252 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,237]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 15 (count at SparkModelUtils.java:252) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,237]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 20(count at SparkModelUtils.java:252) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,237]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,238]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,239]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 20 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,240]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28496) called with curMem=432279, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,241]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28 stored as values in memory (estimated size 27.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,245]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10750) called with curMem=460775, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,245]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.5 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,246]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_28_piece0 in memory on localhost:44046 (size: 10.5 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,246]  INFO {org.apache.spark.SparkContext} -  Created broadcast 28 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,247]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,247]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 20.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,249]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 20.0 (TID 20, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,250]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 20.0 (TID 20) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,253]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_35_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,271]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 20.0 (TID 20). 1751 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,275]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 20.0 (TID 20) in 25 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,275]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 20.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,275]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 20 (count at SparkModelUtils.java:252) finished in 0.026 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,278]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 15 finished: count at SparkModelUtils.java:252, took 0.042030 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,279]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 15 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:47,280]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 15 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,289]  INFO {org.apache.spark.SparkContext} -  Starting job: collect at SupervisedSparkModelBuilder.java:835 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 16 (collect at SupervisedSparkModelBuilder.java:835) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 21(collect at SupervisedSparkModelBuilder.java:835) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,290]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,291]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,291]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 21 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,293]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(28640) called with curMem=441829, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,293]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29 stored as values in memory (estimated size 28.0 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,299]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(10806) called with curMem=470469, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,300]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_29_piece0 stored as bytes in memory (estimated size 10.6 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,304]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_29_piece0 in memory on localhost:44046 (size: 10.6 KB, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,305]  INFO {org.apache.spark.SparkContext} -  Created broadcast 29 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,305]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[35] at mapToPair at RandomForestClassifier.java:63) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,305]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 21.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,307]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 21.0 (TID 21, localhost, PROCESS_LOCAL, 1496 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,308]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 21.0 (TID 21) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,310]  INFO {org.apache.spark.storage.BlockManager} -  Found block rdd_35_0 locally {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,313]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 21.0 (TID 21). 6028 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,318]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 21.0 (TID 21) in 11 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,318]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 21 (collect at SupervisedSparkModelBuilder.java:835) finished in 0.010 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,318]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 21.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,323]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 16 finished: collect at SupervisedSparkModelBuilder.java:835, took 0.033804 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,324]  INFO {org.apache.spark.rdd.ParallelCollectionRDD} -  Removing RDD 38 from persistence list {org.apache.spark.rdd.ParallelCollectionRDD}
[2016-06-23 16:03:47,325]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 38 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,328]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 35 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:03:47,329]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 35 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:03:47,361]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:50 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 39 (map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 17 (collectAsMap at MulticlassMetrics.scala:50) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 23(collectAsMap at MulticlassMetrics.scala:50) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,363]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 22) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,364]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 22) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,365]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 22 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:47), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,366]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2864) called with curMem=465147, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,366]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30 stored as values in memory (estimated size 2.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,371]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1708) called with curMem=468011, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,372]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_30_piece0 stored as bytes in memory (estimated size 1708.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,372]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_30_piece0 in memory on localhost:44046 (size: 1708.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,372]  INFO {org.apache.spark.SparkContext} -  Created broadcast 30 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,373]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[39] at map at MulticlassMetrics.scala:47) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,373]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 22.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,376]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 22.0 (TID 22, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,377]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 22.0 (TID 22) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,404]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 22.0 (TID 22). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,407]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 22.0 (TID 22) in 34 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,407]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 22 (map at MulticlassMetrics.scala:47) finished in 0.019 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,407]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,408]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,408]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 23) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,408]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,407]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 22.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,409]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 23: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,409]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 23 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:49), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,409]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2296) called with curMem=469719, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,410]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31 stored as values in memory (estimated size 2.2 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,413]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1392) called with curMem=472015, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,413]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_31_piece0 stored as bytes in memory (estimated size 1392.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,414]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_31_piece0 in memory on localhost:44046 (size: 1392.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,414]  INFO {org.apache.spark.SparkContext} -  Created broadcast 31 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,415]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 23 (ShuffledRDD[40] at reduceByKey at MulticlassMetrics.scala:49) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,415]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 23.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,415]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 23.0 (TID 23, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,416]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 23.0 (TID 23) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,419]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:47,419]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 2 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:47,429]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 23.0 (TID 23). 889 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,431]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 23.0 (TID 23) in 16 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,431]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 23.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 23 (collectAsMap at MulticlassMetrics.scala:50) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,435]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 17 finished: collectAsMap at MulticlassMetrics.scala:50, took 0.073568 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,461]  INFO {org.apache.spark.SparkContext} -  Starting job: collectAsMap at MulticlassMetrics.scala:60 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,464]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Registering RDD 41 (map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,464]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 18 (collectAsMap at MulticlassMetrics.scala:60) with 1 output partitions (allowLocal=false) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,464]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 25(collectAsMap at MulticlassMetrics.scala:60) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,464]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List(ShuffleMapStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,465]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List(ShuffleMapStage 24) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,467]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ShuffleMapStage 24 (MapPartitionsRDD[41] at map at MulticlassMetrics.scala:57), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,467]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2872) called with curMem=473407, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,468]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32 stored as values in memory (estimated size 2.8 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,472]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1712) called with curMem=476279, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,472]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_32_piece0 stored as bytes in memory (estimated size 1712.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,472]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_32_piece0 in memory on localhost:44046 (size: 1712.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,473]  INFO {org.apache.spark.SparkContext} -  Created broadcast 32 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,473]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[41] at map at MulticlassMetrics.scala:57) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,473]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 24.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,484]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 24.0 (TID 24, localhost, PROCESS_LOCAL, 5554 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,484]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 24.0 (TID 24) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,529]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 24.0 (TID 24). 808 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,541]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 24.0 (TID 24) in 68 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,542]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 24.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,547]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ShuffleMapStage 24 (map at MulticlassMetrics.scala:57) finished in 0.060 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,547]  INFO {org.apache.spark.scheduler.DAGScheduler} -  looking for newly runnable stages {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  running: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  waiting: Set(ResultStage 25) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  failed: Set() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents for ResultStage 25: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,549]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 25 (ShuffledRDD[42] at reduceByKey at MulticlassMetrics.scala:59), which is now runnable {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,550]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(2304) called with curMem=477991, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,550]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33 stored as values in memory (estimated size 2.3 KB, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,567]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1390) called with curMem=480295, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,567]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_33_piece0 stored as bytes in memory (estimated size 1390.0 B, free 982.6 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:03:47,568]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_33_piece0 in memory on localhost:44046 (size: 1390.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:03:47,570]  INFO {org.apache.spark.SparkContext} -  Created broadcast 33 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:03:47,570]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[42] at reduceByKey at MulticlassMetrics.scala:59) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,570]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 25.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,571]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 25.0 (TID 25, localhost, PROCESS_LOCAL, 1165 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,573]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 25.0 (TID 25) {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,578]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Getting 1 non-empty blocks out of 1 blocks {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:47,578]  INFO {org.apache.spark.storage.ShuffleBlockFetcherIterator} -  Started 0 remote fetches in 4 ms {org.apache.spark.storage.ShuffleBlockFetcherIterator}
[2016-06-23 16:03:47,599]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 25.0 (TID 25). 951 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:03:47,604]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 25.0 (TID 25) in 34 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:03:47,604]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 25.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:03:47,607]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 25 (collectAsMap at MulticlassMetrics.scala:60) finished in 0.036 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:03:47,608]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 18 finished: collectAsMap at MulticlassMetrics.scala:60, took 0.146535 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:07:40,854]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_26_piece0 on localhost:44046 in memory (size: 930.0 B, free: 982.9 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,855]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_15_piece0 on localhost:44046 in memory (size: 9.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,859]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 2 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,860]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_14_piece0 on localhost:44046 in memory (size: 209.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,862]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:44046 in memory (size: 3.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,864]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:44046 in memory (size: 8.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,866]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 1 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,867]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:44046 in memory (size: 113.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,868]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:44046 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,868]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_33_piece0 on localhost:44046 in memory (size: 1390.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,869]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_32_piece0 on localhost:44046 in memory (size: 1712.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,870]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 6 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,871]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_31_piece0 on localhost:44046 in memory (size: 1392.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,872]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_30_piece0 on localhost:44046 in memory (size: 1708.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,873]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 5 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,874]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 38 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:07:40,875]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 38 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,876]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_29_piece0 on localhost:44046 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,878]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_28_piece0 on localhost:44046 in memory (size: 10.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,879]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_27_piece0 on localhost:44046 in memory (size: 10.7 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,882]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_25_piece0 on localhost:44046 in memory (size: 877.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,882]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 36 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:07:40,883]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 36 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,884]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_24_piece0 on localhost:44046 in memory (size: 3.5 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,886]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_23_piece0 on localhost:44046 in memory (size: 10.6 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,888]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 35 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:07:40,888]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 35 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,890]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_22_piece0 on localhost:44046 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,891]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_21_piece0 on localhost:44046 in memory (size: 12.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,892]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 4 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,893]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:44046 in memory (size: 173.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,894]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:44046 in memory (size: 3.9 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,895]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:44046 in memory (size: 11.5 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,900]  INFO {org.apache.spark.ContextCleaner} -  Cleaned shuffle 3 {org.apache.spark.ContextCleaner}
[2016-06-23 16:07:40,901]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:44046 in memory (size: 217.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:07:40,902]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:44046 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:13:37,801]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-23 16:13:37,819]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,823]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,823]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,823]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,824]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,825]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,825]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,827]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,828]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,829]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:13:37,881]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:13:37,884]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:13:37,942]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-23 16:13:37,946]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c7a0df50-5f43-4ebb-9c59-51490e998639/blockmgr-34527f91-b655-405a-8e3d-ac9e2e0159fc, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-23 16:13:37,946]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:13:37,947]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-23 16:13:37,948]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:13:37,950]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-23 16:13:37,950]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-23 16:13:37,950]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-23 16:13:37,950]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-c7a0df50-5f43-4ebb-9c59-51490e998639 {org.apache.spark.util.Utils}
[2016-06-23 16:13:37,959]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:13:37,961]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:13:37,983]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:13:38,884]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-23 16:13:42,568]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-23 16:14:16,512]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-23 16:14:16,628]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-23 16:14:16,686]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-23 16:14:16,686]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-23 16:14:16,703]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:14:16,704]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:14:16,704]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-23 16:14:17,222]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-23 16:14:17,271]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-23 16:14:17,411]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:57572] {Remoting}
[2016-06-23 16:14:17,420]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 57572. {org.apache.spark.util.Utils}
[2016-06-23 16:14:17,439]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-23 16:14:17,451]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-23 16:14:17,467]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-97300225-94d4-4145-b04a-8b75ee7f6085/blockmgr-f0da2ee6-dee6-4ec1-bf3c-61914359c091 {org.apache.spark.storage.DiskBlockManager}
[2016-06-23 16:14:17,472]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:14:17,494]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-97300225-94d4-4145-b04a-8b75ee7f6085/httpd-92db8fcc-8e40-43fe-98c0-56046892ab88 {org.apache.spark.HttpFileServer}
[2016-06-23 16:14:17,497]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-23 16:14:17,538]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:14:17,552]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:38693 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:14:17,553]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 38693. {org.apache.spark.util.Utils}
[2016-06-23 16:14:17,563]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-23 16:14:17,659]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:14:17,671]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:14:17,671]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-23 16:14:17,673]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4040 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:14:17,733]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-23 16:14:18,023]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45102. {org.apache.spark.util.Utils}
[2016-06-23 16:14:18,024]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 45102 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-23 16:14:18,026]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:14:18,030]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:45102 with 983.1 MB RAM, BlockManagerId(driver, localhost, 45102) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-23 16:14:18,033]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:14:59,703]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:14:59,706]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:14:59,865]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:14:59,866]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:14:59,868]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:14:59,872]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:14:59,957]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:15:00,010]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:15:00,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,033]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,033]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,038]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,043]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,049]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:00,050]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:00,078]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:00,079]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:00,079]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:45102 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:15:00,088]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:15:00,094]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,096]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:15:00,123]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:15:00,129]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-23 16:15:00,149]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466691299400:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:15:00,161]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 16:15:00,161]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 16:15:00,161]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 16:15:00,161]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 16:15:00,161]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-23 16:15:00,222]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:15:00,250]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 131 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:15:00,252]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:15:00,259]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.147 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:00,266]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.254234 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,519]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,519]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,532]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,532]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,533]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:15:52,534]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:15:52,540]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:15:52,545]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:15:52,546]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,546]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,546]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,547]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,548]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,549]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,549]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,555]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1878) called with curMem=93381, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,555]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 1878.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:15:52,556]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:45102 (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:15:52,556]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:15:52,557]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,557]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:15:52,558]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:15:52,558]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-23 16:15:52,561]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466691352513:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:15:52,567]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:15:52,571]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 14 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:15:52,571]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:15:52,572]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:15:52,572]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.026905 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:15,504]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=95259, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:15,508]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:15,520]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=133747, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:15,521]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:15,522]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:15,523]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-23 16:17:15,643]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 5 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:17:15,651]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 5 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:17:15,673]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 13 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:17:15,675]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:17:58,794]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137836, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,794]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,809]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176324, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,810]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,810]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,811]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:17:58,818]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:17:58,824]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:17:58,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,825]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,827]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,827]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[17] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,828]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=180413, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,828]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,867]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=183629, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,867]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:17:58,868]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:45102 (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,869]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:17:58,869]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,869]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:17:58,871]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:17:58,889]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-23 16:17:58,897]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466691478790:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:17:58,898]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 14 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:17:58,901]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 14 {org.apache.spark.ContextCleaner}
[2016-06-23 16:17:58,908]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 13 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:17:58,908]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 13 {org.apache.spark.ContextCleaner}
[2016-06-23 16:17:58,909]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 5 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:17:58,910]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 5 {org.apache.spark.ContextCleaner}
[2016-06-23 16:17:58,916]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:17:58,918]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:45102 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,921]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:45102 in memory (size: 1878.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,924]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 55 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:17:58,924]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:45102 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,924]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 0.055 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,925]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:17:58,925]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 0.100839 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:17:58,927]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:45102 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:17:58,939]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:45102 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:20:46,471]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47674, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:46,472]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:46,495]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86162, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:46,496]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:46,496]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:20:46,497]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-23 16:20:46,524]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 19 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:20:46,526]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 19 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:20:46,530]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 27 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-23 16:20:46,531]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 27 {org.apache.spark.storage.BlockManager}
[2016-06-23 16:20:53,779]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90251, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,779]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,793]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128739, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,793]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,794]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:45102 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:20:53,795]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:20:53,803]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-23 16:20:53,808]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-23 16:20:53,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,809]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,810]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[31] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,811]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132828, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,812]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,818]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1879) called with curMem=136044, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,818]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 1879.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:20:53,819]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:45102 (size: 1879.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-23 16:20:53,819]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-23 16:20:53,820]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[31] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,820]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:20:53,821]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:20:53,822]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-23 16:20:53,825]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466691653774:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-23 16:20:53,829]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-23 16:20:53,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (first at MLUtils.java:91) finished in 0.015 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: first at MLUtils.java:91, took 0.027182 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:20:53,839]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-23 16:20:53,840]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-23 16:47:41,476]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-23 16:47:41,599]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-23 16:47:41,662]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-23 16:47:41,663]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-23 16:47:41,679]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:47:41,680]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:47:41,680]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-23 16:47:42,059]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-23 16:47:42,113]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-23 16:47:42,245]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:35443] {Remoting}
[2016-06-23 16:47:42,250]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 35443. {org.apache.spark.util.Utils}
[2016-06-23 16:47:42,275]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-23 16:47:42,287]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-23 16:47:42,304]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e2e92238-635b-40c0-bcf9-23587b67a83f/blockmgr-068c69b2-4139-4e9e-a775-0b8488d3cd83 {org.apache.spark.storage.DiskBlockManager}
[2016-06-23 16:47:42,309]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:47:42,334]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e2e92238-635b-40c0-bcf9-23587b67a83f/httpd-0e724709-ce0c-49de-8ff5-fc3c6d070e3b {org.apache.spark.HttpFileServer}
[2016-06-23 16:47:42,337]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-23 16:47:42,375]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:47:42,389]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:59464 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:47:42,389]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 59464. {org.apache.spark.util.Utils}
[2016-06-23 16:47:42,400]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-23 16:47:42,514]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:47:42,523]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 16:47:42,525]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED org.spark-project.jetty.server.Server@4e4a781e: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,528]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,529]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,530]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:42,583]  WARN {org.apache.spark.util.Utils} -  Service 'SparkUI' could not bind on port 4040. Attempting port 4041. {org.apache.spark.util.Utils}
[2016-06-23 16:47:42,587]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:47:42,636]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4041 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:47:42,636]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4041. {org.apache.spark.util.Utils}
[2016-06-23 16:47:42,644]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:47:42,713]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-23 16:47:43,037]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58362. {org.apache.spark.util.Utils}
[2016-06-23 16:47:43,038]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 58362 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-23 16:47:43,039]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:47:43,043]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:58362 with 983.1 MB RAM, BlockManagerId(driver, localhost, 58362) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-23 16:47:43,046]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:47:43,479]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-23 16:47:43,495]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,497]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,498]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,499]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,499]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,501]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,509]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,510]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,513]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,517]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,520]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,521]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,522]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,523]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,523]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,524]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,524]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,524]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,525]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,525]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,526]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,526]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,526]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,526]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,526]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:47:43,579]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:47:43,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:47:43,651]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-23 16:47:43,662]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e2e92238-635b-40c0-bcf9-23587b67a83f/blockmgr-068c69b2-4139-4e9e-a775-0b8488d3cd83, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-23 16:47:43,664]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:47:43,665]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-23 16:47:43,674]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:47:43,678]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-23 16:47:43,679]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-23 16:47:43,679]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-e2e92238-635b-40c0-bcf9-23587b67a83f {org.apache.spark.util.Utils}
[2016-06-23 16:47:43,681]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-23 16:47:43,699]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:47:43,702]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:47:43,748]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:56:40,729]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-23 16:56:40,865]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-23 16:56:40,929]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-23 16:56:40,929]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-23 16:56:40,942]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:56:40,943]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 16:56:40,943]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-23 16:56:41,340]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-23 16:56:41,385]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-23 16:56:41,526]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:43945] {Remoting}
[2016-06-23 16:56:41,533]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 43945. {org.apache.spark.util.Utils}
[2016-06-23 16:56:41,563]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-23 16:56:41,576]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-23 16:56:41,596]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-99f0f96a-d537-4ef7-ad7d-3de3da7d768d/blockmgr-2b789d1a-a655-4e16-ba80-522167dc9ed8 {org.apache.spark.storage.DiskBlockManager}
[2016-06-23 16:56:41,602]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:56:41,626]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-99f0f96a-d537-4ef7-ad7d-3de3da7d768d/httpd-56a9f677-a321-4df6-92b5-344f65f63289 {org.apache.spark.HttpFileServer}
[2016-06-23 16:56:41,628]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-23 16:56:41,669]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:56:41,683]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:44638 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:56:41,683]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 44638. {org.apache.spark.util.Utils}
[2016-06-23 16:56:41,694]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-23 16:56:41,781]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:56:41,792]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 16:56:41,794]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED org.spark-project.jetty.server.Server@2d18a12d: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 16:56:41,796]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,796]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,796]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,796]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,796]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,797]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,798]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,798]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:41,850]  WARN {org.apache.spark.util.Utils} -  Service 'SparkUI' could not bind on port 4040. Attempting port 4041. {org.apache.spark.util.Utils}
[2016-06-23 16:56:41,854]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 16:56:41,864]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4041 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 16:56:41,865]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4041. {org.apache.spark.util.Utils}
[2016-06-23 16:56:41,866]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:56:41,922]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-23 16:56:42,233]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36232. {org.apache.spark.util.Utils}
[2016-06-23 16:56:42,234]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 36232 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-23 16:56:42,235]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:56:42,239]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:36232 with 983.1 MB RAM, BlockManagerId(driver, localhost, 36232) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-23 16:56:42,242]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:56:42,671]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-23 16:56:42,684]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,689]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,690]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,691]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,692]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,696]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,698]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,698]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,699]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,699]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,700]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,701]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,703]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,705]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,706]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,706]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,707]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,708]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,709]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,714]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,715]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,715]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,715]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,716]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,716]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 16:56:42,769]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 16:56:42,771]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 16:56:42,840]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-23 16:56:42,851]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-99f0f96a-d537-4ef7-ad7d-3de3da7d768d/blockmgr-2b789d1a-a655-4e16-ba80-522167dc9ed8, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-23 16:56:42,855]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-23 16:56:42,856]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-23 16:56:42,865]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 16:56:42,867]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-23 16:56:42,867]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-23 16:56:42,868]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-99f0f96a-d537-4ef7-ad7d-3de3da7d768d {org.apache.spark.util.Utils}
[2016-06-23 16:56:42,869]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-23 16:56:42,896]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:56:42,896]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 16:56:42,950]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 17:10:49,536]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-23 17:10:49,672]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-23 17:10:49,750]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.141 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-23 17:10:49,751]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-23 17:10:49,765]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 17:10:49,765]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-23 17:10:49,766]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-23 17:10:50,180]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-23 17:10:50,237]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-23 17:10:50,396]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.141:44745] {Remoting}
[2016-06-23 17:10:50,403]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 44745. {org.apache.spark.util.Utils}
[2016-06-23 17:10:50,426]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-23 17:10:50,441]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-23 17:10:50,462]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d50348f2-974c-45ad-b14c-0ec8f9150e4d/blockmgr-2f17d0a4-c22a-41f3-a126-002ecd911307 {org.apache.spark.storage.DiskBlockManager}
[2016-06-23 17:10:50,467]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-23 17:10:50,494]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d50348f2-974c-45ad-b14c-0ec8f9150e4d/httpd-9eb44d06-f4ff-410b-9d70-027e601ba62e {org.apache.spark.HttpFileServer}
[2016-06-23 17:10:50,497]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-23 17:10:50,537]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 17:10:50,589]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:51256 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 17:10:50,589]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 51256. {org.apache.spark.util.Utils}
[2016-06-23 17:10:50,600]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-23 17:10:50,705]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 17:10:50,714]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 17:10:50,717]  WARN {org.spark-project.jetty.util.component.AbstractLifeCycle} -  FAILED org.spark-project.jetty.server.Server@35634c39: java.net.BindException: Address already in use {org.spark-project.jetty.util.component.AbstractLifeCycle}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:117)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:448)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at org.wso2.carbon.ml.core.internal.MLCoreDS.activate(MLCoreDS.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.ml.database.internal.ds.MLDatabaseServiceDS.activate(MLDatabaseServiceDS.java:42)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:451)
	at org.wso2.carbon.core.init.CarbonServerManager.initializeCarbon(CarbonServerManager.java:514)
	at org.wso2.carbon.core.init.CarbonServerManager.start(CarbonServerManager.java:219)
	at org.wso2.carbon.core.internal.CarbonCoreServiceComponent.activate(CarbonCoreServiceComponent.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:260)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)
	at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)
	at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)
	at org.eclipse.equinox.internal.ds.Resolver.getEligible(Resolver.java:343)
	at org.eclipse.equinox.internal.ds.SCRManager.serviceChanged(SCRManager.java:222)
	at org.eclipse.osgi.internal.serviceregistry.FilteredServiceListener.serviceChanged(FilteredServiceListener.java:107)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.dispatchEvent(BundleContextImpl.java:861)
	at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)
	at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEventPrivileged(ServiceRegistry.java:819)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.publishServiceEvent(ServiceRegistry.java:771)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistrationImpl.register(ServiceRegistrationImpl.java:130)
	at org.eclipse.osgi.internal.serviceregistry.ServiceRegistry.registerService(ServiceRegistry.java:214)
	at org.eclipse.osgi.framework.internal.core.BundleContextImpl.registerService(BundleContextImpl.java:433)
	at org.eclipse.equinox.http.servlet.internal.Activator.registerHttpService(Activator.java:81)
	at org.eclipse.equinox.http.servlet.internal.Activator.addProxyServlet(Activator.java:60)
	at org.eclipse.equinox.http.servlet.internal.ProxyServlet.init(ProxyServlet.java:40)
	at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.init(DelegationServlet.java:38)
	at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1284)
	at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1197)
	at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:1087)
	at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:5262)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5550)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-06-23 17:10:50,722]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,722]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,723]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,724]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,725]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,725]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,725]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,725]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,725]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:50,777]  WARN {org.apache.spark.util.Utils} -  Service 'SparkUI' could not bind on port 4040. Attempting port 4041. {org.apache.spark.util.Utils}
[2016-06-23 17:10:50,780]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-23 17:10:50,790]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4041 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-23 17:10:50,791]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4041. {org.apache.spark.util.Utils}
[2016-06-23 17:10:50,792]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 17:10:50,859]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-23 17:10:51,174]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50547. {org.apache.spark.util.Utils}
[2016-06-23 17:10:51,174]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 50547 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-23 17:10:51,175]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 17:10:51,179]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:50547 with 983.1 MB RAM, BlockManagerId(driver, localhost, 50547) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-23 17:10:51,181]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 17:10:51,634]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-23 17:10:51,649]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,664]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,665]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,665]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,666]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,666]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,667]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,669]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,671]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,675]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,678]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,678]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,679]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,679]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,679]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,680]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,680]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,681]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,681]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,682]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-23 17:10:51,736]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.44.141:4041 {org.apache.spark.ui.SparkUI}
[2016-06-23 17:10:51,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-23 17:10:51,806]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-23 17:10:51,823]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d50348f2-974c-45ad-b14c-0ec8f9150e4d/blockmgr-2f17d0a4-c22a-41f3-a126-002ecd911307, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-23 17:10:51,824]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-23 17:10:51,830]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-23 17:10:51,841]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-23 17:10:51,851]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-23 17:10:51,855]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-23 17:10:51,855]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-23 17:10:51,856]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d50348f2-974c-45ad-b14c-0ec8f9150e4d {org.apache.spark.util.Utils}
[2016-06-23 17:10:51,867]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 17:10:51,878]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-23 17:10:51,936]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 14:59:53,760]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-24 14:59:53,972]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-24 14:59:54,042]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.18.51 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-24 14:59:54,042]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-24 14:59:54,061]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-24 14:59:54,062]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-24 14:59:54,063]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-24 14:59:54,534]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-24 14:59:54,593]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-24 14:59:54,742]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.18.51:55722] {Remoting}
[2016-06-24 14:59:54,747]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 55722. {org.apache.spark.util.Utils}
[2016-06-24 14:59:54,765]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-24 14:59:54,775]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-24 14:59:54,790]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-02a40d5f-a7e1-4330-9ae2-c19d81f05c1b/blockmgr-d9de8e9a-6fc8-420e-8fb0-d696e553308c {org.apache.spark.storage.DiskBlockManager}
[2016-06-24 14:59:54,796]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-24 14:59:54,822]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-02a40d5f-a7e1-4330-9ae2-c19d81f05c1b/httpd-20bb5f92-920c-4f5e-a9ea-1b182aab1d73 {org.apache.spark.HttpFileServer}
[2016-06-24 14:59:54,825]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-24 14:59:54,875]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-24 14:59:54,886]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:43668 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-24 14:59:54,886]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 43668. {org.apache.spark.util.Utils}
[2016-06-24 14:59:54,895]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-24 14:59:55,016]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-24 14:59:55,027]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-24 14:59:55,028]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-24 14:59:55,029]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.18.51:4040 {org.apache.spark.ui.SparkUI}
[2016-06-24 14:59:55,089]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-24 14:59:55,377]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46219. {org.apache.spark.util.Utils}
[2016-06-24 14:59:55,377]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 46219 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-24 14:59:55,378]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 14:59:55,382]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:46219 with 983.1 MB RAM, BlockManagerId(driver, localhost, 46219) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 14:59:55,384]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:00:09,839]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:09,842]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,008]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,008]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,011]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:46219 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:00:10,015]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:00:10,090]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:00:10,140]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:00:10,156]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,157]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,157]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,160]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,165]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,169]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,170]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,206]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,207]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:00:10,207]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:46219 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:00:10,208]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:00:10,214]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,215]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:00:10,238]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:00:10,244]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-24 15:00:10,267]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466773209542:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:00:10,275]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:00:10,275]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:00:10,275]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:00:10,275]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:00:10,275]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:00:10,317]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:00:10,340]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.114 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,347]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.206531 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:00:10,357]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 103 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:00:10,360]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:04:26,968]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:04:26,971]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:04:27,044]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:04:27,045]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:04:27,047]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:46219 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:04:27,064]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:05:36,430]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:05:38,152]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:05:42,941]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:05:42,945]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:16:08,688]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 429311 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-24 15:16:08,692] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 429311 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:16:08,744]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:16:08,745]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:16:08,758]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 46219) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:16:08,759]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-24 15:16:08,759]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-24 15:16:08,759]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:16:08,759]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:16:08,772]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:46219 with 983.1 MB RAM, BlockManagerId(driver, localhost, 46219) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:16:08,773]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:16:08,774]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 6 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-24 15:16:08,775]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-24 15:16:08,775]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:16:08,777]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:46219 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:16:08,780]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:46219 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:16:08,784]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:46219 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:16:08,793]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,794]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,803]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,808]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,809]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,810]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,811]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,814]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,817]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,825]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,826]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,830]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,834]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,835]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,836]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,837]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,842]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,843]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,843]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,845]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,846]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,847]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,847]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,848]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,848]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 15:16:08,901]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.18.51:4040 {org.apache.spark.ui.SparkUI}
[2016-06-24 15:16:08,905]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:16:08,962]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-24 15:16:08,966]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-02a40d5f-a7e1-4330-9ae2-c19d81f05c1b/blockmgr-d9de8e9a-6fc8-420e-8fb0-d696e553308c, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-24 15:16:08,967]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:16:08,967]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-24 15:16:08,968]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:16:08,970]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-24 15:16:08,970]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-24 15:16:08,970]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-24 15:16:08,970]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-02a40d5f-a7e1-4330-9ae2-c19d81f05c1b {org.apache.spark.util.Utils}
[2016-06-24 15:16:08,980]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 15:16:08,982]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 15:16:09,008]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 15:16:09,910]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-24 15:16:13,861]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-24 15:23:30,725]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-24 15:23:30,861]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-24 15:23:30,931]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.18.51 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-24 15:23:30,932]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-24 15:23:30,951]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-24 15:23:30,951]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-24 15:23:30,952]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-24 15:23:31,330]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-24 15:23:31,366]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-24 15:23:31,508]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.18.51:34823] {Remoting}
[2016-06-24 15:23:31,513]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 34823. {org.apache.spark.util.Utils}
[2016-06-24 15:23:31,542]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-24 15:23:31,561]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-24 15:23:31,588]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-af4ff1d0-0ad5-4030-8681-9dbfc7f6ea39/blockmgr-24a8c813-86e8-4d52-8475-cbaaa1f148c1 {org.apache.spark.storage.DiskBlockManager}
[2016-06-24 15:23:31,594]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:23:31,621]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-af4ff1d0-0ad5-4030-8681-9dbfc7f6ea39/httpd-312ea34b-7ad2-4d5d-be96-ceb0d03e4424 {org.apache.spark.HttpFileServer}
[2016-06-24 15:23:31,625]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-24 15:23:31,676]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-24 15:23:31,692]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:42037 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-24 15:23:31,692]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 42037. {org.apache.spark.util.Utils}
[2016-06-24 15:23:31,701]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-24 15:23:31,781]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-24 15:23:31,794]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-24 15:23:31,794]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-24 15:23:31,796]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.18.51:4040 {org.apache.spark.ui.SparkUI}
[2016-06-24 15:23:31,860]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-24 15:23:32,146]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44549. {org.apache.spark.util.Utils}
[2016-06-24 15:23:32,147]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 44549 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-24 15:23:32,148]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:23:32,151]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:44549 with 983.1 MB RAM, BlockManagerId(driver, localhost, 44549) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:23:32,154]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:24:43,734]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:43,737]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:43,934]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:43,935]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:43,937]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:24:43,941]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:24:44,031]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:24:44,090]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:24:44,107]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,108]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,108]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,112]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,117]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,123]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:44,124]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:44,141]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:44,142]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:24:44,143]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:44549 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:24:44,144]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:24:44,151]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,152]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:24:44,188]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:24:44,195]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-24 15:24:44,215]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466774683446:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:24:44,223]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:24:44,223]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:24:44,223]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:24:44,223]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:24:44,223]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-24 15:24:44,261]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:24:44,275]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.103 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:24:44,275]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 90 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:24:44,280]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:24:44,288]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.196999 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:25:52,562]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:25:52,564]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:25:52,631]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:25:52,633]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:25:52,634]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:25:52,650]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:29:49,890]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:29:51,014]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:29:59,286]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:29:59,291]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:32:51,323]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:32:51,477]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-24 15:32:51,480]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:32:56,684]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-24 15:33:23,116]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:44549 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:33:24,655]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:37:11,718]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:11,736]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:12,795]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=81065, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:12,813]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:12,850]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:37:13,028]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:37:14,124]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:37:14,553]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:37:14,589]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:14,591]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:14,594]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:14,653]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:14,695]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:14,973]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:15,062]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:15,486]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=88370, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:15,504]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1882.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:37:15,550]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:44549 (size: 1882.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:37:15,615]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:37:15,628]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:15,630]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:37:15,687]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:37:15,709]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-24 15:37:15,868]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466775428823:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:37:16,636]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:37:17,099]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 1.420 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:37:17,112]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 1399 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:37:17,141]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:37:17,152]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 2.576783 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:41:48,759]  WARN {org.apache.spark.HeartbeatReceiver} -  Removing executor driver with no recent heartbeats: 186223 ms exceeds timeout 120000 ms {org.apache.spark.HeartbeatReceiver}
[2016-06-24 15:41:48,852] ERROR {org.apache.spark.scheduler.TaskSchedulerImpl} -  Lost executor driver on localhost: Executor heartbeat timed out after 186223 ms {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:41:48,883]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Executor lost: driver (epoch 0) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:41:48,894]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Trying to remove executor driver from BlockManagerMaster. {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:41:48,901]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Removing block manager BlockManagerId(driver, localhost, 44549) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:41:48,905]  WARN {org.apache.spark.executor.Executor} -  Told to re-register on heartbeat {org.apache.spark.executor.Executor}
[2016-06-24 15:41:48,906]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager re-registering with master {org.apache.spark.storage.BlockManager}
[2016-06-24 15:41:48,906]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:41:48,907]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:44549 with 983.1 MB RAM, BlockManagerId(driver, localhost, 44549) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-24 15:41:48,907]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:41:48,908]  INFO {org.apache.spark.storage.BlockManager} -  Reporting 6 blocks to the master. {org.apache.spark.storage.BlockManager}
[2016-06-24 15:41:48,910]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:44549 (size: 1882.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:41:48,912]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:41:48,914]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:41:48,907]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Removed driver successfully in removeExecutor {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 15:41:48,928]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Host added was in lost list earlier: localhost {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:42:06,146]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90252, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:42:06,162]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:42:06,766]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128740, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:42:06,782]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:42:06,793]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:42:06,953]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:42:18,752]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:42:18,754]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:42:19,495]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:42:19,496]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:42:40,338]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:42:40,344]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 26 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:42:40,345]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 26 {org.apache.spark.ContextCleaner}
[2016-06-24 15:42:40,347]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:42:40,348]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 25 {org.apache.spark.ContextCleaner}
[2016-06-24 15:42:40,354]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:44549 in memory (size: 1882.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:43:21,816]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:21,874]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:23,357]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=123642, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:23,376]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:23,418]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_6_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:43:23,619]  INFO {org.apache.spark.SparkContext} -  Created broadcast 6 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:43:24,656]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:43:25,089]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:43:25,125]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 2 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:25,127]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 2(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:25,129]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:25,193]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:25,233]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:25,361]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=127731, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:25,378]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:26,080]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=130947, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:26,097]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_7_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:43:26,136]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_7_piece0 in memory on localhost:44549 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:43:26,203]  INFO {org.apache.spark.SparkContext} -  Created broadcast 7 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:43:26,216]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[29] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:26,219]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 2.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:43:26,273]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:43:26,293]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 2.0 (TID 2) {org.apache.spark.executor.Executor}
[2016-06-24 15:43:26,483]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466775798269:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:43:26,965]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 2.0 (TID 2). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:43:27,734]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 2 (first at MLUtils.java:91) finished in 1.466 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:43:27,747]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 2.0 (TID 2) in 1443 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:43:27,779]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 2.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:43:27,785]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 2 finished: first at MLUtils.java:91, took 2.663500 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:45:04,339]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=132828, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:45:04,355]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:45:04,952]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=171316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:45:04,968]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:45:04,979]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_8_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:45:05,133]  INFO {org.apache.spark.SparkContext} -  Created broadcast 8 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:45:05,710]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 31 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:45:05,711]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:45:05,720]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 39 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:45:05,721]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:49:30,724]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 40 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:49:30,726]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 40 {org.apache.spark.ContextCleaner}
[2016-06-24 15:49:30,726]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 39 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:49:30,727]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 39 {org.apache.spark.ContextCleaner}
[2016-06-24 15:49:30,727]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 31 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:49:30,727]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 31 {org.apache.spark.ContextCleaner}
[2016-06-24 15:49:30,728]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_8_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:49:30,729]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_7_piece0 on localhost:44549 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:49:30,730]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_6_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:50:16,431]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=85154, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:16,449]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:17,569]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=123642, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:17,587]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:17,629]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_9_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:50:18,247]  INFO {org.apache.spark.SparkContext} -  Created broadcast 9 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:50:20,115]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:50:20,866]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:50:20,901]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 3 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:20,904]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 3(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:20,906]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:20,966]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:21,016]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 3 (MapPartitionsRDD[43] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:21,133]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=127731, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:21,150]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:21,590]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=130947, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:21,653]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_10_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:50:21,786]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_10_piece0 in memory on localhost:44549 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:50:22,000]  INFO {org.apache.spark.SparkContext} -  Created broadcast 10 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:50:22,013]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[43] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:22,016]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 3.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:50:22,100]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:50:22,120]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 3.0 (TID 3) {org.apache.spark.executor.Executor}
[2016-06-24 15:50:22,281]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466776214987:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:50:23,435]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 3.0 (TID 3). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:50:23,922]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 3.0 (TID 3) in 1836 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:50:23,927]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 3 (first at MLUtils.java:91) finished in 1.857 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:50:23,945]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 3.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:50:23,971]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 3 finished: first at MLUtils.java:91, took 3.081030 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:03,928]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=132828, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:03,929]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:03,949]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=171316, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:03,950]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:03,950]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_11_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:52:03,954]  INFO {org.apache.spark.SparkContext} -  Created broadcast 11 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:52:03,997]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 45 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:52:04,000]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 45 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:52:04,014]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 53 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:52:04,026]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 53 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:52:40,769]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=175405, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:40,787]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:41,766]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=213893, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:41,784]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:41,824]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_12_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:52:42,017]  INFO {org.apache.spark.SparkContext} -  Created broadcast 12 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:52:43,152]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 15:52:43,587]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 15:52:43,622]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 4 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:43,625]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 4(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:43,627]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:43,690]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:43,738]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 4 (MapPartitionsRDD[57] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:43,845]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=217982, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:43,860]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:44,610]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=221198, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:44,628]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_13_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:52:44,668]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_13_piece0 in memory on localhost:44549 (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:52:44,738]  INFO {org.apache.spark.SparkContext} -  Created broadcast 13 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 15:52:44,751]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[57] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:44,754]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 4.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:52:44,806]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 4.0 (TID 4, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:52:44,827]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 4.0 (TID 4) {org.apache.spark.executor.Executor}
[2016-06-24 15:52:44,985]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466776358811:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 15:52:45,594]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 4.0 (TID 4). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 15:52:46,262]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 4.0 (TID 4) in 1438 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 15:52:46,262]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 4 (first at MLUtils.java:91) finished in 1.458 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:52:46,285]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 4.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 15:52:46,314]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 4 finished: first at MLUtils.java:91, took 2.701636 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 15:54:12,989]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=223079, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:54:13,006]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14 stored as values in memory (estimated size 37.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:54:13,999]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=261567, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:54:14,020]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 15:54:14,056]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_14_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:54:14,253]  INFO {org.apache.spark.SparkContext} -  Created broadcast 14 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 15:54:42,724]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 59 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:54:42,730]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 59 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:54:57,866]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 67 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 15:54:57,879]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 67 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:57:35,762]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_12_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:57:35,780]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_13_piece0 on localhost:44549 in memory (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:57:35,781]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 54 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:57:35,782]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 54 {org.apache.spark.ContextCleaner}
[2016-06-24 15:57:35,782]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 53 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:57:35,783]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 53 {org.apache.spark.ContextCleaner}
[2016-06-24 15:57:35,783]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 45 {org.apache.spark.storage.BlockManager}
[2016-06-24 15:57:35,783]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 45 {org.apache.spark.ContextCleaner}
[2016-06-24 15:57:35,784]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_11_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:57:35,785]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_10_piece0 on localhost:44549 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 15:57:35,786]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_9_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:12:18,384]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=127731, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:18,445]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:20,170]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=166219, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:20,242]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:20,356]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_15_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:12:20,698]  INFO {org.apache.spark.SparkContext} -  Created broadcast 15 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 16:12:21,473]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 16:12:22,313]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 16:12:22,348]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 5 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:22,350]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 5(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:22,353]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:22,417]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:22,465]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 5 (MapPartitionsRDD[71] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:22,677]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=170308, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:22,694]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:23,147]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1882) called with curMem=173524, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:23,224]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_16_piece0 stored as bytes in memory (estimated size 1882.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:12:23,350]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_16_piece0 in memory on localhost:44549 (size: 1882.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:12:23,516]  INFO {org.apache.spark.SparkContext} -  Created broadcast 16 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 16:12:23,537]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[71] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:23,540]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 5.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 16:12:23,595]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 5.0 (TID 5, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 16:12:23,618]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 5.0 (TID 5) {org.apache.spark.executor.Executor}
[2016-06-24 16:12:23,777]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466777536425:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 16:12:24,279]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 5.0 (TID 5). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 16:12:25,114]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 5.0 (TID 5) in 1500 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 16:12:25,121]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 5 (first at MLUtils.java:91) finished in 1.523 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:12:25,134]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 5.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 16:12:25,158]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 5 finished: first at MLUtils.java:91, took 2.825515 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:13:58,643]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=175406, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:13:58,667]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:13:59,337]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=213894, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:13:59,355]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:13:59,393]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_17_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:13:59,597]  INFO {org.apache.spark.SparkContext} -  Created broadcast 17 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 16:14:20,068]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 73 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 16:14:20,073]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 73 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:14:20,083]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 81 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 16:14:20,086]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 81 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:14:28,248]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=217983, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,248]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18 stored as values in memory (estimated size 37.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,260]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=256471, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,261]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,262]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_18_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:14:28,262]  INFO {org.apache.spark.SparkContext} -  Created broadcast 18 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 16:14:28,270]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-24 16:14:28,275]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-24 16:14:28,276]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 6 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,276]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 6(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,276]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,277]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 6 (MapPartitionsRDD[85] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,279]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=260560, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,279]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19 stored as values in memory (estimated size 3.1 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,284]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=263776, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,285]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_19_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:28,285]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_19_piece0 in memory on localhost:44549 (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:14:28,286]  INFO {org.apache.spark.SparkContext} -  Created broadcast 19 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-24 16:14:28,286]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[85] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,286]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 6.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 16:14:28,287]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 6.0 (TID 6, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 16:14:28,288]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 6.0 (TID 6) {org.apache.spark.executor.Executor}
[2016-06-24 16:14:28,291]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466777668242:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-24 16:14:28,297]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 6.0 (TID 6). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-24 16:14:28,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 6 (first at MLUtils.java:91) finished in 0.014 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,302]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 6 finished: first at MLUtils.java:91, took 0.027448 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:14:28,303]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 6.0 (TID 6) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-24 16:14:28,303]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 6.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-24 16:14:59,671]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=265657, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:59,672]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20 stored as values in memory (estimated size 37.6 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:59,683]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=304145, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:59,683]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.8 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:14:59,684]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_20_piece0 in memory on localhost:44549 (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:14:59,684]  INFO {org.apache.spark.SparkContext} -  Created broadcast 20 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-24 16:14:59,715]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 87 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 16:14:59,717]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 87 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:14:59,721]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 95 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-24 16:14:59,721]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 95 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:30,551]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 96 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,115]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 96 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,116]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 95 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,117]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 95 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,117]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 87 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,117]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 87 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,118]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_20_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:17:31,119]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_19_piece0 on localhost:44549 in memory (size: 1881.0 B, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:17:31,120]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_18_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.0 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:17:31,122]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 82 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,122]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 82 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,122]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 81 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,122]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 81 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,123]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 73 {org.apache.spark.storage.BlockManager}
[2016-06-24 16:17:31,123]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 73 {org.apache.spark.ContextCleaner}
[2016-06-24 16:17:31,123]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_17_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:17:31,124]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_16_piece0 on localhost:44549 in memory (size: 1882.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:17:31,125]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_15_piece0 on localhost:44549 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-24 16:18:21,430]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-24 16:18:21,446]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,451]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,451]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,457]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,457]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,457]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,457]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,458]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,458]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,458]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-24 16:18:21,512]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://10.126.18.51:4040 {org.apache.spark.ui.SparkUI}
[2016-06-24 16:18:21,514]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-24 16:18:21,575]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-24 16:18:21,581]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-af4ff1d0-0ad5-4030-8681-9dbfc7f6ea39/blockmgr-24a8c813-86e8-4d52-8475-cbaaa1f148c1, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-24 16:18:21,582]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-24 16:18:21,583]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-24 16:18:21,584]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-24 16:18:21,588]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-24 16:18:21,588]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-24 16:18:21,589]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-24 16:18:21,590]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-af4ff1d0-0ad5-4030-8681-9dbfc7f6ea39 {org.apache.spark.util.Utils}
[2016-06-24 16:18:21,598]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 16:18:21,601]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 16:18:21,807]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-24 16:18:22,497]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-24 16:18:26,358]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 14:54:21,341]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 14:54:21,448]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 14:54:21,500]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 14:54:21,500]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 14:54:21,515]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 14:54:21,516]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 14:54:21,516]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 14:54:21,943]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 14:54:21,989]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 14:54:22,135]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:43200] {Remoting}
[2016-06-26 14:54:22,140]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 43200. {org.apache.spark.util.Utils}
[2016-06-26 14:54:22,170]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 14:54:22,190]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 14:54:22,210]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4fbb8c0c-3678-4eb8-a6da-ac43bab8cc31/blockmgr-db5e7506-1135-4e35-b5a2-16d65cf08eb4 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 14:54:22,214]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:22,240]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4fbb8c0c-3678-4eb8-a6da-ac43bab8cc31/httpd-0a5b18de-488d-414b-bd69-fe21ae3849c0 {org.apache.spark.HttpFileServer}
[2016-06-26 14:54:22,242]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 14:54:22,286]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 14:54:22,301]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:44933 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 14:54:22,301]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 44933. {org.apache.spark.util.Utils}
[2016-06-26 14:54:22,311]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 14:54:22,401]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 14:54:22,415]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 14:54:22,416]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 14:54:22,417]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 14:54:22,480]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 14:54:22,785]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40726. {org.apache.spark.util.Utils}
[2016-06-26 14:54:22,786]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 40726 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 14:54:22,786]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 14:54:22,790]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:40726 with 983.1 MB RAM, BlockManagerId(driver, localhost, 40726) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 14:54:22,793]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 14:54:31,178]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,184]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,440]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,440]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,443]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:40726 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 14:54:31,447]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 14:54:31,548]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 14:54:31,600]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 14:54:31,616]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,617]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,624]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,631]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,638]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,640]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,660]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,660]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:54:31,661]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:40726 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 14:54:31,662]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 14:54:31,667]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,669]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 14:54:31,705]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 14:54:31,715]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 14:54:31,738]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466945670828:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 14:54:31,747]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 14:54:31,747]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 14:54:31,747]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 14:54:31,747]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 14:54:31,747]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 14:54:31,792]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 14:54:31,810]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 114 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 14:54:31,812]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.131 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:54:31,818]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 14:54:31,822]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.221845 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 14:55:03,875]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:55:03,875]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:55:03,891]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:55:03,891]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 14:55:03,892]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:40726 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 14:55:03,893]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 14:55:04,016]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 14:55:04,024]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 14:55:04,049]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 14:55:04,050]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:00:08,211]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:00:08,213]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-26 15:00:08,216]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:00:08,217]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-26 15:00:08,218]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:00:08,220]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-26 15:00:08,225]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:40726 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:00:08,227]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:40726 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:00:08,228]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:40726 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:02:44,759]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 15:02:44,773]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,773]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,773]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,773]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,774]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,774]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,774]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,774]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,775]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,775]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,775]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,775]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,776]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,776]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,776]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,776]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,776]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,777]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,777]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,778]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,778]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,778]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,778]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,778]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,779]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:02:44,832]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 15:02:44,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:02:44,891]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 15:02:44,897]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4fbb8c0c-3678-4eb8-a6da-ac43bab8cc31/blockmgr-db5e7506-1135-4e35-b5a2-16d65cf08eb4, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 15:02:44,897]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:02:44,900]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 15:02:44,900]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:02:44,902]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 15:02:44,902]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 15:02:44,903]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 15:02:44,903]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4fbb8c0c-3678-4eb8-a6da-ac43bab8cc31 {org.apache.spark.util.Utils}
[2016-06-26 15:02:44,914]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:02:44,915]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:02:44,933]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:02:45,804]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 15:02:49,651]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 15:06:10,502]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 15:06:10,615]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 15:06:10,664]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 15:06:10,665]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 15:06:10,679]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 15:06:10,680]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 15:06:10,680]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 15:06:11,044]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 15:06:11,081]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 15:06:11,210]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:52332] {Remoting}
[2016-06-26 15:06:11,218]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 52332. {org.apache.spark.util.Utils}
[2016-06-26 15:06:11,241]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 15:06:11,256]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 15:06:11,278]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-863d4e25-d174-405b-a0d6-b405f7eb24d2/blockmgr-8ec90b6a-20c0-492f-8f80-9d173c6a0865 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 15:06:11,284]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:11,314]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-863d4e25-d174-405b-a0d6-b405f7eb24d2/httpd-71bab2a3-9b32-4dc7-a729-4f6d141ed89a {org.apache.spark.HttpFileServer}
[2016-06-26 15:06:11,316]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 15:06:11,362]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 15:06:11,373]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:34409 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 15:06:11,373]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 34409. {org.apache.spark.util.Utils}
[2016-06-26 15:06:11,384]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 15:06:11,498]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 15:06:11,509]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 15:06:11,510]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 15:06:11,513]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 15:06:11,573]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 15:06:11,867]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58063. {org.apache.spark.util.Utils}
[2016-06-26 15:06:11,867]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 58063 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 15:06:11,868]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:06:11,871]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:58063 with 983.1 MB RAM, BlockManagerId(driver, localhost, 58063) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 15:06:11,873]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:06:20,300]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,304]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,490]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,490]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,493]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:58063 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:06:20,497]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 15:06:20,616]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 15:06:20,669]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 15:06:20,690]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,690]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,691]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,698]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,718]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,727]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,729]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,751]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,752]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:20,753]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:58063 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:06:20,753]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 15:06:20,769]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,774]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 15:06:20,817]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 15:06:20,827]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 15:06:20,853]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466946379924:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 15:06:20,861]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:06:20,861]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:06:20,861]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:06:20,861]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:06:20,861]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:06:20,903]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 15:06:20,920]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 114 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 15:06:20,922]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 15:06:20,926]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.133 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:20,932]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.263307 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:06:52,842]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:52,843]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:52,864]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:52,864]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:06:52,865]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:58063 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:06:52,866]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 15:06:52,971]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 15:06:52,977]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:06:53,012]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 15:06:53,013]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:09:10,438]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 15:09:10,453]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,453]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,457]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,458]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,458]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,459]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,460]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,461]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,461]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:09:10,512]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 15:09:10,514]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:09:10,572]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 15:09:10,575]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-863d4e25-d174-405b-a0d6-b405f7eb24d2/blockmgr-8ec90b6a-20c0-492f-8f80-9d173c6a0865, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 15:09:10,577]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:09:10,578]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 15:09:10,579]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:09:10,583]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 15:09:10,583]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 15:09:10,583]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 15:09:10,584]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-863d4e25-d174-405b-a0d6-b405f7eb24d2 {org.apache.spark.util.Utils}
[2016-06-26 15:09:10,591]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:09:10,593]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:09:10,614]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:09:11,491]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 15:09:15,267]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 15:35:18,992]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 15:35:19,113]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 15:35:19,174]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 15:35:19,175]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 15:35:19,191]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 15:35:19,191]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 15:35:19,192]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 15:35:19,598]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 15:35:19,635]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 15:35:19,770]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:42092] {Remoting}
[2016-06-26 15:35:19,780]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 42092. {org.apache.spark.util.Utils}
[2016-06-26 15:35:19,807]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 15:35:19,822]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 15:35:19,843]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-55556f18-4521-4412-abf1-5311c990b9db/blockmgr-d028902b-a274-42b3-ae33-256b766aace6 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 15:35:19,848]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:19,881]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-55556f18-4521-4412-abf1-5311c990b9db/httpd-625c04b9-93eb-4838-826b-c26a2b9b9719 {org.apache.spark.HttpFileServer}
[2016-06-26 15:35:19,884]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 15:35:19,933]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 15:35:19,944]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:42000 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 15:35:19,944]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 42000. {org.apache.spark.util.Utils}
[2016-06-26 15:35:19,952]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 15:35:20,471]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 15:35:20,484]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 15:35:20,484]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 15:35:20,486]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 15:35:20,543]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 15:35:20,829]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41918. {org.apache.spark.util.Utils}
[2016-06-26 15:35:20,830]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 41918 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 15:35:20,830]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:35:20,834]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:41918 with 983.1 MB RAM, BlockManagerId(driver, localhost, 41918) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 15:35:20,836]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:35:40,187]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,191]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,339]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,339]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,341]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:41918 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:35:40,344]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 15:35:40,440]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 15:35:40,497]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 15:35:40,518]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,519]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,519]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,524]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,534]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,544]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,546]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,578]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,579]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:35:40,579]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:41918 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:35:40,580]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 15:35:40,587]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,589]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 15:35:40,625]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 15:35:40,634]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 15:35:40,662]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466948139892:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 15:35:40,674]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:35:40,674]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:35:40,674]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:35:40,674]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:35:40,674]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 15:35:40,723]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 15:35:40,749]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.135 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,747]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 130 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 15:35:40,761]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.259568 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:35:40,757]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 15:36:12,736]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:36:12,736]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:36:12,750]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:36:12,751]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:36:12,752]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:41918 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:36:12,756]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 15:36:12,881]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 15:36:12,892]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:36:12,908]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 15:36:12,909]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:41:09,240]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:41:09,242]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-26 15:41:09,245]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:41:09,246]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-26 15:41:09,246]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 15:41:09,247]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-26 15:41:09,254]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:41918 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:41:09,257]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:41918 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:41:09,258]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:41918 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 15:49:16,971]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 15:49:16,987]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,988]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,989]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,990]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,991]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,993]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,996]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,996]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,996]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,997]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,997]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,997]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,998]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,998]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,998]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,998]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,999]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,999]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:16,999]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,000]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,000]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,000]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,000]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,000]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,001]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 15:49:17,053]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 15:49:17,055]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 15:49:17,115]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 15:49:17,123]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-55556f18-4521-4412-abf1-5311c990b9db/blockmgr-d028902b-a274-42b3-ae33-256b766aace6, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 15:49:17,127]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 15:49:17,128]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 15:49:17,130]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 15:49:17,132]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 15:49:17,134]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 15:49:17,134]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 15:49:17,136]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-55556f18-4521-4412-abf1-5311c990b9db {org.apache.spark.util.Utils}
[2016-06-26 15:49:17,144]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:49:17,146]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:49:17,173]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 15:49:18,029]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 15:49:22,163]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:04:56,892]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 16:04:57,005]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 16:04:57,071]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 16:04:57,071]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 16:04:57,087]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:04:57,088]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:04:57,088]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 16:04:57,494]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 16:04:57,549]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 16:04:57,746]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:46058] {Remoting}
[2016-06-26 16:04:57,759]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 46058. {org.apache.spark.util.Utils}
[2016-06-26 16:04:57,786]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 16:04:57,803]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 16:04:57,832]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4bbd4199-95ac-4b07-a43c-e0eb08d5f68e/blockmgr-df53f457-e114-4a19-89dc-b8d401149976 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 16:04:57,839]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:04:57,864]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4bbd4199-95ac-4b07-a43c-e0eb08d5f68e/httpd-0ed366ea-0208-4c74-baf8-e105b89fc790 {org.apache.spark.HttpFileServer}
[2016-06-26 16:04:57,866]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 16:04:57,911]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:04:57,936]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:38205 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:04:57,936]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 38205. {org.apache.spark.util.Utils}
[2016-06-26 16:04:57,947]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 16:04:58,050]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:04:58,063]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:04:58,063]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 16:04:58,065]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:04:58,126]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 16:04:58,475]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52201. {org.apache.spark.util.Utils}
[2016-06-26 16:04:58,475]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 52201 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 16:04:58,476]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:04:58,480]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:52201 with 983.1 MB RAM, BlockManagerId(driver, localhost, 52201) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 16:04:58,484]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:05:47,500]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,503]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,705]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,706]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,708]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:52201 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:05:47,711]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:05:47,816]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:05:47,885]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:05:47,902]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,903]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,903]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,907]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,911]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,916]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,917]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,945]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,945]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:05:47,947]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:52201 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:05:47,953]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:05:47,963]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:47,965]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:05:47,998]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:05:48,004]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 16:05:48,024]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466949947191:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:05:48,037]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:05:48,037]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:05:48,037]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:05:48,037]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:05:48,037]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:05:48,091]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:05:48,114]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:05:48,118]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.141 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:05:48,120]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:05:48,126]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.240674 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:06:20,015]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:06:20,016]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:06:20,031]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:06:20,031]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:06:20,032]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:52201 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:06:20,034]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:06:20,136]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:06:20,141]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:06:20,174]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:06:20,175]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:09:16,733]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:09:16,735]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-26 16:09:16,736]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:09:16,738]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-26 16:09:16,739]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:09:16,739]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-26 16:09:16,746]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:52201 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:09:16,750]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:52201 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:09:16,751]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:52201 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:11:15,450]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,450]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,462]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38488, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,462]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,463]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:52201 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:11:15,464]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:11:15,471]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:11:15,476]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:11:15,477]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,477]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,477]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,478]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,479]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,481]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42577, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,481]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,487]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=45793, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,487]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:15,488]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:52201 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:11:15,488]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:11:15,489]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,489]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:11:15,490]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:11:15,491]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-26 16:11:15,495]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466950275445:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:11:15,503]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:11:15,509]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:11:15,511]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:11:15,511]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.020 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:15,511]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.035154 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:11:46,927]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47674, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:46,927]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:46,937]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86162, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:46,937]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:11:46,938]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:52201 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:11:46,939]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:11:46,979]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:11:46,986]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:11:46,991]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:11:46,992]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:12:03,516]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 16:12:03,537]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,538]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,539]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,540]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,540]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,541]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,541]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,543]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,543]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,543]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,543]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,544]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,544]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,544]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,546]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,547]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:12:03,599]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:12:03,601]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:12:03,660]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 16:12:03,663]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4bbd4199-95ac-4b07-a43c-e0eb08d5f68e/blockmgr-df53f457-e114-4a19-89dc-b8d401149976, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 16:12:03,664]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:12:03,664]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 16:12:03,666]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:12:03,668]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 16:12:03,668]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 16:12:03,668]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 16:12:03,669]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4bbd4199-95ac-4b07-a43c-e0eb08d5f68e {org.apache.spark.util.Utils}
[2016-06-26 16:12:03,676]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:12:03,677]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:12:03,698]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:12:04,577]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:12:08,864]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:14:30,690]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 16:14:30,814]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 16:14:30,883]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 16:14:30,884]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 16:14:30,904]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:14:30,904]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:14:30,905]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 16:14:31,334]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 16:14:31,389]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 16:14:31,544]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:41296] {Remoting}
[2016-06-26 16:14:31,551]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 41296. {org.apache.spark.util.Utils}
[2016-06-26 16:14:31,575]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 16:14:31,591]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 16:14:31,613]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-24119a56-eb1c-4962-9a9a-5438b2d15058/blockmgr-080158b1-549a-4387-87a9-d53f42233a82 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 16:14:31,619]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:31,648]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-24119a56-eb1c-4962-9a9a-5438b2d15058/httpd-5854ceee-f812-4465-8a11-cdf9719a81c2 {org.apache.spark.HttpFileServer}
[2016-06-26 16:14:31,650]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 16:14:31,681]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:14:31,692]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:52106 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:14:31,692]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 52106. {org.apache.spark.util.Utils}
[2016-06-26 16:14:31,700]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 16:14:31,790]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:14:31,807]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:14:31,808]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 16:14:31,809]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:14:31,869]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 16:14:32,175]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45109. {org.apache.spark.util.Utils}
[2016-06-26 16:14:32,176]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 45109 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 16:14:32,177]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:14:32,181]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:45109 with 983.1 MB RAM, BlockManagerId(driver, localhost, 45109) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 16:14:32,183]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:14:54,496]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,499]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,633]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,634]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,636]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:45109 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:14:54,638]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:14:54,740]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:14:54,796]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:14:54,816]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,817]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,817]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,823]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,831]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,837]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,839]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,856]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,857]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:14:54,859]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:45109 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:14:54,860]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:14:54,877]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:54,881]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:14:54,906]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:14:54,914]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 16:14:54,935]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466950494240:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:14:54,943]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:14:54,943]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:14:54,943]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:14:54,943]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:14:54,943]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:14:54,987]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:14:55,001]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.110 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:55,008]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.211583 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:14:55,012]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 99 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:14:55,016]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:15:26,879]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:15:26,880]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:15:26,892]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:15:26,892]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:15:26,893]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:45109 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:15:26,894]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:15:26,996]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:15:27,002]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:15:27,023]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:15:27,026]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:16:56,159]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,159]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,170]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=128653, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,170]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,171]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:45109 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:16:56,172]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:16:56,179]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:16:56,186]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:16:56,187]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,187]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,187]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,188]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,189]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,190]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=132742, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,190]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,195]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1881) called with curMem=135958, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,195]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_4_piece0 stored as bytes in memory (estimated size 1881.0 B, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:16:56,196]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_4_piece0 in memory on localhost:45109 (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:16:56,197]  INFO {org.apache.spark.SparkContext} -  Created broadcast 4 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:16:56,197]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,197]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:16:56,199]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:16:56,199]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-26 16:16:56,203]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466950616155:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:16:56,210]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:16:56,215]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:16:56,215]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.017 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:16:56,216]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:16:56,216]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.029639 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:17:27,658]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=137839, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:17:27,658]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5 stored as values in memory (estimated size 37.6 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:17:27,668]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=176327, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:17:27,668]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 982.9 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:17:27,669]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_5_piece0 in memory on localhost:45109 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:17:27,669]  INFO {org.apache.spark.SparkContext} -  Created broadcast 5 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:17:27,705]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 17 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:17:27,710]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 17 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:17:27,716]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 25 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:17:27,717]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 25 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:19:35,941]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:45109 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:19:35,988]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_4_piece0 on localhost:45109 in memory (size: 1881.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:19:35,991]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_3_piece0 on localhost:45109 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:19:36,004]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 12 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:19:36,837]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 12 {org.apache.spark.ContextCleaner}
[2016-06-26 16:19:36,838]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:19:36,838]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 11 {org.apache.spark.ContextCleaner}
[2016-06-26 16:19:36,839]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:19:36,839]  INFO {org.apache.spark.ContextCleaner} -  Cleaned RDD 3 {org.apache.spark.ContextCleaner}
[2016-06-26 16:19:36,840]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_2_piece0 on localhost:45109 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:19:36,842]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:45109 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:19:55,429]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 16:19:55,448]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,450]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,451]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,451]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,452]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,453]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,453]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,453]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,454]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,455]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,456]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:19:55,508]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:19:55,510]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:19:55,568]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 16:19:55,573]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-24119a56-eb1c-4962-9a9a-5438b2d15058/blockmgr-080158b1-549a-4387-87a9-d53f42233a82, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 16:19:55,573]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:19:55,574]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 16:19:55,575]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:19:55,577]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 16:19:55,577]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 16:19:55,578]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-24119a56-eb1c-4962-9a9a-5438b2d15058 {org.apache.spark.util.Utils}
[2016-06-26 16:19:55,582]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 16:19:55,586]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:19:55,587]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:19:55,605]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:19:56,479]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:20:00,180]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:43:56,646]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 16:43:56,762]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 16:43:56,828]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 16:43:56,828]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 16:43:56,844]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:43:56,844]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:43:56,845]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 16:43:57,179]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 16:43:57,234]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 16:43:57,370]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:53557] {Remoting}
[2016-06-26 16:43:57,375]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 53557. {org.apache.spark.util.Utils}
[2016-06-26 16:43:57,394]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 16:43:57,406]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 16:43:57,424]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-43593a1f-e244-46c4-8388-bc1ae25a1642/blockmgr-86a7f7a3-b20e-4c9d-82f4-e8a4b92ee2c9 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 16:43:57,429]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:43:57,454]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-43593a1f-e244-46c4-8388-bc1ae25a1642/httpd-e0c29a19-bfb8-4bcb-a5ef-73983ae081df {org.apache.spark.HttpFileServer}
[2016-06-26 16:43:57,456]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 16:43:57,507]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:43:57,523]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:53737 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:43:57,523]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 53737. {org.apache.spark.util.Utils}
[2016-06-26 16:43:57,538]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 16:43:57,682]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:43:57,698]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:43:57,699]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 16:43:57,701]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:43:57,780]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 16:43:58,190]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40902. {org.apache.spark.util.Utils}
[2016-06-26 16:43:58,191]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 40902 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 16:43:58,192]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:43:58,196]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:40902 with 983.1 MB RAM, BlockManagerId(driver, localhost, 40902) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 16:43:58,199]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:44:32,490]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,493]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,631]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,631]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,633]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:40902 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:44:32,637]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:44:32,723]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:44:32,778]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:44:32,796]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,797]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,797]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,801]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,806]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,811]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,812]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,824]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,824]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:44:32,826]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:40902 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:44:32,826]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:44:32,838]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,840]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:44:32,872]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:44:32,879]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 16:44:32,897]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466952272202:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:44:32,905]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:44:32,905]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:44:32,905]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:44:32,905]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:44:32,905]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:44:32,946]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:44:32,961]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 95 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:44:32,962]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.106 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:44:32,967]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:44:32,972]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.193481 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:45:04,880]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:45:04,881]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:45:04,894]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:45:04,894]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:45:04,895]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:40902 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:45:04,896]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:45:04,986]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:45:04,995]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:45:05,013]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:45:05,015]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:46:58,058]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 16:46:58,074]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,074]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,075]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,075]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,076]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,077]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,077]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,079]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,079]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,080]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,080]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,081]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,081]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,081]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,082]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,082]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,082]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,082]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,082]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,083]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:46:58,135]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:46:58,142]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:46:58,214]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 16:46:58,217]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-43593a1f-e244-46c4-8388-bc1ae25a1642/blockmgr-86a7f7a3-b20e-4c9d-82f4-e8a4b92ee2c9, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 16:46:58,218]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:46:58,220]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 16:46:58,221]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:46:58,223]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 16:46:58,223]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 16:46:58,224]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 16:46:58,224]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-43593a1f-e244-46c4-8388-bc1ae25a1642 {org.apache.spark.util.Utils}
[2016-06-26 16:46:58,231]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:46:58,233]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:46:58,260]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:46:59,159]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:47:03,151]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:47:36,169]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 16:47:36,292]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 16:47:36,347]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 16:47:36,347]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 16:47:36,358]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:47:36,358]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 16:47:36,359]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 16:47:36,687]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 16:47:36,732]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 16:47:36,866]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:45457] {Remoting}
[2016-06-26 16:47:36,870]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 45457. {org.apache.spark.util.Utils}
[2016-06-26 16:47:36,888]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 16:47:36,898]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 16:47:36,913]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-addefb40-5819-4c54-bef3-d24f39256426/blockmgr-ce2d61f0-539d-4643-8c33-23a3961a43fd {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 16:47:36,917]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:36,935]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-addefb40-5819-4c54-bef3-d24f39256426/httpd-e8738154-fe56-4ee9-94a1-8e9300ad75c8 {org.apache.spark.HttpFileServer}
[2016-06-26 16:47:36,937]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 16:47:36,974]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:47:36,986]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:57630 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:47:36,986]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 57630. {org.apache.spark.util.Utils}
[2016-06-26 16:47:36,994]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 16:47:37,086]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 16:47:37,097]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 16:47:37,097]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 16:47:37,098]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:47:37,157]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 16:47:37,436]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57943. {org.apache.spark.util.Utils}
[2016-06-26 16:47:37,436]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 57943 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 16:47:37,437]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:47:37,440]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:57943 with 983.1 MB RAM, BlockManagerId(driver, localhost, 57943) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 16:47:37,442]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:47:47,656]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,660]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,802]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,802]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,804]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:57943 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:47:47,807]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:47:47,887]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-26 16:47:47,928]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-26 16:47:47,949]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:47,950]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:47,950]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:47,955]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:47,958]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:47,964]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,966]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,993]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,993]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:47:47,994]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:57943 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:47:47,995]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-26 16:47:48,001]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:48,005]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:47:48,029]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:47:48,042]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-26 16:47:48,062]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466952467376:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-26 16:47:48,069]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:47:48,070]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:47:48,070]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:47:48,070]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:47:48,070]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-26 16:47:48,110]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-26 16:47:48,125]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.110 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:48,125]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 100 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-26 16:47:48,131]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.201779 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:47:48,135]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-26 16:48:19,915]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:48:19,915]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:48:19,932]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:48:19,932]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:48:19,933]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:57943 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-26 16:48:19,934]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-26 16:48:20,037]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:48:20,042]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:48:20,065]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-26 16:48:20,066]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-26 16:49:43,008]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 16:49:43,022]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,022]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,023]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,023]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,023]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,023]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,023]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,024]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,024]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,024]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,024]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,024]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,025]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,025]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,025]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,025]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,025]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,026]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,026]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,027]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 16:49:43,079]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 16:49:43,081]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 16:49:43,138]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 16:49:43,142]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-addefb40-5819-4c54-bef3-d24f39256426/blockmgr-ce2d61f0-539d-4643-8c33-23a3961a43fd, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 16:49:43,142]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 16:49:43,143]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 16:49:43,144]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 16:49:43,146]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 16:49:43,147]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 16:49:43,147]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 16:49:43,147]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-addefb40-5819-4c54-bef3-d24f39256426 {org.apache.spark.util.Utils}
[2016-06-26 16:49:43,159]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:49:43,160]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:49:43,182]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 16:49:44,060]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 16:49:47,776]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 23:41:38,061]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-26 23:41:38,171]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-26 23:41:38,230]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-26 23:41:38,231]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-26 23:41:38,241]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 23:41:38,242]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-26 23:41:38,242]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-26 23:41:38,808]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-26 23:41:38,845]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-26 23:41:38,976]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:34170] {Remoting}
[2016-06-26 23:41:38,982]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 34170. {org.apache.spark.util.Utils}
[2016-06-26 23:41:38,999]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-26 23:41:39,010]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-26 23:41:39,026]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-22571d81-8341-498c-8f26-31e7fb8c073e/blockmgr-9d55097b-b209-4a2a-bcb6-79b3267a57d9 {org.apache.spark.storage.DiskBlockManager}
[2016-06-26 23:41:39,030]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-26 23:41:39,051]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-22571d81-8341-498c-8f26-31e7fb8c073e/httpd-c613e5b0-980f-4599-a184-c4808cb7987a {org.apache.spark.HttpFileServer}
[2016-06-26 23:41:39,052]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-26 23:41:39,082]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 23:41:39,092]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:40621 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 23:41:39,093]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 40621. {org.apache.spark.util.Utils}
[2016-06-26 23:41:39,100]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-26 23:41:39,195]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-26 23:41:39,205]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-26 23:41:39,206]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-26 23:41:39,207]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 23:41:39,258]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-26 23:41:39,387]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58717. {org.apache.spark.util.Utils}
[2016-06-26 23:41:39,388]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 58717 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-26 23:41:39,388]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 23:41:39,392]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:58717 with 983.1 MB RAM, BlockManagerId(driver, localhost, 58717) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-26 23:41:39,394]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 23:46:00,144]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-26 23:46:00,167]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,168]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,168]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,168]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,169]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,169]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,169]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,170]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,171]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,171]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,171]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,171]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,172]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,172]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,172]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,173]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-26 23:46:00,226]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-26 23:46:00,230]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-26 23:46:00,289]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-26 23:46:00,305]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-22571d81-8341-498c-8f26-31e7fb8c073e/blockmgr-9d55097b-b209-4a2a-bcb6-79b3267a57d9, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-26 23:46:00,306]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-26 23:46:00,307]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-26 23:46:00,308]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-26 23:46:00,310]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-26 23:46:00,310]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-26 23:46:00,310]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-26 23:46:00,311]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-22571d81-8341-498c-8f26-31e7fb8c073e {org.apache.spark.util.Utils}
[2016-06-26 23:46:00,319]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 23:46:00,321]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 23:46:00,338]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-26 23:46:01,236]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-26 23:46:04,757]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:01:22,001]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-27 00:01:22,125]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-27 00:01:22,177]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-27 00:01:22,177]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-27 00:01:22,187]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:01:22,187]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:01:22,187]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-27 00:01:22,498]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-27 00:01:22,557]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-27 00:01:22,691]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:56660] {Remoting}
[2016-06-27 00:01:22,695]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 56660. {org.apache.spark.util.Utils}
[2016-06-27 00:01:22,719]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-27 00:01:22,736]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-27 00:01:22,762]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4e6d1d47-d3e2-4ef1-987b-cb9948420524/blockmgr-9d3f689f-f643-4903-ac40-c11d75979a44 {org.apache.spark.storage.DiskBlockManager}
[2016-06-27 00:01:22,768]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:22,798]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4e6d1d47-d3e2-4ef1-987b-cb9948420524/httpd-42ba72d3-599e-4c7a-ac2a-d64844b5aee9 {org.apache.spark.HttpFileServer}
[2016-06-27 00:01:22,800]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-27 00:01:22,840]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:01:22,853]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:40601 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:01:22,854]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 40601. {org.apache.spark.util.Utils}
[2016-06-27 00:01:22,863]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-27 00:01:22,963]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:01:22,974]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:01:22,974]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-27 00:01:22,975]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:01:23,020]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-27 00:01:23,344]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52847. {org.apache.spark.util.Utils}
[2016-06-27 00:01:23,344]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 52847 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-27 00:01:23,345]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:01:23,349]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:52847 with 983.1 MB RAM, BlockManagerId(driver, localhost, 52847) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-27 00:01:23,351]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:01:44,530]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,533]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,687]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,687]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,689]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:52847 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:01:44,692]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:01:44,773]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-27 00:01:44,816]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:01:44,834]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,835]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,840]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,845]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,849]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,850]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,866]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,867]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:01:44,868]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:52847 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:01:44,868]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-27 00:01:44,874]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:44,875]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:01:44,905]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:01:44,915]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-27 00:01:44,937]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466978504300:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-27 00:01:44,947]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:01:44,947]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:01:44,947]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:01:44,947]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:01:44,947]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:01:44,990]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-27 00:01:45,027]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.142 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:45,032]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.215831 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:01:45,033]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 129 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:01:45,040]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:02:17,686]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,687]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,703]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,704]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,705]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:52847 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:02:17,705]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:02:17,711]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-27 00:02:17,715]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:02:17,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 1 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 1(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,718]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,719]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,720]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=90165, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,721]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,725]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1880) called with curMem=93381, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,725]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_3_piece0 stored as bytes in memory (estimated size 1880.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:02:17,726]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_3_piece0 in memory on localhost:52847 (size: 1880.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:02:17,727]  INFO {org.apache.spark.SparkContext} -  Created broadcast 3 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-27 00:02:17,727]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,727]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 1.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:02:17,728]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:02:17,728]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 1.0 (TID 1) {org.apache.spark.executor.Executor}
[2016-06-27 00:02:17,731]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466978537681:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-27 00:02:17,737]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 1.0 (TID 1). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-27 00:02:17,742]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 1.0 (TID 1) in 15 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:02:17,743]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 1.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:02:17,743]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 1 (first at MLUtils.java:91) finished in 0.016 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:02:17,743]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 1 finished: first at MLUtils.java:91, took 0.027485 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:03:04,213]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-27 00:03:04,229]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,230]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,230]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,231]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,232]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,232]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,232]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,233]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,233]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,233]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,234]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,234]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,235]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,235]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,235]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,236]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,236]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,237]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,237]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,238]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:03:04,290]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:03:04,291]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:03:04,350]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-27 00:03:04,352]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4e6d1d47-d3e2-4ef1-987b-cb9948420524/blockmgr-9d3f689f-f643-4903-ac40-c11d75979a44, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-27 00:03:04,353]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:03:04,353]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-27 00:03:04,354]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:03:04,356]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-27 00:03:04,357]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-27 00:03:04,357]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4e6d1d47-d3e2-4ef1-987b-cb9948420524 {org.apache.spark.util.Utils}
[2016-06-27 00:03:04,359]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-27 00:03:04,366]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:03:04,367]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:03:04,385]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:03:05,252]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:03:09,136]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:09:35,408]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-27 00:09:35,558]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-27 00:09:35,625]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-27 00:09:35,625]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-27 00:09:35,641]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:09:35,642]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:09:35,642]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-27 00:09:36,028]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-27 00:09:36,068]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-27 00:09:36,247]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:54779] {Remoting}
[2016-06-27 00:09:36,256]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 54779. {org.apache.spark.util.Utils}
[2016-06-27 00:09:36,274]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-27 00:09:36,288]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-27 00:09:36,342]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4dd26a4e-8f59-4c21-9aa6-c1162254a5c7/blockmgr-b07b40b0-0ff5-43a0-a4d9-d219b3570212 {org.apache.spark.storage.DiskBlockManager}
[2016-06-27 00:09:36,348]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:36,378]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4dd26a4e-8f59-4c21-9aa6-c1162254a5c7/httpd-ead698fc-2066-426c-b8cb-0ad80ef1bd0f {org.apache.spark.HttpFileServer}
[2016-06-27 00:09:36,381]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-27 00:09:36,413]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:09:36,425]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:49354 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:09:36,425]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 49354. {org.apache.spark.util.Utils}
[2016-06-27 00:09:36,434]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-27 00:09:36,535]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:09:36,544]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:09:36,544]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-27 00:09:36,546]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:09:36,601]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-27 00:09:36,893]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33824. {org.apache.spark.util.Utils}
[2016-06-27 00:09:36,894]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 33824 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-27 00:09:36,895]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:09:36,898]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:33824 with 983.1 MB RAM, BlockManagerId(driver, localhost, 33824) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-27 00:09:36,900]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:09:48,131]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,134]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,290]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,291]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,293]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:33824 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:09:48,296]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:09:48,385]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-27 00:09:48,438]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:09:48,458]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,459]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,459]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,465]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,471]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,476]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,477]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,498]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,504]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:09:48,505]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:33824 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:09:48,507]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-27 00:09:48,517]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,519]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:09:48,548]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:09:48,555]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-27 00:09:48,580]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466978987850:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-27 00:09:48,591]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:09:48,591]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:09:48,591]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:09:48,592]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:09:48,592]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:09:48,642]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-27 00:09:48,661]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:09:48,663]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.130 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:09:48,663]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:09:48,676]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.237427 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:10:20,549]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:10:20,550]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:10:20,562]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:10:20,563]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:10:20,564]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:33824 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:10:20,565]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-27 00:10:20,709]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-27 00:10:20,714]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-27 00:10:20,732]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-27 00:10:20,733]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-27 00:15:57,015]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_1_piece0 on localhost:33824 in memory (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:15:57,018]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Removed broadcast_0_piece0 on localhost:33824 in memory (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:24:31,520]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-27 00:24:31,542]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,551]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,555]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,565]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,571]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,572]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,576]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,580]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,582]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,582]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,583]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,587]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,588]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,589]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,591]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,597]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,607]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,615]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,616]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,618]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:24:31,679]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:24:31,689]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:24:31,764]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-27 00:24:31,773]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4dd26a4e-8f59-4c21-9aa6-c1162254a5c7/blockmgr-b07b40b0-0ff5-43a0-a4d9-d219b3570212, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-27 00:24:31,787]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:24:31,788]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-27 00:24:31,789]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:24:31,821]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-27 00:24:31,822]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-27 00:24:31,823]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-4dd26a4e-8f59-4c21-9aa6-c1162254a5c7 {org.apache.spark.util.Utils}
[2016-06-27 00:24:31,827]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-27 00:24:31,859]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:24:31,863]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:24:31,929]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:24:32,675]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:24:40,263]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:40:41,213]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-27 00:40:41,327]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-27 00:40:41,382]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-27 00:40:41,383]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-27 00:40:41,397]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:40:41,397]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:40:41,398]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-27 00:40:41,813]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-27 00:40:41,848]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-27 00:40:42,003]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:36735] {Remoting}
[2016-06-27 00:40:42,010]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 36735. {org.apache.spark.util.Utils}
[2016-06-27 00:40:42,031]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-27 00:40:42,047]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-27 00:40:42,065]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d8940581-7bee-455f-9f3d-0c3ea617e2b6/blockmgr-e3646f4d-493b-4cf9-993f-7a87e821fb98 {org.apache.spark.storage.DiskBlockManager}
[2016-06-27 00:40:42,070]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:40:42,093]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d8940581-7bee-455f-9f3d-0c3ea617e2b6/httpd-2e0b583a-5fe9-4534-84aa-44f5ed2e5ddb {org.apache.spark.HttpFileServer}
[2016-06-27 00:40:42,095]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-27 00:40:42,135]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:40:42,156]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:50671 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:40:42,157]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 50671. {org.apache.spark.util.Utils}
[2016-06-27 00:40:42,165]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-27 00:40:42,257]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:40:42,270]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:40:42,270]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-27 00:40:42,272]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:40:42,330]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-27 00:40:42,619]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45961. {org.apache.spark.util.Utils}
[2016-06-27 00:40:42,619]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 45961 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-27 00:40:42,620]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:40:42,623]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:45961 with 983.1 MB RAM, BlockManagerId(driver, localhost, 45961) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-27 00:40:42,626]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:41:36,477]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,480]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,633]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,633]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,635]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:45961 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:41:36,639]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:41:36,739]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-27 00:41:36,799]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 00:41:36,823]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,823]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,824]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,833]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,843]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,850]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,851]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,884]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,885]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:41:36,886]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:45961 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:41:36,887]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-27 00:41:36,896]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:36,898]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:41:36,945]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:41:36,955]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-27 00:41:36,985]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1466980896178:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-27 00:41:36,998]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:41:36,998]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:41:36,998]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:41:36,998]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:41:36,998]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 00:41:37,074]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-27 00:41:37,093]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.180 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:41:37,092]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 158 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 00:41:37,105]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 00:41:37,107]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.308099 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:42:09,100]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:42:09,101]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:42:09,114]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:42:09,115]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:42:09,115]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:45961 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 00:42:09,116]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
[2016-06-27 00:42:09,230]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 3 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-27 00:42:09,244]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 3 {org.apache.spark.storage.BlockManager}
[2016-06-27 00:42:09,272]  INFO {org.apache.spark.rdd.MapPartitionsRDD} -  Removing RDD 11 from persistence list {org.apache.spark.rdd.MapPartitionsRDD}
[2016-06-27 00:42:09,273]  INFO {org.apache.spark.storage.BlockManager} -  Removing RDD 11 {org.apache.spark.storage.BlockManager}
[2016-06-27 00:44:22,037]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-27 00:44:22,053]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,053]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,054]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,054]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,054]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,054]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,055]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,056]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,057]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,058]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,059]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,059]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 00:44:22,112]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:44:22,113]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 00:44:22,171]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-27 00:44:22,176]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d8940581-7bee-455f-9f3d-0c3ea617e2b6/blockmgr-e3646f4d-493b-4cf9-993f-7a87e821fb98, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-27 00:44:22,177]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:44:22,178]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-27 00:44:22,179]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:44:22,183]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-27 00:44:22,184]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-27 00:44:22,185]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-27 00:44:22,186]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-d8940581-7bee-455f-9f3d-0c3ea617e2b6 {org.apache.spark.util.Utils}
[2016-06-27 00:44:22,192]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:44:22,194]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:44:22,229]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 00:44:23,084]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:44:27,243]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 00:49:18,288]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-27 00:49:18,407]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-27 00:49:18,466]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 192.168.1.11 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-27 00:49:18,466]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-27 00:49:18,485]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:49:18,486]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 00:49:18,486]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-27 00:49:19,212]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-27 00:49:19,269]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-27 00:49:19,426]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.11:43014] {Remoting}
[2016-06-27 00:49:19,428]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 43014. {org.apache.spark.util.Utils}
[2016-06-27 00:49:19,450]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-27 00:49:19,465]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-27 00:49:19,487]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-215e8292-e124-42a9-a23e-3431c2187d63/blockmgr-bb37a228-3627-4a21-9007-6f534b0d3428 {org.apache.spark.storage.DiskBlockManager}
[2016-06-27 00:49:19,493]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-27 00:49:19,524]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-215e8292-e124-42a9-a23e-3431c2187d63/httpd-357f8aaf-478c-4609-b229-640a367050df {org.apache.spark.HttpFileServer}
[2016-06-27 00:49:19,527]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-27 00:49:19,574]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:49:19,587]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:36994 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:49:19,587]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 36994. {org.apache.spark.util.Utils}
[2016-06-27 00:49:19,595]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-27 00:49:19,693]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 00:49:19,711]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 00:49:19,714]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-27 00:49:19,715]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 00:49:19,771]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-27 00:49:19,936]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42052. {org.apache.spark.util.Utils}
[2016-06-27 00:49:19,937]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 42052 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-27 00:49:19,937]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 00:49:19,942]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:42052 with 983.1 MB RAM, BlockManagerId(driver, localhost, 42052) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-27 00:49:19,945]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 01:04:34,419]  INFO {org.apache.spark.SparkContext} -  Invoking stop() from shutdown hook {org.apache.spark.SparkContext}
[2016-06-27 01:04:34,438]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/metrics/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,438]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,438]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/api,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,439]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,439]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/static,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,439]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,439]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,440]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,440]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/executors,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,440]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,440]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/environment,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,440]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,441]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/rdd,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,441]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,441]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/storage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,441]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,441]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/pool,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,442]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,442]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/stage,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/stages,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/job,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs/json,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,443]  INFO {org.spark-project.jetty.server.handler.ContextHandler} -  stopped o.s.j.s.ServletContextHandler{/jobs,null} {org.spark-project.jetty.server.handler.ContextHandler}
[2016-06-27 01:04:34,497]  INFO {org.apache.spark.ui.SparkUI} -  Stopped Spark web UI at http://192.168.1.11:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 01:04:34,502]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Stopping DAGScheduler {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 01:04:34,573]  INFO {org.apache.spark.MapOutputTrackerMasterEndpoint} -  MapOutputTrackerMasterEndpoint stopped! {org.apache.spark.MapOutputTrackerMasterEndpoint}
[2016-06-27 01:04:34,580]  INFO {org.apache.spark.util.Utils} -  path = /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-215e8292-e124-42a9-a23e-3431c2187d63/blockmgr-bb37a228-3627-4a21-9007-6f534b0d3428, already present as root for deletion. {org.apache.spark.util.Utils}
[2016-06-27 01:04:34,584]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore cleared {org.apache.spark.storage.MemoryStore}
[2016-06-27 01:04:34,585]  INFO {org.apache.spark.storage.BlockManager} -  BlockManager stopped {org.apache.spark.storage.BlockManager}
[2016-06-27 01:04:34,591]  INFO {org.apache.spark.storage.BlockManagerMaster} -  BlockManagerMaster stopped {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 01:04:34,595]  INFO {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint} -  OutputCommitCoordinator stopped! {org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint}
[2016-06-27 01:04:34,595]  INFO {org.apache.spark.SparkContext} -  Successfully stopped SparkContext {org.apache.spark.SparkContext}
[2016-06-27 01:04:34,595]  INFO {org.apache.spark.util.Utils} -  Shutdown hook called {org.apache.spark.util.Utils}
[2016-06-27 01:04:34,596]  INFO {org.apache.spark.util.Utils} -  Deleting directory /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-215e8292-e124-42a9-a23e-3431c2187d63 {org.apache.spark.util.Utils}
[2016-06-27 01:04:34,607]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Shutting down remote daemon. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 01:04:34,609]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remote daemon shut down; proceeding with flushing remote transports. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 01:04:34,635]  INFO {akka.remote.RemoteActorRefProvider$RemotingTerminator} -  Remoting shut down. {akka.remote.RemoteActorRefProvider$RemotingTerminator}
[2016-06-27 01:04:35,525]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 01:04:39,482]  INFO {org.apache.spark.SparkContext} -  SparkContext already stopped. {org.apache.spark.SparkContext}
[2016-06-27 15:14:22,174]  INFO {org.apache.spark.SparkContext} -  Running Spark version 1.4.1 {org.apache.spark.SparkContext}
[2016-06-27 15:14:22,288]  WARN {org.apache.hadoop.util.NativeCodeLoader} -  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable {org.apache.hadoop.util.NativeCodeLoader}
[2016-06-27 15:14:22,350]  WARN {org.apache.spark.util.Utils} -  Your hostname, pekasa-ThinkPad-L450 resolves to a loopback address: 127.0.1.1; using 10.126.44.84 instead (on interface wlan0) {org.apache.spark.util.Utils}
[2016-06-27 15:14:22,350]  WARN {org.apache.spark.util.Utils} -  Set SPARK_LOCAL_IP if you need to bind to another address {org.apache.spark.util.Utils}
[2016-06-27 15:14:22,364]  INFO {org.apache.spark.SecurityManager} -  Changing view acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 15:14:22,364]  INFO {org.apache.spark.SecurityManager} -  Changing modify acls to: pekasa {org.apache.spark.SecurityManager}
[2016-06-27 15:14:22,365]  INFO {org.apache.spark.SecurityManager} -  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pekasa); users with modify permissions: Set(pekasa) {org.apache.spark.SecurityManager}
[2016-06-27 15:14:22,803]  INFO {akka.event.slf4j.Slf4jLogger} -  Slf4jLogger started {akka.event.slf4j.Slf4jLogger}
[2016-06-27 15:14:22,849]  INFO {Remoting} -  Starting remoting {Remoting}
[2016-06-27 15:14:22,992]  INFO {Remoting} -  Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.126.44.84:39830] {Remoting}
[2016-06-27 15:14:22,997]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'sparkDriver' on port 39830. {org.apache.spark.util.Utils}
[2016-06-27 15:14:23,015]  INFO {org.apache.spark.SparkEnv} -  Registering MapOutputTracker {org.apache.spark.SparkEnv}
[2016-06-27 15:14:23,025]  INFO {org.apache.spark.SparkEnv} -  Registering BlockManagerMaster {org.apache.spark.SparkEnv}
[2016-06-27 15:14:23,041]  INFO {org.apache.spark.storage.DiskBlockManager} -  Created local directory at /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-ce10e68c-c020-4dd6-9c68-e51b53c95fb9/blockmgr-03515c49-7dbf-4a44-95f1-59e32d866f65 {org.apache.spark.storage.DiskBlockManager}
[2016-06-27 15:14:23,046]  INFO {org.apache.spark.storage.MemoryStore} -  MemoryStore started with capacity 983.1 MB {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:23,074]  INFO {org.apache.spark.HttpFileServer} -  HTTP File server directory is /home/pekasa/GSOC/wso2ml-1.1.0/tmp/spark-ce10e68c-c020-4dd6-9c68-e51b53c95fb9/httpd-78650d26-99cc-4aec-a1d9-21f6f9e61e4f {org.apache.spark.HttpFileServer}
[2016-06-27 15:14:23,077]  INFO {org.apache.spark.HttpServer} -  Starting HTTP Server {org.apache.spark.HttpServer}
[2016-06-27 15:14:23,118]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 15:14:23,131]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SocketConnector@0.0.0.0:48083 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 15:14:23,131]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'HTTP file server' on port 48083. {org.apache.spark.util.Utils}
[2016-06-27 15:14:23,140]  INFO {org.apache.spark.SparkEnv} -  Registering OutputCommitCoordinator {org.apache.spark.SparkEnv}
[2016-06-27 15:14:23,232]  INFO {org.spark-project.jetty.server.Server} -  jetty-8.y.z-SNAPSHOT {org.spark-project.jetty.server.Server}
[2016-06-27 15:14:23,243]  INFO {org.spark-project.jetty.server.AbstractConnector} -  Started SelectChannelConnector@0.0.0.0:4040 {org.spark-project.jetty.server.AbstractConnector}
[2016-06-27 15:14:23,247]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'SparkUI' on port 4040. {org.apache.spark.util.Utils}
[2016-06-27 15:14:23,248]  INFO {org.apache.spark.ui.SparkUI} -  Started SparkUI at http://10.126.44.84:4040 {org.apache.spark.ui.SparkUI}
[2016-06-27 15:14:23,308]  INFO {org.apache.spark.executor.Executor} -  Starting executor ID driver on host localhost {org.apache.spark.executor.Executor}
[2016-06-27 15:14:23,596]  INFO {org.apache.spark.util.Utils} -  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46941. {org.apache.spark.util.Utils}
[2016-06-27 15:14:23,596]  INFO {org.apache.spark.network.netty.NettyBlockTransferService} -  Server created on 46941 {org.apache.spark.network.netty.NettyBlockTransferService}
[2016-06-27 15:14:23,597]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Trying to register BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 15:14:23,601]  INFO {org.apache.spark.storage.BlockManagerMasterEndpoint} -  Registering block manager localhost:46941 with 983.1 MB RAM, BlockManagerId(driver, localhost, 46941) {org.apache.spark.storage.BlockManagerMasterEndpoint}
[2016-06-27 15:14:23,604]  INFO {org.apache.spark.storage.BlockManagerMaster} -  Registered BlockManager {org.apache.spark.storage.BlockManagerMaster}
[2016-06-27 15:14:29,138]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38416) called with curMem=0, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,141]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0 stored as values in memory (estimated size 37.5 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,309]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=38416, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,310]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,312]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_0_piece0 in memory on localhost:46941 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 15:14:29,316]  INFO {org.apache.spark.SparkContext} -  Created broadcast 0 from textFile at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 15:14:29,408]  INFO {org.apache.hadoop.mapred.FileInputFormat} -  Total input paths to process : 1 {org.apache.hadoop.mapred.FileInputFormat}
[2016-06-27 15:14:29,464]  INFO {org.apache.spark.SparkContext} -  Starting job: first at MLUtils.java:91 {org.apache.spark.SparkContext}
[2016-06-27 15:14:29,482]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Got job 0 (first at MLUtils.java:91) with 1 output partitions (allowLocal=true) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,482]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Final stage: ResultStage 0(first at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,483]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Parents of final stage: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,488]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Missing parents: List() {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,492]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91), which has no missing parents {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,497]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(3216) called with curMem=42505, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,498]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,516]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(1867) called with curMem=45721, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,516]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_1_piece0 stored as bytes in memory (estimated size 1867.0 B, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:14:29,517]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_1_piece0 in memory on localhost:46941 (size: 1867.0 B, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 15:14:29,518]  INFO {org.apache.spark.SparkContext} -  Created broadcast 1 from broadcast at DAGScheduler.scala:874 {org.apache.spark.SparkContext}
[2016-06-27 15:14:29,529]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at MLUtils.java:91) {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,531]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Adding task set 0.0 with 1 tasks {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 15:14:29,568]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1476 bytes) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 15:14:29,579]  INFO {org.apache.spark.executor.Executor} -  Running task 0.0 in stage 0.0 (TID 0) {org.apache.spark.executor.Executor}
[2016-06-27 15:14:29,615]  INFO {org.apache.spark.rdd.HadoopRDD} -  Input split: file:/home/pekasa/GSOC/wso2ml-1.1.0/datasets/breastCancerWisconsin-stacking-dataset.-1234.1467033268785:0+20060 {org.apache.spark.rdd.HadoopRDD}
[2016-06-27 15:14:29,629]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.tip.id is deprecated. Instead, use mapreduce.task.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 15:14:29,629]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 15:14:29,629]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 15:14:29,629]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 15:14:29,629]  INFO {org.apache.hadoop.conf.Configuration.deprecation} -  mapred.job.id is deprecated. Instead, use mapreduce.job.id {org.apache.hadoop.conf.Configuration.deprecation}
[2016-06-27 15:14:29,702]  INFO {org.apache.spark.executor.Executor} -  Finished task 0.0 in stage 0.0 (TID 0). 1944 bytes result sent to driver {org.apache.spark.executor.Executor}
[2016-06-27 15:14:29,717]  INFO {org.apache.spark.scheduler.DAGScheduler} -  ResultStage 0 (first at MLUtils.java:91) finished in 0.173 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:14:29,719]  INFO {org.apache.spark.scheduler.TaskSetManager} -  Finished task 0.0 in stage 0.0 (TID 0) in 157 ms on localhost (1/1) {org.apache.spark.scheduler.TaskSetManager}
[2016-06-27 15:14:29,728]  INFO {org.apache.spark.scheduler.TaskSchedulerImpl} -  Removed TaskSet 0.0, whose tasks have all completed, from pool  {org.apache.spark.scheduler.TaskSchedulerImpl}
[2016-06-27 15:14:29,735]  INFO {org.apache.spark.scheduler.DAGScheduler} -  Job 0 finished: first at MLUtils.java:91, took 0.271488 s {org.apache.spark.scheduler.DAGScheduler}
[2016-06-27 15:15:01,759]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(38488) called with curMem=47588, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:15:01,760]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2 stored as values in memory (estimated size 37.6 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:15:01,780]  INFO {org.apache.spark.storage.MemoryStore} -  ensureFreeSpace(4089) called with curMem=86076, maxMem=1030823608 {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:15:01,781]  INFO {org.apache.spark.storage.MemoryStore} -  Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 983.0 MB) {org.apache.spark.storage.MemoryStore}
[2016-06-27 15:15:01,782]  INFO {org.apache.spark.storage.BlockManagerInfo} -  Added broadcast_2_piece0 in memory on localhost:46941 (size: 4.0 KB, free: 983.1 MB) {org.apache.spark.storage.BlockManagerInfo}
[2016-06-27 15:15:01,783]  INFO {org.apache.spark.SparkContext} -  Created broadcast 2 from textFile at MLModelHandler.java:953 {org.apache.spark.SparkContext}
